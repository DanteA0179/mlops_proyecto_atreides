# Gu√≠a para Agentes de IA - Energy Optimization Copilot

Este archivo contiene instrucciones y buenas pr√°cticas para agentes de IA que trabajen en este proyecto.

---

## üéØ Contexto del Proyecto

**Nombre**: Energy Optimization Copilot  
**Tipo**: Proyecto MLOps - Sistema de Optimizaci√≥n Energ√©tica con IA  
**Stack**: Python 3.11+, Poetry, DVC, MLflow, DuckDB, FastAPI, Scikit-Learn  
**Objetivo**: Predicci√≥n y an√°lisis de consumo energ√©tico en la industria sider√∫rgica

---

## üìã Buenas Pr√°cticas Establecidas

### 1. Idioma y Documentaci√≥n

#### ‚úÖ Notebooks
- **Texto explicativo**: SIEMPRE en espa√±ol
- **C√≥digo**: SIEMPRE en ingl√©s
- **Comentarios en c√≥digo**: En ingl√©s
- **Markdown cells**: En espa√±ol

**Ejemplo correcto**:
```python
# Notebook cell (Markdown)
## 1. An√°lisis Exploratorio de Datos

Este an√°lisis explora los patrones de consumo energ√©tico...

# Notebook cell (Code)
# Load data and perform initial exploration
df = pl.read_parquet("data/processed/steel_cleaned.parquet")
summary_stats = df.describe()
```

#### ‚úÖ C√≥digo Python
- **Nombres de variables**: En ingl√©s
- **Nombres de funciones**: En ingl√©s
- **Docstrings**: En ingl√©s (estilo Google)
- **Comentarios**: En ingl√©s

#### ‚úÖ Documentaci√≥n
- **README.md**: En espa√±ol
- **Documentaci√≥n t√©cnica**: En espa√±ol
- **Docstrings en c√≥digo**: En ingl√©s
- **Comentarios inline**: En ingl√©s

---

### 2. Estructura de C√≥digo

#### ‚úÖ Funciones Reutilizables

**SIEMPRE** crear funciones reutilizables en `src/utils/` en lugar de c√≥digo duplicado en notebooks.

**‚ùå Incorrecto** (c√≥digo en notebook):
```python
# En notebook
conn = duckdb.connect("data/steel.duckdb")
df = conn.execute("SELECT * FROM steel_cleaned").pl()
conn.close()
```

**‚úÖ Correcto** (usar funci√≥n de utils):
```python
# En notebook
from src.utils.duckdb_utils import quick_query
df = quick_query("SELECT * FROM steel_cleaned")
```

#### ‚úÖ Organizaci√≥n de Utilidades

```
src/utils/
‚îú‚îÄ‚îÄ duckdb_utils.py          # Funciones para DuckDB
‚îú‚îÄ‚îÄ data_cleaning.py         # Limpieza de datos
‚îú‚îÄ‚îÄ data_quality.py          # An√°lisis de calidad
‚îú‚îÄ‚îÄ outlier_detection.py     # Detecci√≥n de outliers
‚îú‚îÄ‚îÄ visualization.py         # Visualizaciones
‚îî‚îÄ‚îÄ secrets.py               # Manejo de secretos
```

**Regla**: Si una funci√≥n se usa m√°s de una vez, debe estar en `src/utils/`.

---

### 3. Programaci√≥n Orientada a Objetos

#### ‚úÖ Transformers de Scikit-Learn

Para feature engineering, SIEMPRE usar clases que hereden de `BaseEstimator` y `TransformerMixin`:

```python
from sklearn.base import BaseEstimator, TransformerMixin

class TemporalFeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Creates temporal features from NSM column.
    
    Parameters
    ----------
    nsm_column : str, default='NSM'
        Name of the column containing seconds from midnight
    """
    
    def __init__(self, nsm_column='NSM'):
        self.nsm_column = nsm_column
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = X.copy()
        X['hour'] = (X[self.nsm_column] // 3600).astype(int)
        return X
```

**Beneficios**:
- ‚úÖ Reutilizable en pipelines de sklearn
- ‚úÖ Compatible con `fit()` y `transform()`
- ‚úÖ F√°cil de testear

---

### 4. Pipelines de Scikit-Learn

#### ‚úÖ SIEMPRE usar pipelines

**‚ùå Incorrecto**:
```python
# C√≥digo suelto
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
model = XGBRegressor()
model.fit(X_scaled, y_train)
```

**‚úÖ Correcto**:
```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', XGBRegressor())
])
pipeline.fit(X_train, y_train)
```

---

### 5. Manejo de Nombres de Columnas

#### ‚úÖ Columnas con Caracteres Especiales

El dataset tiene columnas con caracteres especiales:
- `CO2(tCO2)` - Par√©ntesis
- `Lagging_Current_Reactive.Power_kVarh` - Punto

**Las funciones en `src/utils/duckdb_utils.py` manejan esto autom√°ticamente**.

**‚úÖ Correcto**:
```python
# Las funciones manejan autom√°ticamente los caracteres especiales
stats = get_stats_by_column('steel_cleaned', 'Load_Type', 'CO2(tCO2)')
corr = get_correlation('steel_cleaned', 'Usage_kWh', 'CO2(tCO2)')
```

**Para queries SQL personalizados**, usar comillas dobles:
```python
df = quick_query('''
    SELECT "CO2(tCO2)" as CO2
    FROM steel_cleaned
''')
```

---

### 6. Testing

#### ‚úÖ Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ unit/              # Tests unitarios
‚îú‚îÄ‚îÄ integration/       # Tests de integraci√≥n
‚îî‚îÄ‚îÄ e2e/              # Tests end-to-end
```

#### ‚úÖ Convenciones de Tests

- **Nombres**: `test_*.py`
- **Clases**: `Test*`
- **Funciones**: `test_*`
- **Coverage m√≠nimo**: >70%

**Ejemplo**:
```python
import pytest

class TestDuckDBUtils:
    """Tests para funciones de DuckDB."""
    
    def test_quick_query_basic(self):
        """Test query b√°sico."""
        df = quick_query("SELECT * FROM steel_cleaned LIMIT 5")
        assert len(df) == 5
```

---

### 7. Versionado de Datos y Modelos

#### ‚úÖ Usar DVC

**NUNCA** commitear archivos grandes a Git. Usar DVC:

```bash
# ‚úÖ Correcto
dvc add data/processed/steel_cleaned.parquet
git add data/processed/steel_cleaned.parquet.dvc
git commit -m "data: add cleaned dataset"

# ‚ùå Incorrecto
git add data/processed/steel_cleaned.parquet
```

#### ‚úÖ Archivos que van con DVC

- ‚úÖ Datasets (CSV, Parquet, etc.)
- ‚úÖ Modelos entrenados (pkl, pth, h5)
- ‚úÖ Archivos >1MB

#### ‚úÖ Archivos que van con Git

- ‚úÖ C√≥digo fuente
- ‚úÖ Configs (<1KB)
- ‚úÖ Documentaci√≥n
- ‚úÖ Tests

---

### 8. MLflow Experiment Tracking

#### ‚úÖ SIEMPRE loggear experimentos

```python
import mlflow

with mlflow.start_run(run_name="xgboost_baseline"):
    # Log parameters
    mlflow.log_params(model.get_params())
    
    # Train
    model.fit(X_train, y_train)
    
    # Log metrics
    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("mae", mae)
    
    # Log model
    mlflow.sklearn.log_model(model, "model")
```

---

### 9. Convenciones de C√≥digo

#### ‚úÖ Formateo

- **Formatter**: Black (line-length=100)
- **Linter**: Ruff
- **Type checker**: MyPy (opcional)

```bash
# Antes de commitear
poetry run black .
poetry run ruff check .
```

#### ‚úÖ Docstrings

**Estilo Google** para todas las funciones:

```python
def get_stats_by_column(
    table_name: str,
    group_by_column: str,
    agg_column: str
) -> pl.DataFrame:
    """
    Obtiene estad√≠sticas agregadas agrupadas por una columna.
    
    Parameters
    ----------
    table_name : str
        Nombre de la tabla
    group_by_column : str
        Columna para agrupar
    agg_column : str
        Columna para agregar
        
    Returns
    -------
    pl.DataFrame
        DataFrame con estad√≠sticas (count, avg, min, max, std)
        
    Examples
    --------
    >>> stats = get_stats_by_column('steel_cleaned', 'Load_Type', 'Usage_kWh')
    >>> print(stats)
    """
```

#### ‚úÖ Type Hints

SIEMPRE usar type hints:

```python
# ‚úÖ Correcto
def process_data(df: pl.DataFrame, threshold: float = 0.5) -> pl.DataFrame:
    pass

# ‚ùå Incorrecto
def process_data(df, threshold=0.5):
    pass
```

---

### 10. Estructura de Notebooks

#### ‚úÖ Orden Est√°ndar

```markdown
# 1. T√≠tulo y Descripci√≥n (en espa√±ol)

## 2. Imports
import sys
sys.path.append('../..')
from src.utils.duckdb_utils import setup_database

## 3. Configuraci√≥n
conn = setup_database(...)

## 4. An√°lisis
### 4.1 Secci√≥n 1
### 4.2 Secci√≥n 2

## 5. Conclusiones (en espa√±ol)

## 6. Limpieza
conn.close()
```

#### ‚úÖ Usar Funciones de Utils

**SIEMPRE** preferir funciones de `src/utils/` sobre c√≥digo inline:

```python
# ‚úÖ Correcto
from src.utils.duckdb_utils import get_stats_by_column
stats = get_stats_by_column('steel_cleaned', 'Load_Type', 'Usage_kWh')

# ‚ùå Incorrecto
stats = conn.execute("""
    SELECT Load_Type, COUNT(*) as count, AVG(Usage_kWh) as avg
    FROM steel_cleaned
    GROUP BY Load_Type
""").pl()
```

---

### 11. Manejo de Errores

#### ‚úÖ Logging en lugar de prints

```python
import logging

logger = logging.getLogger(__name__)

# ‚úÖ Correcto
logger.info("Processing data...")
logger.error(f"Failed to load file: {e}")

# ‚ùå Incorrecto
print("Processing data...")
print(f"Error: {e}")
```

#### ‚úÖ Manejo de Excepciones

```python
# ‚úÖ Correcto
try:
    df = load_data(path)
except FileNotFoundError:
    logger.error(f"File not found: {path}")
    raise
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    raise

# ‚ùå Incorrecto
try:
    df = load_data(path)
except:
    pass
```

---

### 12. Git Commits

#### ‚úÖ Conventional Commits

```bash
# Formato: <type>(<scope>): <description>

# Tipos:
feat:     # Nueva funcionalidad
fix:      # Correcci√≥n de bug
docs:     # Cambios en documentaci√≥n
style:    # Formateo (no afecta c√≥digo)
refactor: # Refactorizaci√≥n
test:     # Agregar/modificar tests
chore:    # Tareas de mantenimiento

# Ejemplos:
git commit -m "feat(duckdb): add setup_database function"
git commit -m "fix(utils): handle special characters in column names"
git commit -m "docs: update AGENTS.md with best practices"
git commit -m "test(duckdb): add tests for correlation function"
```

---

### 13. Dependencias

#### ‚úÖ Gesti√≥n con Poetry

```bash
# Agregar dependencia
poetry add polars

# Agregar dependencia de desarrollo
poetry add --group dev pytest

# Actualizar dependencias
poetry update

# NUNCA editar pyproject.toml manualmente para dependencias
```

---

### 14. Configuraci√≥n de Notebooks

#### ‚úÖ Setup Inicial

**SIEMPRE** incluir al inicio de notebooks:

```python
# Imports
import sys
sys.path.append('../..')  # Para importar desde src/

# Configuraci√≥n de visualizaci√≥n
import warnings
warnings.filterwarnings('ignore')

# Polars config
pl.Config.set_tbl_rows(20)
pl.Config.set_fmt_str_lengths(100)
```

---

### 15. Funciones Reutilizables Disponibles

#### ‚úÖ DuckDB (`src/utils/duckdb_utils.py`)

```python
from src.utils.duckdb_utils import (
    setup_database,              # Setup autom√°tico de DB
    quick_query,                 # Query r√°pido
    get_stats_by_column,         # Estad√≠sticas por columna
    get_temporal_stats,          # An√°lisis temporal
    get_top_n,                   # Top N registros
    get_correlation,             # Correlaciones
    get_weekend_vs_weekday_stats # Comparaci√≥n fin de semana
)
```

#### ‚úÖ Data Cleaning (`src/utils/data_cleaning.py`)

```python
from src.utils.data_cleaning import (
    convert_data_types,          # Conversi√≥n de tipos
    handle_null_values,          # Manejo de nulos
    correct_range_violations,    # Correcci√≥n de rangos
    treat_outliers,              # Tratamiento de outliers
    remove_duplicates            # Eliminaci√≥n de duplicados
)
```

#### ‚úÖ Data Quality (`src/utils/data_quality.py`)

```python
from src.utils.data_quality import (
    compare_schemas,             # Comparar schemas
    analyze_nulls,               # An√°lisis de nulos
    validate_types               # Validaci√≥n de tipos
)
```

---

## üö´ Anti-Patrones (Evitar)

### ‚ùå C√≥digo Duplicado

```python
# ‚ùå NO hacer esto en m√∫ltiples notebooks
conn = duckdb.connect("data/steel.duckdb")
df = conn.execute("SELECT * FROM steel_cleaned").pl()
conn.close()

# ‚úÖ Usar funci√≥n reutilizable
from src.utils.duckdb_utils import quick_query
df = quick_query("SELECT * FROM steel_cleaned")
```

### ‚ùå Hardcoded Paths

```python
# ‚ùå Incorrecto
df = pl.read_csv("C:/Users/arthu/Desktop/data.csv")

# ‚úÖ Correcto
from pathlib import Path
data_dir = Path("data/raw")
df = pl.read_csv(data_dir / "data.csv")
```

### ‚ùå Magic Numbers

```python
# ‚ùå Incorrecto
df = df.filter(pl.col('value') > 0.5)

# ‚úÖ Correcto
THRESHOLD = 0.5
df = df.filter(pl.col('value') > THRESHOLD)
```

### ‚ùå Commits de Archivos Grandes

```python
# ‚ùå NUNCA hacer esto
git add data/large_dataset.csv
git commit -m "add dataset"

# ‚úÖ Usar DVC
dvc add data/large_dataset.csv
git add data/large_dataset.csv.dvc
git commit -m "data: add large dataset with DVC"
```

---

## üìö Referencias R√°pidas

### Estructura del Proyecto

```
mlops_proyecto_atreides/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/           # Scripts de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ features/       # Feature engineering (POO)
‚îÇ   ‚îú‚îÄ‚îÄ models/         # Training y pipelines
‚îÇ   ‚îú‚îÄ‚îÄ api/            # FastAPI backend
‚îÇ   ‚îî‚îÄ‚îÄ utils/          # Funciones reutilizables ‚≠ê
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ exploratory/    # EDA (texto en espa√±ol)
‚îÇ   ‚îî‚îÄ‚îÄ experimental/   # Experimentos
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/          # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ integration/   # Tests de integraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ e2e/           # Tests end-to-end
‚îú‚îÄ‚îÄ data/              # Datos (versionados con DVC)
‚îú‚îÄ‚îÄ models/            # Modelos (versionados con DVC)
‚îî‚îÄ‚îÄ docs/              # Documentaci√≥n
```

### Comandos √ötiles

```bash
# Tests
poetry run pytest tests/ -v
poetry run pytest --cov=src --cov-report=html

# Formateo
poetry run black .
poetry run ruff check .

# DVC
dvc add data/file.csv
dvc push
dvc pull

# MLflow
poetry run mlflow ui

# Git
git status
git add .
git commit -m "type(scope): description"
git push
```

---

## ‚úÖ Checklist para Nuevas Funcionalidades

Antes de considerar una tarea completada:

- [ ] C√≥digo en ingl√©s, documentaci√≥n en espa√±ol
- [ ] Funciones reutilizables en `src/utils/` si aplica
- [ ] Docstrings estilo Google
- [ ] Type hints en todas las funciones
- [ ] Tests unitarios (coverage >70%)
- [ ] Formateado con Black
- [ ] Sin warnings de Ruff
- [ ] Datos/modelos versionados con DVC
- [ ] Experimentos loggeados en MLflow
- [ ] Commit con Conventional Commits
- [ ] Documentaci√≥n actualizada

---

## üéì Principios de Dise√±o

1. **DRY (Don't Repeat Yourself)**: Si lo usas dos veces, crea una funci√≥n
2. **KISS (Keep It Simple, Stupid)**: C√≥digo simple es mejor que c√≥digo complejo
3. **YAGNI (You Aren't Gonna Need It)**: No agregues funcionalidad que no necesitas ahora
4. **Separation of Concerns**: Cada m√≥dulo tiene una responsabilidad clara
5. **Code Reusability**: Funciones en utils, no c√≥digo duplicado en notebooks

---

## üö® C√≥digo Natural - Evitar Marcadores de IA

### ‚ùå NO usar elementos que delatan c√≥digo generado por IA

**Evitar estos patrones t√≠picos de IA**:

```python
# ‚ùå NO usar emojis en comentarios
# üîç Load the data
# ‚úÖ Process the results
# üöÄ Train the model

# ‚ùå NO usar separadores decorativos
print("="*70)
print("-"*50)
print("*"*80)

# ‚ùå NO usar comentarios excesivamente descriptivos
# Step 1: Initialize the connection
# Step 2: Load the data
# Step 3: Process the results

# ‚ùå NO usar banners ASCII
# ============================================
# SECTION: DATA LOADING
# ============================================
```

**‚úÖ Usar c√≥digo natural y profesional**:

```python
# Load and validate data
df = pl.read_parquet("data/processed/steel_cleaned.parquet")

# Calculate summary statistics
summary = df.describe()

# Train model with cross-validation
model.fit(X_train, y_train)
```

### ‚úÖ Estilo de Comentarios Profesional

```python
# ‚úÖ Correcto - Comentarios concisos y t√©cnicos
# Filter outliers using IQR method
df_clean = df.filter((pl.col('value') > q1) & (pl.col('value') < q3))

# Calculate rolling average for smoothing
df = df.with_columns(pl.col('usage').rolling_mean(window_size=7))

# ‚ùå Incorrecto - Comentarios obvios o decorativos
# Now we will filter the outliers from our dataset using the IQR method
# This is an important step in our data cleaning process
df_clean = df.filter((pl.col('value') > q1) & (pl.col('value') < q3))
```

### ‚úÖ Output y Logging Natural

```python
# ‚úÖ Correcto - Mensajes informativos simples
logger.info(f"Loaded {len(df)} records")
logger.info(f"Training completed in {elapsed:.2f}s")

# ‚ùå Incorrecto - Mensajes decorativos
logger.info("="*70)
logger.info("üéØ Starting training process...")
logger.info("="*70)
```

### ‚úÖ Separaci√≥n de Secciones en Notebooks

```python
# ‚úÖ Correcto - Usar markdown cells para secciones
# En Markdown cell:
## 2. Data Loading

# En Code cell:
df = pl.read_parquet("data/processed/steel_cleaned.parquet")

# ‚ùå Incorrecto - Separadores en c√≥digo
print("\n" + "="*70)
print("SECTION 2: DATA LOADING")
print("="*70 + "\n")
df = pl.read_parquet("data/processed/steel_cleaned.parquet")
```

### Reglas Generales

1. **Sin emojis** en c√≥digo o comentarios
2. **Sin separadores decorativos** (=, -, *)
3. **Comentarios concisos** y t√©cnicos, no narrativos
4. **Mensajes de log simples** y directos
5. **Usar markdown cells** para estructura en notebooks, no prints decorativos

---

**Versi√≥n**: 1.0  
**√öltima actualizaci√≥n**: 2025-10-16  
**Mantenido por**: MLOps Team - Proyecto Atreides

---

## üìû Contacto

Si tienes dudas sobre estas pr√°cticas, consulta:
- `docs/` - Documentaci√≥n del proyecto
- `README.md` - Gu√≠a de inicio
- `STRUCTURE.md` - Estructura del proyecto
