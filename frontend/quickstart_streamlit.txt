# âš¡ Quick Start - Streamlit UI

GuÃ­a rÃ¡pida para levantar la UI de Streamlit en **menos de 5 minutos**.

## ğŸ¯ OpciÃ³n 1: Desarrollo Local (MÃ¡s RÃ¡pido)

### Paso 1: Instalar Dependencias

```bash
# Con pip
pip install streamlit plotly requests pandas

# O instalar todas las dependencias del proyecto
pip install -r requirements.txt
```

### Paso 2: Configurar Secrets

Crear archivo `.streamlit/secrets.toml`:

```bash
# Crear directorio
mkdir -p .streamlit

# Crear archivo de secrets
cat > .streamlit/secrets.toml << EOF
API_URL = "http://localhost:8000"
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:3b"
EOF
```

### Paso 3: Ejecutar App

```bash
streamlit run streamlit_app.py
```

âœ… Abre tu navegador en: http://localhost:8501

---

## ğŸ³ OpciÃ³n 2: Docker (Todo-en-Uno)

### Prerequisito: Docker instalado

```bash
# Verificar Docker
docker --version
```

### Ejecutar con Docker Compose

```bash
# Iniciar todos los servicios (UI + API + Ollama + MLflow)
docker-compose -f docker-compose.streamlit.yml up -d

# Ver logs
docker-compose -f docker-compose.streamlit.yml logs -f

# Acceder a:
# - Streamlit UI: http://localhost:8501
# - API: http://localhost:8000
# - MLflow: http://localhost:5000
```

### Detener servicios

```bash
docker-compose -f docker-compose.streamlit.yml down
```

---

## â˜ï¸ OpciÃ³n 3: Streamlit Cloud (Gratis)

### Paso 1: Push a GitHub

```bash
git add .
git commit -m "feat: add streamlit ui"
git push origin main
```

### Paso 2: Deployar

1. Ir a [share.streamlit.io](https://share.streamlit.io/)
2. Conectar con GitHub
3. Seleccionar repo: `DanteA0179/mlops_proyecto_atreides`
4. Main file: `streamlit_app.py`
5. Click **Deploy**

### Paso 3: Configurar Secrets

En Streamlit Cloud â†’ Settings â†’ Secrets:

```toml
API_URL = "https://your-api-url.run.app"
OLLAMA_HOST = "http://localhost:11434"
```

âœ… Tu app estarÃ¡ en: `https://[usuario]-mlops-energy.streamlit.app`

---

## ğŸ§ª Verificar InstalaciÃ³n

### Test 1: Verificar Streamlit

```bash
streamlit --version
# Debe mostrar: Streamlit, version 1.29.0 o superior
```

### Test 2: Verificar PÃ¡gina Principal

```bash
# Ejecutar app
streamlit run streamlit_app.py

# Abrir en navegador: http://localhost:8501
# Debe mostrar pÃ¡gina de bienvenida
```

### Test 3: Verificar API (Opcional)

```bash
# Verificar que la API estÃ© corriendo
curl http://localhost:8000/health

# Respuesta esperada: {"status": "healthy"}
```

### Test 4: Verificar Ollama (Opcional)

```bash
# Verificar Ollama
curl http://localhost:11434/api/tags

# Si no estÃ¡ instalado:
# 1. Instalar: curl -fsSL https://ollama.com/install.sh | sh
# 2. Iniciar: ollama serve
# 3. Descargar modelo: ollama pull llama3.2:3b
```

---

## ğŸ® Primeros Pasos

### 1. Explorar la UI

- **ğŸ  Home**: InformaciÃ³n del proyecto
- **ğŸ”® PredicciÃ³n**: Hacer predicciones de consumo
- **ğŸ’¬ Copiloto IA**: Chatear con el asistente

### 2. Hacer tu Primera PredicciÃ³n

1. Ir a **"ğŸ”® PredicciÃ³n"**
2. Ingresar valores:
   - Potencia Reactiva: 10.0 kVarh
   - CO2: 0.5 tCO2
   - Factor de Potencia: 0.85
3. Seleccionar hora y dÃ­a
4. Click en **"ğŸš€ Realizar PredicciÃ³n"**
5. Ver resultados y visualizaciones

### 3. Usar el Copiloto IA

1. Ir a **"ğŸ’¬ Copiloto IA"**
2. Hacer una pregunta o usar preguntas sugeridas:
   - "Â¿CÃ³mo puedo reducir mi consumo energÃ©tico?"
   - "Â¿QuÃ© significa el factor de potencia?"
3. Ver respuesta del asistente

---

## ğŸ“ Archivos Necesarios

Estructura mÃ­nima para que la app funcione:

```
mlops_proyecto_atreides/
â”œâ”€â”€ streamlit_app.py              âœ… App principal
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 1_ğŸ”®_PredicciÃ³n.py       âœ… PÃ¡gina de predicciÃ³n
â”‚   â””â”€â”€ 2_ğŸ’¬_Copiloto_IA.py      âœ… Copiloto IA
â”œâ”€â”€ .streamlit/
â”‚   â”œâ”€â”€ config.toml               âš™ï¸ ConfiguraciÃ³n
â”‚   â””â”€â”€ secrets.toml              ğŸ” Secretos (crear)
â””â”€â”€ requirements.txt              ğŸ“¦ Dependencias
```

---

## âš™ï¸ ConfiguraciÃ³n MÃ­nima

### requirements.txt

```txt
streamlit>=1.29.0
plotly>=5.18.0
pandas>=2.0.0
requests>=2.31.0
```

### .streamlit/config.toml

```toml
[server]
port = 8501
address = "0.0.0.0"

[theme]
primaryColor = "#667eea"
```

### .streamlit/secrets.toml

```toml
API_URL = "http://localhost:8000"
OLLAMA_HOST = "http://localhost:11434"
```

---

## ğŸ”§ Troubleshooting RÃ¡pido

### Error: "ModuleNotFoundError: No module named 'streamlit'"

```bash
pip install streamlit plotly requests pandas
```

### Error: "Port 8501 is already in use"

```bash
# Usar puerto diferente
streamlit run streamlit_app.py --server.port 8502
```

### Error: "API not available"

La app funcionarÃ¡ en **modo simulado** si la API no estÃ¡ disponible.  
Las predicciones se harÃ¡n con heurÃ­sticas simples.

### Error: "Ollama not connected"

El copiloto funcionarÃ¡ en **modo simulado** con respuestas predefinidas.  
Para usar Ollama real:

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Iniciar servicio
ollama serve

# Descargar modelo
ollama pull llama3.2:3b
```

---

## ğŸ“ PrÃ³ximos Pasos

1. **Personalizar la UI**: Editar colores, agregar secciones
2. **Conectar con tu API**: Actualizar `API_URL` en secrets
3. **Deployar a producciÃ³n**: Seguir [STREAMLIT_DEPLOYMENT.md](docs/STREAMLIT_DEPLOYMENT.md)
4. **Optimizar performance**: Usar cachÃ© de Streamlit
5. **Agregar features**: Nuevas visualizaciones, anÃ¡lisis

---

## ğŸ“š DocumentaciÃ³n Completa

- ğŸ“– [UI_README.md](docs/UI_README.md) - DocumentaciÃ³n detallada
- ğŸš€ [STREAMLIT_DEPLOYMENT.md](docs/STREAMLIT_DEPLOYMENT.md) - GuÃ­a de deployment
- ğŸ  [README.md](README.md) - DocumentaciÃ³n del proyecto

---

## ğŸ’¡ Tips

âœ… **Para desarrollo**: Usa modo local con hot reload  
âœ… **Para demos**: Deploya en Streamlit Cloud (gratis)  
âœ… **Para producciÃ³n**: Usa Google Cloud Run  

---

## â“ Â¿Necesitas Ayuda?

- ğŸ“˜ [DocumentaciÃ³n de Streamlit](https://docs.streamlit.io/)
- ğŸ› [Abrir Issue en GitHub](https://github.com/DanteA0179/mlops_proyecto_atreides/issues)
- ğŸ’¬ Contactar al equipo Atreides

---

**Tiempo total de setup**: â±ï¸ < 5 minutos  
**Ãšltima actualizaciÃ³n**: Noviembre 2025
