# Ensemble LightGBM Training Configuration
model:
  type: ensemble_lightgbm
  version: v1.0

# Data paths (preprocessed from US-012)
data:
  train: data/processed/steel_preprocessed_train.parquet
  val: data/processed/steel_preprocessed_val.parquet
  test: data/processed/steel_preprocessed_test.parquet
  target_col: Usage_kWh

# Base models for ensemble
base_models:
  - type: xgboost
    hyperparameters:
      max_depth: 8
      learning_rate: 0.0395
      n_estimators: 151
      tree_method: gpu_hist
      device: cuda
      random_state: 42
  - type: lightgbm
    hyperparameters:
      num_leaves: 31
      learning_rate: 0.05
      n_estimators: 150
      device: gpu
      random_state: 42
  - type: catboost
    hyperparameters:
      depth: 8
      learning_rate: 0.05
      iterations: 150
      task_type: GPU
      random_seed: 42

# Meta-model (LightGBM)
hyperparameters:
  num_leaves: 15
  max_depth: 5
  learning_rate: 0.03
  n_estimators: 100
  device: gpu
  random_state: 42
  verbose: -1

# Performance thresholds
thresholds:
  rmse: 14.0   # kWh (more strict for ensemble)
  r2: 0.82     # 82% variance explained
  mae: 3.8     # kWh

# MLflow configuration
mlflow:
  experiment_name: steel_energy_training_pipeline
  tracking_uri: http://localhost:5000
  tags:
    model_type: ensemble_lightgbm
    pipeline_version: v1.0
    optimization: ensemble_stacking

# DVC configuration
dvc:
  remote: gcs_remote
  auto_push: true
  skip_push: false

# Prefect configuration
prefect:
  flow_name: training-pipeline-ensemble-lightgbm
  retries: 3
  timeout_seconds: 2700  # 45 minutes (longer for ensemble)

# Notification configuration
notifications:
  channels:
    - console
    - file
  slack:
    enabled: false
    webhook_url: null
  email:
    enabled: false
    recipients: []
