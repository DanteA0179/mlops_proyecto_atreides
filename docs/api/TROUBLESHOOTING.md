# Energy Optimization API - Troubleshooting

Gu√≠a completa para solucionar problemas comunes con la API.

---

## üìã Tabla de Contenidos

1. [Errores de Conexi√≥n](#errores-de-conexi√≥n)
2. [Errores de Validaci√≥n (422)](#errores-de-validaci√≥n-422)
3. [Errores del Servidor (500)](#errores-del-servidor-500)
4. [Problemas de Performance](#problemas-de-performance)
5. [Problemas con el Modelo](#problemas-con-el-modelo)
6. [Debugging](#debugging)
7. [FAQ](#faq)

---

## üîå Errores de Conexi√≥n

### Connection Refused

**S√≠ntoma**:
```
requests.exceptions.ConnectionError: Connection refused
```

**Causas Posibles**:
1. Servidor no est√° corriendo
2. Puerto incorrecto
3. Firewall bloqueando conexi√≥n

**Soluciones**:

#### Verificar que el servidor est√° corriendo
```bash
# Check proceso
ps aux | grep uvicorn

# Check puerto
netstat -tuln | grep 8000
```

#### Iniciar el servidor
```bash
# Opci√≥n 1: Directo con Python
python src/api/main.py

# Opci√≥n 2: Con uvicorn
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

# Opci√≥n 3: Con Docker
docker-compose up
```

#### Verificar puerto correcto
```python
# Asegurarse de usar el puerto correcto
API_URL = "http://localhost:8000"  # ‚úÖ
# No usar:
API_URL = "http://localhost:5000"  # ‚ùå
```

---

### Timeout

**S√≠ntoma**:
```
requests.exceptions.Timeout: Request timed out
```

**Causas Posibles**:
1. Servidor sobrecargado
2. Modelo muy grande
3. Batch demasiado grande
4. Red lenta

**Soluciones**:

#### Aumentar timeout
```python
import requests

response = requests.post(
    f"{API_URL}/predict",
    json=data,
    timeout=30  # Aumentar de 5 a 30 segundos
)
```

#### Reducir tama√±o del batch
```python
# ‚ùå MALO: Batch muy grande
batch = {"predictions": [pred] * 1000}  # 1000 predicciones

# ‚úÖ BUENO: Batch m√°s peque√±o
batch = {"predictions": [pred] * 100}  # 100 predicciones
```

#### Verificar salud del servidor
```bash
curl http://localhost:8000/health
```

---

## ‚ö†Ô∏è Errores de Validaci√≥n (422)

### Invalid load_type

**Error**:
```json
{
  "detail": [
    {
      "loc": ["body", "load_type"],
      "msg": "load_type must be one of ['Light', 'Medium', 'Maximum']",
      "type": "value_error"
    }
  ]
}
```

**Soluci√≥n**:
```python
# ‚ùå INCORRECTO
data = {"load_type": "medium"}  # Min√∫sculas
data = {"load_type": "HIGH"}    # Valor inv√°lido

# ‚úÖ CORRECTO
data = {"load_type": "Medium"}  # Capitalizado correcto
data = {"load_type": "Light"}   # Otro valor v√°lido
data = {"load_type": "Maximum"} # Otro valor v√°lido
```

---

### Invalid power_factor Range

**Error**:
```json
{
  "detail": [
    {
      "loc": ["body", "lagging_power_factor"],
      "msg": "ensure this value is less than or equal to 1.0",
      "type": "value_error"
    }
  ]
}
```

**Soluci√≥n**:
```python
# ‚ùå INCORRECTO
data = {
    "lagging_power_factor": 1.5,  # > 1.0
    "leading_power_factor": -0.5  # < 0.0
}

# ‚úÖ CORRECTO
data = {
    "lagging_power_factor": 0.85,  # 0.0 <= x <= 1.0
    "leading_power_factor": 0.92   # 0.0 <= x <= 1.0
}
```

---

### Invalid nsm (Seconds from Midnight)

**Error**:
```json
{
  "detail": [
    {
      "loc": ["body", "nsm"],
      "msg": "ensure this value is less than or equal to 86400",
      "type": "value_error"
    }
  ]
}
```

**Soluci√≥n**:
```python
# ‚ùå INCORRECTO
data = {"nsm": 90000}  # > 86400 (m√°s de 24 horas)

# ‚úÖ CORRECTO
# Convertir hora a segundos
hour = 10  # 10:00 AM
minute = 30
nsm = (hour * 3600) + (minute * 60)  # 37800 segundos

data = {"nsm": nsm}  # 0 <= nsm <= 86400
```

**Helper Function**:
```python
def time_to_nsm(hour, minute=0, second=0):
    """Convert time to NSM (Number of Seconds from Midnight)."""
    return (hour * 3600) + (minute * 60) + second

# Usage
data = {"nsm": time_to_nsm(10, 30)}  # 10:30 AM
```

---

### Invalid day_of_week

**Error**:
```json
{
  "detail": [
    {
      "loc": ["body", "day_of_week"],
      "msg": "ensure this value is less than or equal to 6",
      "type": "value_error"
    }
  ]
}
```

**Soluci√≥n**:
```python
# ‚ùå INCORRECTO
data = {"day_of_week": 7}  # Solo 0-6

# ‚úÖ CORRECTO
# 0 = Lunes, 1 = Martes, ..., 6 = Domingo
data = {"day_of_week": 0}  # Lunes
data = {"day_of_week": 5}  # S√°bado
```

**Helper Function**:
```python
from datetime import datetime

def get_day_of_week():
    """Get current day of week (0=Monday)."""
    return datetime.now().weekday()

# Usage
data = {"day_of_week": get_day_of_week()}
```

---

### Negative Values

**Error**:
```json
{
  "detail": [
    {
      "loc": ["body", "co2"],
      "msg": "ensure this value is greater than or equal to 0",
      "type": "value_error"
    }
  ]
}
```

**Soluci√≥n**:
```python
# ‚ùå INCORRECTO
data = {
    "lagging_reactive_power": -5.0,  # Negativo
    "co2": -0.01                     # Negativo
}

# ‚úÖ CORRECTO
data = {
    "lagging_reactive_power": 23.45,  # >= 0
    "co2": 0.05                       # >= 0
}
```

---

### Batch Empty or Too Large

**Error**:
```json
{
  "detail": "Batch cannot be empty"
}
```

o

```json
{
  "detail": "Batch cannot exceed 1000 predictions"
}
```

**Soluci√≥n**:
```python
# ‚ùå INCORRECTO
batch = {"predictions": []}  # Vac√≠o
batch = {"predictions": [pred] * 2000}  # Muy grande

# ‚úÖ CORRECTO
batch = {"predictions": [pred1, pred2]}  # 1-1000 items
```

---

## üî• Errores del Servidor (500)

### Internal Server Error

**S√≠ntoma**:
```json
{
  "detail": "Prediction failed: Model not found"
}
```

**Causas Posibles**:
1. Modelo no cargado
2. Archivo del modelo corrupto
3. Dependencias faltantes
4. Error en feature engineering

**Soluciones**:

#### 1. Verificar Health Check
```bash
curl http://localhost:8000/health
```

Buscar:
```json
{
  "status": "unhealthy",
  "model_loaded": false
}
```

#### 2. Verificar logs del servidor
```bash
# Ver logs en tiempo real
tail -f logs/api.log

# Buscar errores
grep "ERROR" logs/api.log
```

#### 3. Verificar que el modelo existe
```bash
# Listar modelos disponibles
ls -la models/

# Verificar tama√±o del archivo
du -h models/stacking_ensemble.pkl
```

#### 4. Reinstalar dependencias
```bash
poetry install
# o
pip install -r requirements.txt
```

#### 5. Reiniciar el servidor
```bash
# Matar proceso
pkill -f uvicorn

# Reiniciar
python src/api/main.py
```

---

### Model Loading Failed

**Error en logs**:
```
ERROR: Failed to load model: No such file or directory
```

**Soluci√≥n**:

#### Descargar modelo con DVC
```bash
dvc pull models/stacking_ensemble.pkl.dvc
```

#### Verificar configuraci√≥n
```python
# src/api/utils/config.py
MODEL_PATH = Path("models")
MODEL_TYPE = "stacking_ensemble"
```

#### Re-entrenar modelo
```bash
python src/models/train_stacking_ensemble.py
```

---

## üêå Problemas de Performance

### Latencia Alta

**S√≠ntoma**: Respuestas > 200ms

**Diagn√≥stico**:
```bash
curl http://localhost:8000/model/metrics
```

Revisar `p95_prediction_time_ms`.

**Causas y Soluciones**:

#### 1. Batch muy grande
```python
# ‚ùå MALO
batch = {"predictions": [pred] * 1000}  # Muy grande

# ‚úÖ BUENO
batch = {"predictions": [pred] * 100}  # M√°s manejable
```

#### 2. Servidor sobrecargado
```bash
# Ver uso de recursos
curl http://localhost:8000/health | jq '.memory_usage_mb, .cpu_usage_percent'
```

**Soluci√≥n**: Escalar horizontalmente o agregar m√°s recursos.

#### 3. Modelo no optimizado
- Usar modelo m√°s r√°pido (LightGBM en lugar de ensemble)
- Reducir n√∫mero de features
- Cuantizar modelo

---

### Memory Usage Alto

**S√≠ntoma**: `memory_usage_mb > 1000`

**Soluciones**:

#### 1. Reiniciar servidor peri√≥dicamente
```bash
# Cron job para reiniciar diariamente
0 3 * * * systemctl restart energy-api
```

#### 2. Limitar tama√±o de batch
```python
MAX_BATCH_SIZE = 500  # En lugar de 1000
```

#### 3. Optimizar modelo
- Usar formato ONNX
- Comprimir modelo
- Usar modelo m√°s peque√±o

---

## ü§ñ Problemas con el Modelo

### Model Not Loaded

**S√≠ntoma**:
```json
{
  "status": "unhealthy",
  "model_loaded": false
}
```

**Soluciones**:

1. **Verificar modelo existe**:
```bash
ls -la models/stacking_ensemble.pkl
```

2. **Descargar con DVC**:
```bash
dvc pull
```

3. **Verificar permisos**:
```bash
chmod 644 models/stacking_ensemble.pkl
```

4. **Reiniciar servidor**:
```bash
python src/api/main.py
```

---

### Predictions Not Matching

**S√≠ntoma**: Predicciones inconsistentes con valores esperados

**Diagn√≥stico**:

#### 1. Verificar versi√≥n del modelo
```bash
curl http://localhost:8000/model/info | jq '.model_version'
```

#### 2. Verificar m√©tricas de entrenamiento
```bash
curl http://localhost:8000/model/info | jq '.training_metrics'
```

#### 3. Verificar features
```python
# Asegurarse de usar todas las features requeridas
response = requests.get(f"{API_URL}/model/info")
required_features = [f["name"] for f in response.json()["features"]]
print(required_features)
```

---

## üêõ Debugging

### Modo Debug

#### Habilitar logs detallados
```bash
# Modificar config
export LOG_LEVEL=DEBUG

# Reiniciar servidor
python src/api/main.py
```

#### Ver logs en tiempo real
```bash
tail -f logs/api.log
```

### Request/Response Logging

```python
import requests
import logging

# Habilitar logging de requests
logging.basicConfig(level=logging.DEBUG)

response = requests.post(f"{API_URL}/predict", json=data)
```

### Validaci√≥n Manual

```python
from src.api.models.requests import PredictionRequest
from pydantic import ValidationError

# Validar datos antes de enviar
try:
    request = PredictionRequest(**data)
    print("‚úÖ Datos v√°lidos")
except ValidationError as e:
    print("‚ùå Errores de validaci√≥n:")
    print(e.json())
```

---

## ‚ùì FAQ

### ¬øPor qu√© mi predicci√≥n es muy diferente de lo esperado?

1. Verificar que los datos de entrada son correctos
2. Verificar unidades (kVarh, tCO2, etc.)
3. Verificar que `load_type` es apropiado
4. Comparar con intervalos de confianza

### ¬øC√≥mo s√© qu√© versi√≥n del modelo est√° activa?

```bash
curl http://localhost:8000/model/info | jq '.model_version'
```

### ¬øPuedo usar la API sin internet?

S√≠, la API funciona localmente sin conexi√≥n a internet una vez que:
- El c√≥digo est√° clonado
- Las dependencias est√°n instaladas
- El modelo est√° descargado con DVC

### ¬øQu√© hacer si el servidor crashea constantemente?

1. Verificar logs: `tail -f logs/api.log`
2. Verificar memoria disponible
3. Reducir `MAX_BATCH_SIZE`
4. Verificar que el modelo no est√° corrupto
5. Reinstalar dependencias

### ¬øC√≥mo actualizar a una nueva versi√≥n del modelo?

```bash
# Pull nueva versi√≥n con DVC
dvc pull models/new_model.pkl.dvc

# Actualizar configuraci√≥n
export MODEL_TYPE=new_model

# Reiniciar servidor
python src/api/main.py
```

---

## üìû Soporte

Si ninguna de estas soluciones funciona:

1. **Verificar GitHub Issues**: [Issues](https://github.com/DanteA0179/mlops_proyecto_atreides/issues)
2. **Crear nuevo issue** con:
   - Descripci√≥n del problema
   - Logs relevantes
   - Pasos para reproducir
   - Versi√≥n de la API
3. **Email**: mlops@atreides.com

---

## üìö Recursos Adicionales

- [Documentaci√≥n Completa](./API_DOCUMENTATION.md)
- [Gu√≠a de Inicio R√°pido](./QUICK_START.md)
- [Ejemplos de C√≥digo](./EXAMPLES.md)
- [Swagger UI](http://localhost:8000/docs)

---

**√öltima actualizaci√≥n**: 16 de Noviembre, 2025
