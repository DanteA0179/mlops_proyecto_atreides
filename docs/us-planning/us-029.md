# US-029: Setup Ollama y Llama 3.2 - Plan de ImplementaciÃ³n

**Estado**: ğŸ“‹ EN PLANIFICACIÃ“N
**Fecha de CreaciÃ³n**: 17 de Noviembre, 2025
**Responsable**: Arthur (AI Engineer / MLOps)
**Sprint**: Sprint 3 - Copilot & Deployment
**Tipo**: Infrastructure + AI/LLM Setup

---

## ğŸ“‹ User Story

**Como**: AI Engineer
**Quiero**: Correr un LLM local en la GPU
**Para que**: Podamos procesar consultas en lenguaje natural

---

## âœ… Criterios de AceptaciÃ³n

### Funcionales
- [x] Ollama instalado en mÃ¡quina de Arthur âœ… (Ya instalado)
- [x] Llama 3.2 3B descargado y funcionando âœ… (Ya descargado)
- [ ] API de Ollama corriendo en puerto 11434 âœ… (Ya funcionando - verificado)
- [ ] Latencia de respuesta < 2 segundos
- [ ] Uso de VRAM < 8GB

### No Funcionales
- [ ] DocumentaciÃ³n de setup y configuraciÃ³n
- [ ] Scripts de validaciÃ³n y testing
- [ ] IntegraciÃ³n con variables de entorno del proyecto
- [ ] Benchmarks de performance (latencia, throughput, VRAM)
- [ ] Plan de fallback a Gemini 2.0 Flash para producciÃ³n


---

## ğŸ¯ Objetivos de la ImplementaciÃ³n

### Objetivo Principal
Validar y documentar el setup de Ollama + Llama 3.2 3B para procesamiento local de consultas en lenguaje natural, con un plan de migraciÃ³n a Gemini 2.0 Flash para producciÃ³n.

### Objetivos Secundarios
1. **Benchmarking completo**: Medir latencia, throughput y uso de recursos
2. **IntegraciÃ³n con proyecto**: Configurar variables de entorno y mÃ³dulos Python
3. **DocumentaciÃ³n**: GuÃ­a completa de setup, uso y troubleshooting
4. **Testing**: Scripts de validaciÃ³n automatizados
5. **Estrategia dual**: Plan para usar Ollama en desarrollo y Gemini en producciÃ³n

---

## ğŸ—ï¸ Arquitectura Propuesta

### Arquitectura Local (Desarrollo)

```
Developer Machine (Windows 11 + RTX 4070)
    â†“
Ollama Service (localhost:11434)
    â†“
Llama 3.2 3B Model (VRAM: ~4-6GB)
    â†“
FastAPI Backend (src/api/copilot.py)
    â†“
LangChain Integration
```

### Arquitectura ProducciÃ³n (GCP)

```
Cloud Run (FastAPI)
    â†“
Gemini 2.0 Flash API
    â†“
Google AI Platform
    â†“
Response to User
```

### Estrategia Dual

**Desarrollo/Testing (Local)**:
- Ollama + Llama 3.2 3B
- Sin costos
- Latencia: ~1-2s
- VRAM: 4-6GB
- Ideal para iteraciÃ³n rÃ¡pida

**ProducciÃ³n (GCP)**:
- Gemini 2.0 Flash
- Costo: ~$0.00001/token
- Latencia: ~500ms
- Sin infraestructura GPU
- Scale-to-zero compatible


---

## ğŸ“Š Plan de ImplementaciÃ³n

### Fase 1: ValidaciÃ³n de Setup Actual âœ… (COMPLETADO)

**Objetivo**: Verificar que Ollama y Llama 3.2 estÃ¡n funcionando correctamente.

**Tareas**:
- [x] Verificar instalaciÃ³n de Ollama
- [x] Verificar descarga de modelo Llama 3.2 3B
- [x] Test bÃ¡sico de API con curl/PowerShell
- [x] Validar respuesta correcta

**Resultado**: âœ… Ollama funcionando correctamente en puerto 11434

---

### Fase 2: Benchmarking y MediciÃ³n de Performance

**Objetivo**: Medir latencia, throughput y uso de recursos para validar criterios de aceptaciÃ³n.

**Tareas**:
1. **Script de Benchmarking** (`scripts/benchmark_ollama.py`)
   - Medir latencia promedio, p50, p95, p99
   - Medir throughput (tokens/segundo)
   - Medir uso de VRAM durante inferencia
   - Probar con diferentes tamaÃ±os de prompt
   - Generar reporte JSON con mÃ©tricas

2. **Casos de Prueba**:
   - Prompt corto (50 tokens): "Â¿QuÃ© es el consumo energÃ©tico?"
   - Prompt medio (200 tokens): AnÃ¡lisis de datos de consumo
   - Prompt largo (500 tokens): ExplicaciÃ³n detallada con contexto

3. **MÃ©tricas a Capturar**:
   - `latency_ms`: Tiempo de respuesta total
   - `tokens_per_second`: Velocidad de generaciÃ³n
   - `vram_usage_mb`: Uso de memoria GPU
   - `prompt_eval_duration`: Tiempo de procesamiento del prompt
   - `eval_duration`: Tiempo de generaciÃ³n de respuesta

**Entregables**:
- `scripts/benchmark_ollama.py` (~200 lÃ­neas)
- `reports/ollama_benchmark_results.json`
- ValidaciÃ³n de criterios: latencia < 2s, VRAM < 8GB


---

### Fase 3: IntegraciÃ³n con Proyecto

**Objetivo**: Integrar Ollama en la estructura del proyecto con mÃ³dulos reutilizables.

**Tareas**:

1. **MÃ³dulo de Cliente Ollama** (`src/llm/ollama_client.py`)
   - Clase `OllamaClient` con mÃ©todos:
     - `generate()`: GeneraciÃ³n de texto
     - `chat()`: ConversaciÃ³n con contexto
     - `embed()`: Embeddings (futuro)
   - Manejo de errores y timeouts
   - Logging estructurado
   - Type hints completos
   - Docstrings estilo Google

2. **ConfiguraciÃ³n** (`.env` y `config/llm_config.yaml`)
   - Variables de entorno:
     - `OLLAMA_HOST=http://localhost:11434`
     - `OLLAMA_MODEL=llama3.2:3b`
     - `OLLAMA_TIMEOUT=120`
     - `OLLAMA_TEMPERATURE=0.7`
     - `OLLAMA_MAX_TOKENS=1000`
   - ConfiguraciÃ³n YAML para prompts y templates

3. **Utilidades** (`src/llm/utils.py`)
   - `format_prompt()`: Formateo de prompts con templates
   - `parse_response()`: Parsing de respuestas
   - `validate_model()`: ValidaciÃ³n de modelo disponible
   - `get_model_info()`: InformaciÃ³n del modelo

**Entregables**:
- `src/llm/__init__.py` (~30 lÃ­neas)
- `src/llm/ollama_client.py` (~250 lÃ­neas)
- `src/llm/utils.py` (~150 lÃ­neas)
- `config/llm_config.yaml` (~50 lÃ­neas)
- ActualizaciÃ³n de `.env.example`


---

### Fase 4: Testing y ValidaciÃ³n

**Objetivo**: Crear suite de tests para validar funcionalidad del cliente Ollama.

**Tareas**:

1. **Tests Unitarios** (`tests/unit/test_ollama_client.py`)
   - Test de inicializaciÃ³n del cliente
   - Test de generaciÃ³n de texto
   - Test de manejo de errores
   - Test de timeouts
   - Mocks para evitar llamadas reales a Ollama

2. **Tests de IntegraciÃ³n** (`tests/integration/test_ollama_integration.py`)
   - Test de conexiÃ³n real a Ollama
   - Test de generaciÃ³n con diferentes prompts
   - Test de latencia
   - Test de uso de memoria

3. **Script de ValidaciÃ³n** (`scripts/validate_ollama_setup.py`)
   - Verificar que Ollama estÃ¡ corriendo
   - Verificar que el modelo estÃ¡ disponible
   - Ejecutar tests bÃ¡sicos de funcionalidad
   - Generar reporte de validaciÃ³n

**Entregables**:
- `tests/unit/test_ollama_client.py` (~200 lÃ­neas)
- `tests/integration/test_ollama_integration.py` (~150 lÃ­neas)
- `scripts/validate_ollama_setup.py` (~180 lÃ­neas)
- Coverage > 70% en mÃ³dulo `src/llm/`


---

### Fase 5: DocumentaciÃ³n Completa

**Objetivo**: Documentar setup, uso y troubleshooting de Ollama.

**Tareas**:

1. **GuÃ­a de Setup** (`docs/guides/OLLAMA_SETUP.md`)
   - InstalaciÃ³n de Ollama en Windows
   - Descarga de modelos
   - ConfiguraciÃ³n de variables de entorno
   - VerificaciÃ³n de instalaciÃ³n
   - Troubleshooting comÃºn

2. **GuÃ­a de Uso** (`docs/guides/OLLAMA_USAGE.md`)
   - Uso del cliente Python
   - Ejemplos de cÃ³digo
   - Mejores prÃ¡cticas
   - OptimizaciÃ³n de prompts
   - Manejo de errores

3. **Comparativa Ollama vs Gemini** (`docs/guides/LLM_STRATEGY.md`)
   - Tabla comparativa de features
   - AnÃ¡lisis de costos
   - Recomendaciones por ambiente
   - Plan de migraciÃ³n a producciÃ³n

**Entregables**:
- `docs/guides/OLLAMA_SETUP.md` (~400 lÃ­neas)
- `docs/guides/OLLAMA_USAGE.md` (~350 lÃ­neas)
- `docs/guides/LLM_STRATEGY.md` (~300 lÃ­neas)


---

### Fase 6: PreparaciÃ³n para Gemini 2.0 Flash (ProducciÃ³n)

**Objetivo**: Preparar infraestructura para usar Gemini en producciÃ³n.

**Tareas**:

1. **Cliente Gemini** (`src/llm/gemini_client.py`)
   - Clase `GeminiClient` con misma interfaz que `OllamaClient`
   - IntegraciÃ³n con Google AI Platform
   - Manejo de API keys
   - Rate limiting y retry logic

2. **Factory Pattern** (`src/llm/llm_factory.py`)
   - Factory para crear cliente segÃºn ambiente
   - `get_llm_client(provider="ollama")` â†’ OllamaClient
   - `get_llm_client(provider="gemini")` â†’ GeminiClient
   - ConfiguraciÃ³n vÃ­a variable de entorno `LLM_PROVIDER`

3. **ConfiguraciÃ³n de ProducciÃ³n**
   - Variables de entorno para Gemini:
     - `GEMINI_API_KEY`
     - `GEMINI_MODEL=gemini-2.0-flash`
     - `GEMINI_PROJECT_ID`
   - ConfiguraciÃ³n en Cloud Run

**Entregables**:
- `src/llm/gemini_client.py` (~300 lÃ­neas)
- `src/llm/llm_factory.py` (~100 lÃ­neas)
- `config/llm_config.yaml` actualizado
- DocumentaciÃ³n de setup de Gemini


---

## ğŸ“ Estructura de Archivos a Crear

```
src/llm/
â”œâ”€â”€ __init__.py                    (~30 lÃ­neas)
â”œâ”€â”€ ollama_client.py               (~250 lÃ­neas)
â”œâ”€â”€ gemini_client.py               (~300 lÃ­neas)
â”œâ”€â”€ llm_factory.py                 (~100 lÃ­neas)
â””â”€â”€ utils.py                       (~150 lÃ­neas)

scripts/
â”œâ”€â”€ benchmark_ollama.py            (~200 lÃ­neas)
â””â”€â”€ validate_ollama_setup.py       (~180 lÃ­neas)

tests/unit/
â””â”€â”€ test_ollama_client.py          (~200 lÃ­neas)

tests/integration/
â””â”€â”€ test_ollama_integration.py     (~150 lÃ­neas)

config/
â””â”€â”€ llm_config.yaml                (~50 lÃ­neas)

docs/guides/
â”œâ”€â”€ OLLAMA_SETUP.md                (~400 lÃ­neas)
â”œâ”€â”€ OLLAMA_USAGE.md                (~350 lÃ­neas)
â””â”€â”€ LLM_STRATEGY.md                (~300 lÃ­neas)

reports/
â””â”€â”€ ollama_benchmark_results.json

docs/us-resolved/
â””â”€â”€ us-029.md                      (~800 lÃ­neas)
```

**Total estimado**: ~3,460 lÃ­neas de cÃ³digo + documentaciÃ³n


---

## ğŸ¯ Criterios de Ã‰xito Detallados

### Performance
- âœ… Latencia promedio < 2 segundos (Target: ~1-1.5s)
- âœ… Latencia p95 < 3 segundos
- âœ… Uso de VRAM < 8GB (Target: 4-6GB)
- âœ… Throughput > 20 tokens/segundo

### Funcionalidad
- âœ… Cliente Python funcional con API limpia
- âœ… Manejo robusto de errores y timeouts
- âœ… Logging estructurado
- âœ… Tests con coverage > 70%

### DocumentaciÃ³n
- âœ… GuÃ­as completas de setup y uso
- âœ… Ejemplos de cÃ³digo funcionales
- âœ… Troubleshooting documentado
- âœ… Estrategia de producciÃ³n definida

### IntegraciÃ³n
- âœ… Variables de entorno configuradas
- âœ… MÃ³dulos reutilizables en `src/llm/`
- âœ… Factory pattern para mÃºltiples providers
- âœ… Compatible con estructura del proyecto


---

## ğŸ”§ Decisiones TÃ©cnicas

### 1. Â¿Por quÃ© Llama 3.2 3B y no un modelo mÃ¡s grande?

**DecisiÃ³n**: Usar Llama 3.2 3B en lugar de 7B o 13B.

**Razones**:
- âœ… **VRAM**: 3B usa ~4-6GB, cabe cÃ³modamente en RTX 4070 (12GB)
- âœ… **Latencia**: Respuestas en 1-2s vs 3-5s de modelos mÃ¡s grandes
- âœ… **Suficiente para el caso de uso**: Consultas sobre datos energÃ©ticos
- âœ… **Desarrollo rÃ¡pido**: IteraciÃ³n mÃ¡s rÃ¡pida durante desarrollo

### 2. Â¿Por quÃ© Gemini 2.0 Flash para producciÃ³n?

**DecisiÃ³n**: Usar Gemini 2.0 Flash en lugar de desplegar Ollama en GCP.

**Razones**:
- âœ… **Costo**: Gemini Flash es mÃ¡s barato que Compute Engine con GPU
  - Gemini: ~$0.00001/token = ~$0.01 por 1000 requests
  - Compute Engine GPU: ~$0.50/hora = ~$360/mes
- âœ… **Scale-to-zero**: Compatible con Cloud Run
- âœ… **Latencia**: ~500ms vs 1-2s de Ollama
- âœ… **Mantenimiento**: Sin infraestructura que mantener
- âœ… **Disponibilidad**: 99.9% SLA de Google

### 3. Â¿Por quÃ© Factory Pattern?

**DecisiÃ³n**: Usar Factory Pattern para abstraer el provider de LLM.

**Razones**:
- âœ… **Flexibilidad**: Cambiar entre Ollama y Gemini sin modificar cÃ³digo
- âœ… **Testing**: FÃ¡cil mockear el cliente en tests
- âœ… **Extensibilidad**: Agregar nuevos providers (OpenAI, Claude) fÃ¡cilmente
- âœ… **Best Practice**: PatrÃ³n estÃ¡ndar en la industria


---

## ğŸ’° AnÃ¡lisis de Costos

### Desarrollo (Ollama Local)
- **Costo**: $0 USD
- **Hardware**: RTX 4070 (ya disponible)
- **Electricidad**: ~$0.10/hora (despreciable)

### ProducciÃ³n - OpciÃ³n 1: Compute Engine con GPU
- **Instancia**: n1-standard-4 + NVIDIA T4
- **Costo**: ~$0.50/hora = ~$360/mes
- **VRAM**: 16GB
- **Latencia**: ~1-2s
- âŒ **No viable**: Excede presupuesto de $50 USD

### ProducciÃ³n - OpciÃ³n 2: Gemini 2.0 Flash âœ… (RECOMENDADO)
- **Costo**: ~$0.00001/token
- **EstimaciÃ³n**: 1000 requests/mes Ã— 500 tokens = 500k tokens
- **Costo mensual**: ~$5 USD
- **Latencia**: ~500ms
- âœ… **Viable**: Dentro del presupuesto

### Comparativa

| MÃ©trica | Ollama Local | Compute Engine GPU | Gemini Flash |
|---------|--------------|-------------------|--------------|
| **Costo/mes** | $0 | $360 | $5 |
| **Latencia** | 1-2s | 1-2s | 500ms |
| **VRAM** | 4-6GB | 16GB | N/A |
| **Scale-to-zero** | âŒ | âŒ | âœ… |
| **Mantenimiento** | Bajo | Alto | Ninguno |
| **Recomendado para** | Desarrollo | N/A | ProducciÃ³n |


---

## ğŸ§ª Plan de Testing

### Tests Unitarios
```python
# tests/unit/test_ollama_client.py

class TestOllamaClient:
    """Tests unitarios para OllamaClient."""
    
    def test_client_initialization(self):
        """Test inicializaciÃ³n del cliente."""
        client = OllamaClient(host="http://localhost:11434")
        assert client.host == "http://localhost:11434"
    
    def test_generate_with_mock(self, mocker):
        """Test generaciÃ³n con mock."""
        mock_response = {"response": "Test response"}
        mocker.patch("requests.post", return_value=mock_response)
        
        client = OllamaClient()
        response = client.generate("Test prompt")
        assert response == "Test response"
    
    def test_timeout_handling(self):
        """Test manejo de timeouts."""
        client = OllamaClient(timeout=0.001)
        with pytest.raises(TimeoutError):
            client.generate("Test prompt")
```

### Tests de IntegraciÃ³n
```python
# tests/integration/test_ollama_integration.py

class TestOllamaIntegration:
    """Tests de integraciÃ³n con Ollama real."""
    
    @pytest.mark.integration
    def test_real_generation(self):
        """Test generaciÃ³n real con Ollama."""
        client = OllamaClient()
        response = client.generate("Â¿QuÃ© es el consumo energÃ©tico?")
        assert len(response) > 0
        assert "energÃ­a" in response.lower()
    
    @pytest.mark.integration
    def test_latency_requirement(self):
        """Test que latencia < 2s."""
        client = OllamaClient()
        start = time.time()
        client.generate("Test prompt")
        latency = time.time() - start
        assert latency < 2.0
```


---

## ğŸ“Š Benchmarks Esperados

### Latencia (Llama 3.2 3B en RTX 4070)
- **Prompt corto (50 tokens)**: ~800ms
- **Prompt medio (200 tokens)**: ~1.5s
- **Prompt largo (500 tokens)**: ~2.5s
- **Target p95**: < 3s

### Throughput
- **Tokens/segundo**: 20-30 tokens/s
- **Requests/minuto**: ~30-40 (con prompts cortos)

### Uso de Recursos
- **VRAM en idle**: ~3.5GB
- **VRAM durante inferencia**: ~5-6GB
- **VRAM pico**: ~7GB
- **Target**: < 8GB âœ…

### Comparativa con Gemini Flash
| MÃ©trica | Llama 3.2 3B (Local) | Gemini 2.0 Flash |
|---------|---------------------|------------------|
| Latencia promedio | 1.5s | 500ms |
| Latencia p95 | 2.5s | 800ms |
| Throughput | 25 tokens/s | 50+ tokens/s |
| VRAM | 5-6GB | N/A |
| Costo | $0 | $0.00001/token |


---

## âœ… Cumplimiento con AGENTS.md

### CÃ³digo
- âœ… **CÃ³digo en inglÃ©s**: Todos los mÃ³dulos en `src/llm/`
- âœ… **DocumentaciÃ³n en espaÃ±ol**: GuÃ­as en `docs/guides/`
- âœ… **Funciones reutilizables**: MÃ³dulos en `src/llm/`, no cÃ³digo en notebooks
- âœ… **Type hints**: 100% de funciones pÃºblicas
- âœ… **Docstrings**: Estilo Google en todas las funciones
- âœ… **Sin cÃ³digo duplicado**: Factory pattern para reutilizaciÃ³n
- âœ… **Sin magic numbers**: ConfiguraciÃ³n en YAML y .env

### Testing
- âœ… **Tests unitarios**: `tests/unit/test_ollama_client.py`
- âœ… **Tests de integraciÃ³n**: `tests/integration/test_ollama_integration.py`
- âœ… **Coverage > 70%**: Target para mÃ³dulo `src/llm/`
- âœ… **Pytest**: Framework estÃ¡ndar del proyecto

### DocumentaciÃ³n
- âœ… **Sin emojis en cÃ³digo**: Solo en documentaciÃ³n
- âœ… **Sin separadores decorativos**: CÃ³digo limpio
- âœ… **Comentarios concisos**: TÃ©cnicos, no narrativos
- âœ… **Logging estructurado**: No prints

### Git
- âœ… **Conventional Commits**: 
  - `feat(llm): add ollama client`
  - `test(llm): add ollama integration tests`
  - `docs: add ollama setup guide`


---

## ğŸš€ Roadmap de ImplementaciÃ³n

### Semana 1: Setup y Benchmarking
- **DÃ­a 1-2**: Fase 1 (ValidaciÃ³n) âœ… + Fase 2 (Benchmarking)
- **DÃ­a 3-4**: Fase 3 (IntegraciÃ³n con proyecto)
- **DÃ­a 5**: Fase 4 (Testing)

### Semana 2: DocumentaciÃ³n y ProducciÃ³n
- **DÃ­a 1-2**: Fase 5 (DocumentaciÃ³n)
- **DÃ­a 3-4**: Fase 6 (PreparaciÃ³n Gemini)
- **DÃ­a 5**: Testing final y documentaciÃ³n de resoluciÃ³n

---

## ğŸ“ Lecciones Esperadas

### TÃ©cnicas
1. **LLM Local vs Cloud**: Tradeoffs de latencia, costo y mantenimiento
2. **GPU Memory Management**: OptimizaciÃ³n de VRAM para modelos grandes
3. **Factory Pattern**: AbstracciÃ³n de providers de LLM
4. **Benchmarking**: MediciÃ³n precisa de performance de LLMs

### ArquitectÃ³nicas
1. **Dual Strategy**: Desarrollo local + producciÃ³n cloud
2. **Cost Optimization**: Gemini Flash vs Compute Engine GPU
3. **Abstraction Layers**: Cliente unificado para mÃºltiples providers
4. **Configuration Management**: YAML + env vars para flexibilidad

---

## ğŸ”® Mejoras Futuras

### Corto Plazo (Sprint 3)
- [ ] IntegraciÃ³n con FastAPI endpoint `/copilot/chat`
- [ ] Prompt engineering para consultas energÃ©ticas
- [ ] Context management para conversaciones

### Mediano Plazo
- [ ] RAG (Retrieval Augmented Generation) con datos histÃ³ricos
- [ ] Fine-tuning de Llama 3.2 con datos del dominio
- [ ] Caching de respuestas frecuentes

### Largo Plazo
- [ ] Multi-model ensemble (Llama + Gemini)
- [ ] A/B testing de prompts
- [ ] Feedback loop para mejorar respuestas


---

## ğŸ“š Referencias

### Ollama
- [Ollama Documentation](https://ollama.ai/docs)
- [Llama 3.2 Model Card](https://ollama.ai/library/llama3.2)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)

### Gemini
- [Gemini API Documentation](https://ai.google.dev/docs)
- [Gemini 2.0 Flash Pricing](https://ai.google.dev/pricing)
- [Google AI Python SDK](https://github.com/google/generative-ai-python)

### LangChain
- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [LangChain Ollama Integration](https://python.langchain.com/docs/integrations/llms/ollama)
- [LangChain Google AI Integration](https://python.langchain.com/docs/integrations/llms/google_ai)

### Proyecto
- **US-020**: FastAPI Endpoints (base para `/copilot/chat`)
- **US-025**: Cloud Run Deployment (para Gemini en producciÃ³n)
- **AGENTS.md**: EstÃ¡ndares del proyecto

---

## ğŸ‰ ConclusiÃ³n del Plan

Este plan de implementaciÃ³n para US-029 estÃ¡ diseÃ±ado para:

1. âœ… **Validar** que Ollama + Llama 3.2 3B cumple con los criterios de aceptaciÃ³n
2. âœ… **Integrar** el LLM en la estructura del proyecto de forma modular y reutilizable
3. âœ… **Documentar** completamente el setup, uso y troubleshooting
4. âœ… **Preparar** la migraciÃ³n a Gemini 2.0 Flash para producciÃ³n
5. âœ… **Mantener** el mismo nivel de excelencia de US-025 y US-028b

**Enfoque dual**:
- **Desarrollo**: Ollama local (sin costos, iteraciÃ³n rÃ¡pida)
- **ProducciÃ³n**: Gemini Flash (bajo costo, alta disponibilidad)

**EstimaciÃ³n de esfuerzo**: 8-10 dÃ­as de trabajo
**LÃ­neas de cÃ³digo**: ~3,460 lÃ­neas (cÃ³digo + tests + docs)
**Costo de producciÃ³n**: ~$5 USD/mes (Gemini Flash)

---

**Documento creado por**: Arthur (AI Engineer / MLOps)
**Fecha**: 17 de Noviembre, 2025
**VersiÃ³n**: 1.0
**Estado**: ğŸ“‹ LISTO PARA APROBACIÃ“N

