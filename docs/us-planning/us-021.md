# US-021: Sistema de Monitoreo de Modelo en ProducciÃ³n (MVP)

**Estado**: ğŸ“‹ EN PLANEACIÃ“N
**Fecha de planeaciÃ³n**: Noviembre 2025
**Tipo**: MLOps Monitoring + Model Observability
**Prioridad**: Alta (Sprint 2)

---

## ğŸ“‹ Resumen Ejecutivo

Se implementarÃ¡ un **sistema de monitoreo MVP ultra-simple** para el modelo Stacking Ensemble (ensemble_lightgbm_v1.pkl) en producciÃ³n. El sistema utilizarÃ¡ **Evidently AI** para detectar data drift, generando un reporte HTML bÃ¡sico.

**Problema a Resolver**: El modelo en producciÃ³n (US-020) no tiene visibilidad sobre cambios en la distribuciÃ³n de datos de entrada (data drift).

**SoluciÃ³n MVP Ultra-Simple**:
1. Captura predicciones y features de entrada (CSV simple)
2. Script que genera 1 reporte de Evidently (Data Drift)
3. Dashboard HTML bÃ¡sico

**Alcance MVP**: Sistema funcional mÃ­nimo para detectar drift. Sin alertas, sin performance metrics complejos, sin middleware. Solo lo esencial para tener visibilidad bÃ¡sica del modelo en producciÃ³n.

**Tiempo de ImplementaciÃ³n**: 8 horas

---

## ğŸ¯ Objetivos del MVP

### âœ… Objetivo 1: Captura BÃ¡sica de Datos
- Loggear predicciones directamente en endpoint de API (append a CSV)
- Capturar: features de entrada, predicciÃ³n, timestamp
- Archivo CSV simple: `data/monitoring/predictions.csv`

### âœ… Objetivo 2: Dataset de Referencia
- Preparar archivo de referencia desde training data
- 1,000 muestras aleatorias del training set
- Guardar como: `data/monitoring/reference_data.csv`

### âœ… Objetivo 3: Reporte de Data Drift
- 1 script que genera reporte HTML de Evidently
- Detectar drift en features principales
- Comparar producciÃ³n vs referencia
- Output: `reports/monitoring/drift_report.html`

---

## ğŸ—ï¸ Arquitectura Simplificada

### Flujo de Datos

```
API /predict â†’ Log to CSV â†’ (manual) Run script â†’ HTML Report
```

### Estructura de CÃ³digo

```
src/monitoring/
â”œâ”€â”€ __init__.py                      # MÃ³dulo simple
â””â”€â”€ log_prediction.py                # FunciÃ³n simple de logging (50 lÃ­neas)

scripts/
â”œâ”€â”€ prepare_reference_data.py        # Genera reference_data.csv (50 lÃ­neas)
â””â”€â”€ generate_drift_report.py         # Genera reporte Evidently (100 lÃ­neas)

data/monitoring/
â”œâ”€â”€ reference_data.csv               # 1,000 muestras de training
â””â”€â”€ predictions.csv                  # Predicciones de producciÃ³n (append)

reports/monitoring/
â””â”€â”€ drift_report.html                # Reporte HTML de Evidently

tests/unit/
â””â”€â”€ test_monitoring.py               # Tests bÃ¡sicos (5 tests)
```

**Total de archivos**: 5 archivos Python (~250 lÃ­neas totales)

---

## ğŸ’¡ Componentes a Implementar (Ultra-Simplificados)

### 1. FunciÃ³n de Logging Simple

**Archivo**: `src/monitoring/log_prediction.py` (~50 lÃ­neas)

```python
import csv
from pathlib import Path
from datetime import datetime

def log_prediction(features: dict, prediction: float) -> None:
    """
    Append prediction to CSV file.

    Parameters
    ----------
    features : dict
        Input features (18 features after engineering)
    prediction : float
        Model prediction
    """
    log_file = Path("data/monitoring/predictions.csv")

    # Create file with header if doesn't exist
    if not log_file.exists():
        log_file.parent.mkdir(parents=True, exist_ok=True)
        with open(log_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp'] + list(features.keys()) + ['prediction'])

    # Append prediction
    with open(log_file, 'a', newline='') as f:
        writer = csv.writer(f)
        row = [datetime.now().isoformat()] + list(features.values()) + [prediction]
        writer.writerow(row)
```

**IntegraciÃ³n en API** (modificar `src/api/routes/predict.py`):
```python
from src.monitoring.log_prediction import log_prediction

@router.post("/predict")
async def predict(request: PredictionRequest):
    # ... existing code ...

    # Log prediction (simple append)
    try:
        log_prediction(features_dict, predicted_usage)
    except Exception as e:
        logger.warning(f"Failed to log prediction: {e}")

    return response
```

### 2. Script para Preparar Datos de Referencia

**Archivo**: `scripts/prepare_reference_data.py` (~50 lÃ­neas)

```python
import pandas as pd
from pathlib import Path

def prepare_reference_data():
    """Sample 1000 rows from training data as reference."""
    # Load training data
    df_train = pd.read_parquet("data/processed/steel_preprocessed_train.parquet")

    # Sample 1000 rows
    df_ref = df_train.sample(n=1000, random_state=42)

    # Save as CSV
    output_path = Path("data/monitoring/reference_data.csv")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df_ref.to_csv(output_path, index=False)

    print(f"Reference data saved: {output_path}")
    print(f"Shape: {df_ref.shape}")

if __name__ == "__main__":
    prepare_reference_data()
```

### 3. Script para Generar Reporte de Drift

**Archivo**: `scripts/generate_drift_report.py` (~100 lÃ­neas)

```python
import pandas as pd
from pathlib import Path
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

def generate_drift_report():
    """Generate Evidently data drift report."""
    # Load reference data
    reference_df = pd.read_csv("data/monitoring/reference_data.csv")

    # Load production data
    production_df = pd.read_csv("data/monitoring/predictions.csv")

    # Remove timestamp column
    production_df = production_df.drop('timestamp', axis=1)

    # Create Evidently report
    report = Report(metrics=[
        DataDriftPreset(),
    ])

    # Run report
    report.run(
        reference_data=reference_df,
        current_data=production_df
    )

    # Save HTML report
    output_path = Path("reports/monitoring/drift_report.html")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    report.save_html(str(output_path))

    print(f"Report generated: {output_path}")

if __name__ == "__main__":
    generate_drift_report()
```

---

## ğŸ”§ Pasos de ImplementaciÃ³n Simplificados

### Paso 1: Preparar Dataset de Referencia (30 min)

```bash
# Ejecutar script para crear reference_data.csv
poetry run python scripts/prepare_reference_data.py
```

Este script toma 1,000 muestras aleatorias del training set.

### Paso 2: Implementar Logging en API (1 hora)

1. Crear `src/monitoring/log_prediction.py` (funciÃ³n simple de 50 lÃ­neas)
2. Modificar `src/api/routes/predict.py` para llamar a `log_prediction()`
3. Testear con 10 predicciones, verificar CSV creado

### Paso 3: Crear Script de Reporte (1 hora)

1. Crear `scripts/generate_drift_report.py` (~100 lÃ­neas)
2. Usar `DataDriftPreset` de Evidently
3. Generar HTML report

### Paso 4: Testing BÃ¡sico (1 hora)

1. Hacer 50 predicciones a travÃ©s de la API
2. Ejecutar script de reporte: `poetry run python scripts/generate_drift_report.py`
3. Abrir HTML y verificar visualizaciones

### Paso 5: DocumentaciÃ³n (30 min)

1. Crear README simple en `src/monitoring/`
2. Documentar uso de scripts

---

## ğŸ“ Archivos a Crear

### CÃ³digo Fuente (~200 lÃ­neas total)

| Archivo | LÃ­neas | PropÃ³sito |
|---------|--------|-----------|
| `src/monitoring/__init__.py` | 5 | Exports del mÃ³dulo |
| `src/monitoring/log_prediction.py` | 50 | FunciÃ³n simple de logging |

### Scripts (~150 lÃ­neas total)

| Archivo | LÃ­neas | PropÃ³sito |
|---------|--------|-----------|
| `scripts/prepare_reference_data.py` | 50 | Preparar dataset de referencia |
| `scripts/generate_drift_report.py` | 100 | Generar reporte Evidently |

### Tests (~100 lÃ­neas)

| Archivo | LÃ­neas | PropÃ³sito |
|---------|--------|-----------|
| `tests/unit/test_monitoring.py` | 100 | Tests bÃ¡sicos (5 tests) |

### DocumentaciÃ³n (~50 lÃ­neas)

| Archivo | LÃ­neas | PropÃ³sito |
|---------|--------|-----------|
| `src/monitoring/README.md` | 50 | README simple del mÃ³dulo |

**Total Estimado**: ~500 lÃ­neas (cÃ³digo + tests + docs)

---

## ğŸ§ª Estrategia de Testing (Simplificada)

### Unit Tests BÃ¡sicos

**Test Coverage Target**: >70% (suficiente para MVP)

**Archivo**: `tests/unit/test_monitoring.py`

**Tests** (5 tests bÃ¡sicos):
1. `test_log_prediction_creates_file` - Verifica que se crea CSV
2. `test_log_prediction_appends` - Verifica que appends funcionan
3. `test_prepare_reference_data` - Verifica script de referencia
4. `test_generate_drift_report` - Verifica que reporte se genera
5. `test_csv_has_correct_columns` - Verifica schema del CSV

### Manual Testing

**Checklist mÃ­nimo**:
- [ ] Hacer 20 predicciones a travÃ©s de la API
- [ ] Verificar que se creÃ³ `data/monitoring/predictions.csv`
- [ ] Ejecutar script de generaciÃ³n de reporte
- [ ] Abrir `drift_report.html` y verificar visualizaciones
- [ ] Verificar que muestra drift detection

---

## ğŸ“Š MÃ©tricas de Ã‰xito (Simplificadas)

### MÃ©tricas de Calidad MÃ­nimas

| MÃ©trica | Target |
|---------|--------|
| Test Coverage | >70% |
| Tests passing | 100% (5 tests) |
| Ruff checks | 0 errors crÃ­ticos |
| Prediction Logging funciona | SÃ­ |
| Reporte se genera | SÃ­ |

### MÃ©tricas de Monitoreo

| MÃ©trica | DescripciÃ³n |
|---------|-------------|
| **Data Drift Detection** | Detecta drift en features principales |
| **Reporte HTML** | Se genera y es accesible |
| **Predicciones loggeadas** | 100% de predicciones se capturan |

---

## âœ… Criterios de AceptaciÃ³n (MVP)

### Funcionales MÃ­nimos

- [ ] Predicciones se loggean en CSV automÃ¡ticamente
- [ ] Dataset de referencia generado (1,000 muestras)
- [ ] Script `generate_drift_report.py` funciona
- [ ] Reporte HTML se genera correctamente
- [ ] Reporte muestra data drift en features
- [ ] Sistema maneja >20 predicciones sin errores

### No Funcionales MÃ­nimos

- [ ] Tests bÃ¡sicos (5 tests) pasan
- [ ] Coverage >70%
- [ ] CÃ³digo cumple con AGENTS.md bÃ¡sico (docstrings, type hints)
- [ ] README simple en src/monitoring/

### DocumentaciÃ³n MÃ­nima

- [ ] README.md en src/monitoring/ con ejemplos bÃ¡sicos
- [ ] Docstrings en funciones principales
- [ ] Comentarios en cÃ³digo complejo

---

## â±ï¸ EstimaciÃ³n de Esfuerzo (MVP Ultra-Simple)

### Desarrollo (Total: 8 horas)

| Tarea | Horas |
|-------|-------|
| **Preparar dataset de referencia** | 0.5h |
| - Script simple (50 lÃ­neas) | |
| **Implementar logging en API** | 2h |
| - Crear `log_prediction.py` (50 lÃ­neas) | 1h |
| - Modificar endpoint `/predict` | 0.5h |
| - Testear logging manualmente | 0.5h |
| **Script de reporte Evidently** | 2h |
| - Crear `generate_drift_report.py` (100 lÃ­neas) | 1.5h |
| - Testear generaciÃ³n de reporte | 0.5h |
| **Tests unitarios bÃ¡sicos** | 2h |
| - 5 tests bÃ¡sicos | 1.5h |
| - Ejecutar y verificar coverage >70% | 0.5h |
| **DocumentaciÃ³n mÃ­nima** | 1h |
| - README en src/monitoring/ | 0.5h |
| - Docstrings bÃ¡sicos | 0.5h |
| **Buffer y QA** | 0.5h |
| - Bug fixes menores | |

**Total: 8 horas (1 dÃ­a de trabajo completo)**

---

## âš ï¸ Riesgos y MitigaciÃ³n (Simplificados)

### Riesgo 1: Logging Lento

**Riesgo**: CSV append puede ser lento con muchas predicciones.

**MitigaciÃ³n**:
- Para MVP, es aceptable (bajo volumen)
- Futuro: migrar a Parquet con buffering

### Riesgo 2: CSV Crece Mucho

**Riesgo**: Archivo CSV crece indefinidamente.

**MitigaciÃ³n**:
- Documentar limpieza manual periÃ³dica
- Futuro: implementar rotaciÃ³n automÃ¡tica

### Riesgo 3: Evidently VersiÃ³n

**Riesgo**: Breaking changes en Evidently.

**MitigaciÃ³n**:
- Pin version: `evidently==0.4.30`
- Documentar versiÃ³n compatible

---

## ğŸ”„ Mejoras Futuras (Post-MVP)

Si el MVP funciona bien, considerar:

1. **Alertas automÃ¡ticas** cuando drift > threshold
2. **Parquet con buffering** en lugar de CSV simple
3. **RotaciÃ³n automÃ¡tica** de archivos
4. **Dashboard Streamlit** con mÃ©tricas en vivo
5. **Performance metrics** con ground truth capturado

---

## ğŸ“š Referencias

### DocumentaciÃ³n

- [Evidently AI Docs](https://docs.evidentlyai.com/)
- [Evidently DataDriftPreset](https://docs.evidentlyai.com/reference/all-metrics)

### CÃ³digo Relacionado

- US-020: FastAPI Endpoints (`src/api/`)
- US-012: Preprocessing Pipeline

### Dependencias

- `evidently==0.4.30` (agregar a pyproject.toml)

---

## ğŸ¯ Alcance del MVP Ultra-Simple

### âœ… Incluido en MVP

- [x] Logging bÃ¡sico de predicciones a CSV
- [x] Dataset de referencia (1,000 muestras)
- [x] 1 reporte HTML de data drift
- [x] Script simple para generar reporte
- [x] 5 tests bÃ¡sicos (>70% coverage)
- [x] README mÃ­nimo

### âŒ NO Incluido en MVP

- [ ] Alertas automÃ¡ticas
- [ ] Dashboard en tiempo real
- [ ] Performance metrics (requiere ground truth)
- [ ] RotaciÃ³n automÃ¡tica de archivos
- [ ] Buffering en memoria
- [ ] Parquet (solo CSV simple)
- [ ] Multiple reportes (solo data drift)
- [ ] Target drift report
- [ ] Integration con Prometheus

---

## ğŸ“ Pasos para ImplementaciÃ³n (8 horas)

### Orden de ImplementaciÃ³n

1. **Agregar dependencia** (5 min)
   ```bash
   poetry add evidently==0.4.30
   ```

2. **Crear `scripts/prepare_reference_data.py`** (30 min)
   - Samplear 1,000 rows de training data
   - Guardar como CSV

3. **Crear `src/monitoring/log_prediction.py`** (1 hora)
   - FunciÃ³n simple para append a CSV
   - Testear manualmente

4. **Modificar `src/api/routes/predict.py`** (30 min)
   - Importar y llamar `log_prediction()`
   - Hacer 20 predicciones de prueba

5. **Crear `scripts/generate_drift_report.py`** (2 horas)
   - Leer CSVs
   - Crear reporte Evidently
   - Testear generaciÃ³n

6. **Crear tests bÃ¡sicos** (2 horas)
   - `tests/unit/test_monitoring.py`
   - 5 tests simples

7. **Crear README** (1 hora)
   - `src/monitoring/README.md`
   - Ejemplos de uso

---

## ğŸ‰ DefiniciÃ³n de "Done" (MVP)

La US-021 MVP se considerarÃ¡ completa cuando:

1. âœ… CÃ³digo implementado (5 archivos, ~500 lÃ­neas)
2. âœ… Tests bÃ¡sicos pasando (5 tests, >70% coverage)
3. âœ… 20+ predicciones loggeadas exitosamente en CSV
4. âœ… Reporte HTML generado y verificado visualmente
5. âœ… README con ejemplos de uso
6. âœ… CÃ³digo cumple AGENTS.md bÃ¡sico (docstrings, type hints)
7. âœ… Ruff clean (0 errores crÃ­ticos)
8. âœ… Peer review aprobado
9. âœ… Merged to main branch

---

**Documento creado por**: MLOps Team - Proyecto Atreides
**Fecha de planeaciÃ³n**: Noviembre 2025
**VersiÃ³n**: 2.0 (MVP Ultra-Simplificado)
**Estado**: ğŸ“‹ EN PLANEACIÃ“N - Pendiente de aprobaciÃ³n
**Tiempo estimado**: 8 horas (1 dÃ­a)

---

## ğŸ“‹ AprobaciÃ³n y Sign-Off

**Pendiente de revisiÃ³n por**:
- [ ] Julian (ML Engineer) - RevisiÃ³n tÃ©cnica
- [ ] Equipo completo - AprobaciÃ³n de scope reducido

**Fecha objetivo de inicio**: DespuÃ©s de aprobaciÃ³n
**Sprint target**: Sprint 2

---

## ğŸ’¬ Resumen

Este MVP ultra-simplificado proporciona **lo mÃ­nimo funcional** para tener visibilidad del modelo en producciÃ³n:

- **CSV logging simple**: Sin overhead de performance
- **1 reporte de drift**: Data drift en features
- **ImplementaciÃ³n rÃ¡pida**: 8 horas total
- **Extensible**: FÃ¡cil de mejorar despuÃ©s

El sistema permite detectar cuando la distribuciÃ³n de datos en producciÃ³n difiere del training, que es el problema mÃ¡s crÃ­tico a monitorear.

**Â¿Listo para implementar?** ğŸš€
