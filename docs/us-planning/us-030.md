# US-030: Refactorizaci√≥n y Code Quality

**Estado**: üìã EN PLANEACI√ìN
**Fecha de Creaci√≥n**: 17 de Noviembre, 2025
**Responsable**: Julian (ML Engineer) + Code Quality Review Team
**Sprint**: Sprint 2/3 - MLOps Quality & Refinement
**Tipo**: Code Quality + Refactoring + Technical Debt

---

## üìã Contexto y Justificaci√≥n

### Situaci√≥n Actual

Despu√©s de analizar el codebase actual con herramientas de linting, se identificaron las siguientes m√©tricas de calidad de c√≥digo:

| Directorio | Errores Ruff | Fixables | Tipos Principales |
|------------|--------------|----------|-------------------|
| **src/** | 811 | 490 | W293 (461), UP006 (87), UP045 (66), W291 (53) |
| **scripts/** | 138 | 126 | W293 (78), F541 (28), F401 (7) |
| **tests/** | 915 | 877 | W293 (835), I001 (22), W291 (18) |
| **TOTAL** | **1,864** | **1,493** | - |

**Errores m√°s comunes**:
- **W293** (1,374 ocurrencias): L√≠neas en blanco con whitespace
- **W291** (73 ocurrencias): Trailing whitespace
- **UP006** (88 ocurrencias): Annotations no-PEP585 (List[] en lugar de list[])
- **UP045** (67 ocurrencias): Optional[] en lugar de | None
- **I001** (51 ocurrencias): Imports sin ordenar
- **F401** (34 ocurrencias): Imports sin usar
- **F841** (20 ocurrencias): Variables sin usar

### Problemas Identificados

#### 1. Falta de Principios SOLID
- **Violaci√≥n SRP**: Algunas clases/funciones tienen m√∫ltiples responsabilidades
- **Violaci√≥n DIP**: Dependencias concretas en lugar de abstracciones
- **Oportunidades OCP**: C√≥digo no extensible sin modificaci√≥n

#### 2. Constantes Hardcodeadas
```python
# ‚ùå Anti-patr√≥n encontrado en m√∫ltiples archivos
df = df.filter(pl.col('value') > 0.5)  # Magic number
conn = duckdb.connect("data/steel.duckdb")  # Hardcoded path
model_path = "models/ensembles/ensemble_v3.pkl"  # Path hardcodeado
```

#### 3. Type Hints Inconsistentes
- ~30% de funciones sin type hints completos
- Uso de tipos deprecated (List, Dict, Optional en lugar de list, dict, | None)
- Falta de anotaciones en funciones de retorno

#### 4. Docstrings Incompletos
- Algunas funciones sin docstrings
- Docstrings sin formato Google consistente
- Falta de ejemplos en funciones complejas

### Alineaci√≥n con AGENTS.md

Este US est√° directamente alineado con el checklist de AGENTS.md:
- ‚úÖ Type hints en todas las funciones
- ‚úÖ Docstrings estilo Google
- ‚úÖ Sin warnings de Ruff
- ‚úÖ Sin magic numbers (extraer a config.py)
- ‚úÖ Principios SOLID donde aplique
- ‚úÖ C√≥digo limpio y mantenible

---

## üéØ Objetivos de la US-030

### Objetivo Principal
**Refactorizar el codebase completo para eliminar warnings de linting, aplicar principios SOLID, estandarizar type hints y docstrings, y centralizar constantes en archivos de configuraci√≥n.**

### Objetivos Espec√≠ficos

| # | Objetivo | Prioridad | Complejidad |
|---|----------|-----------|-------------|
| 1 | Eliminar 1,864 warnings de Ruff | üî¥ ALTA | MEDIA |
| 2 | Aplicar principios SOLID en clases existentes | üü† MEDIA | ALTA |
| 3 | Extraer constantes a config.py centralizado | üî¥ ALTA | BAJA |
| 4 | Type hints completos en 100% funciones p√∫blicas | üî¥ ALTA | MEDIA |
| 5 | Docstrings Google en 100% funciones p√∫blicas | üü† MEDIA | MEDIA |
| 6 | Pasar MyPy sin errores cr√≠ticos | üü° BAJA | MEDIA |

---

## üìä An√°lisis de Impacto

### Beneficios Esperados

#### 1. Mantenibilidad
- **Antes**: 1,864 warnings dificultan lectura del c√≥digo
- **Despu√©s**: Codebase limpio, f√°cil de navegar y modificar
- **M√©tricas**: Code quality score aumenta de 6.5/10 a 9.0/10

#### 2. Extensibilidad (SOLID)
- **Antes**: Modificaciones requieren cambios en m√∫ltiples lugares
- **Despu√©s**: Nuevas features se agregan sin modificar c√≥digo existente
- **Ejemplo**: Agregar nuevo modelo sin modificar ModelService

#### 3. Seguridad y Robustez (Type Hints)
- **Antes**: Errores de tipo se detectan en runtime
- **Despu√©s**: Type checking en desarrollo con MyPy
- **Reducci√≥n**: ~40% menos bugs relacionados con tipos

#### 4. Onboarding de Nuevos Desarrolladores
- **Antes**: 2-3 d√≠as para entender estructura
- **Despu√©s**: 1 d√≠a con c√≥digo autodocumentado
- **Docstrings**: Ejemplos inline aceleran aprendizaje

### Riesgos y Mitigaci√≥n

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|--------------|---------|------------|
| Introducir bugs en refactoring | MEDIA | ALTO | Tests exhaustivos antes/despu√©s |
| Romper compatibilidad con notebooks | BAJA | MEDIO | Validar notebooks despu√©s de cambios |
| Tiempo excesivo en refactoring | ALTA | MEDIO | Priorizar fixes autom√°ticos primero |
| Conflictos de merge con otras US | MEDIA | MEDIO | Coordinar con equipo, hacer en branch dedicado |

---

## üèóÔ∏è Plan de Implementaci√≥n

### Fase 1: Preparaci√≥n y Configuraci√≥n (2-3 horas)

#### 1.1 Crear Estructura de Configuraci√≥n Centralizada

```
src/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ paths.py              # Paths y directorios
‚îÇ   ‚îú‚îÄ‚îÄ constants.py          # Constantes de negocio
‚îÇ   ‚îú‚îÄ‚îÄ model_config.py       # Configuraci√≥n de modelos
‚îÇ   ‚îî‚îÄ‚îÄ api_config.py         # Configuraci√≥n de API
```

**Archivo: `src/config/paths.py`**
```python
"""
Centralized path configuration.

All file paths and directories used across the project.
"""
from pathlib import Path
from typing import Final

# Project root
PROJECT_ROOT: Final[Path] = Path(__file__).parent.parent.parent

# Data directories
DATA_DIR: Final[Path] = PROJECT_ROOT / "data"
DATA_RAW_DIR: Final[Path] = DATA_DIR / "raw"
DATA_PROCESSED_DIR: Final[Path] = DATA_DIR / "processed"
DATA_PRODUCTION_DIR: Final[Path] = DATA_DIR / "production"

# Database
DUCKDB_PATH: Final[Path] = DATA_DIR / "steel.duckdb"

# Models
MODELS_DIR: Final[Path] = PROJECT_ROOT / "models"
MODELS_ENSEMBLES_DIR: Final[Path] = MODELS_DIR / "ensembles"
MODELS_ONNX_DIR: Final[Path] = MODELS_DIR / "onnx"

# Reports
REPORTS_DIR: Final[Path] = PROJECT_ROOT / "reports"
REPORTS_MONITORING_DIR: Final[Path] = REPORTS_DIR / "monitoring"

# Logs
LOGS_DIR: Final[Path] = PROJECT_ROOT / "logs"
```

**Archivo: `src/config/constants.py`**
```python
"""
Business and domain constants.

All magic numbers and business rules centralized here.
"""
from typing import Final, Literal

# Dataset constants
SECONDS_PER_DAY: Final[int] = 86400
MINUTES_PER_HOUR: Final[int] = 60
HOURS_PER_DAY: Final[int] = 24

# Feature thresholds
OUTLIER_Z_SCORE_THRESHOLD: Final[float] = 3.0
DRIFT_DETECTION_THRESHOLD: Final[float] = 0.7
CORRELATION_THRESHOLD: Final[float] = 0.8

# Model thresholds
MAX_BATCH_SIZE: Final[int] = 1000
MODEL_TIMEOUT_SECONDS: Final[int] = 30
MIN_CONFIDENCE_SCORE: Final[float] = 0.5

# Load types
LoadType = Literal["Light", "Medium", "Maximum"]
VALID_LOAD_TYPES: Final[tuple[str, ...]] = ("Light", "Medium", "Maximum")

# Weekday status
WeekStatus = Literal["Weekday", "Weekend"]
VALID_WEEK_STATUS: Final[tuple[str, ...]] = ("Weekday", "Weekend")

# Power factor bounds
MIN_POWER_FACTOR: Final[float] = 0.0
MAX_POWER_FACTOR: Final[float] = 1.0
```

**Archivo: `src/config/model_config.py`**
```python
"""
Model configuration and hyperparameters.
"""
from dataclasses import dataclass
from typing import Final


@dataclass
class ModelConfig:
    """Configuration for ML models."""
    
    name: str
    type: str
    version: str
    path: str
    hyperparameters: dict[str, any]


# Default model configurations
DEFAULT_MODEL_TYPE: Final[str] = "stacking_ensemble"
DEFAULT_MODEL_VERSION: Final[str] = "v3"

LIGHTGBM_CONFIG = ModelConfig(
    name="lightgbm",
    type="gradient_boosting",
    version="v3",
    path="models/lightgbm/lightgbm_v3.pkl",
    hyperparameters={
        "n_estimators": 1000,
        "learning_rate": 0.05,
        "max_depth": 7,
        "num_leaves": 31,
    }
)

XGBOOST_CONFIG = ModelConfig(
    name="xgboost",
    type="gradient_boosting",
    version="v2",
    path="models/xgboost/xgboost_v2.pkl",
    hyperparameters={
        "n_estimators": 800,
        "learning_rate": 0.03,
        "max_depth": 6,
    }
)
```

#### 1.2 Actualizar pyproject.toml

```toml
[tool.ruff]
line-length = 100
target-version = "py311"

[tool.ruff.lint]  # Migrar de secci√≥n ra√≠z
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C",   # flake8-comprehensions
    "B",   # flake8-bugbear
    "UP",  # pyupgrade
]
ignore = [
    "E501",  # line too long (handled by black)
    "B008",  # function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101"]  # Allow assert in tests

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true  # ‚¨ÖÔ∏è Cambiar a true
disallow_incomplete_defs = true  # ‚¨ÖÔ∏è Cambiar a true
check_untyped_defs = true
disallow_untyped_decorators = false
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
follow_imports = "normal"
ignore_missing_imports = true
```

---

### Fase 2: Fixes Autom√°ticos con Ruff (1-2 horas)

#### 2.1 Aplicar Fixes Autom√°ticos

```powershell
# Fix autom√°tico de 1,493 errores
poetry run ruff check src/ scripts/ tests/ --fix

# Fix con unsafe fixes (268 adicionales)
poetry run ruff check src/ scripts/ tests/ --fix --unsafe-fixes

# Formatear con Black despu√©s de fixes
poetry run black src/ scripts/ tests/
```

**Errores que se arreglan autom√°ticamente**:
- ‚úÖ W293, W291: Trailing whitespace
- ‚úÖ I001: Import sorting
- ‚úÖ F401: Unused imports (removidos)
- ‚úÖ UP006, UP045: Type annotations modernas (list[] en lugar de List[])
- ‚úÖ F541: f-strings sin placeholders

**Resultado esperado**: ~80% de warnings eliminados (1,493 de 1,864)

#### 2.2 Validar Despu√©s de Fixes

```powershell
# Verificar que no se introdujeron errores
poetry run pytest tests/ -v

# Verificar notebooks (sample)
jupyter nbconvert --to notebook --execute notebooks/exploratory/01_data_exploration.ipynb
```

---

### Fase 3: Type Hints Completos (4-6 horas)

#### 3.1 Auditor√≠a de Type Hints

```powershell
# Generar reporte de funciones sin type hints
poetry run mypy src/ --disallow-untyped-defs --show-error-codes > mypy_report.txt
```

#### 3.2 Priorizar Archivos por Impacto

**Orden de refactorizaci√≥n**:

1. **src/utils/** (15 archivos) - M√°xima prioridad
   - `duckdb_utils.py` - 498 l√≠neas
   - `data_cleaning.py` - ~400 l√≠neas
   - `preprocessing_utils.py`
   - `feature_importance.py`

2. **src/api/** (12 archivos) - Alta prioridad
   - `services/model_service.py` - 273 l√≠neas
   - `services/feature_engineering.py` - 237 l√≠neas
   - `routes/*.py`

3. **src/models/** (18 archivos) - Media prioridad
   - `sklearn_pipeline.py` - 232 l√≠neas
   - `train_*.py` scripts

4. **src/features/** (7 archivos) - Media prioridad
   - `temporal_transformers.py`
   - `preprocessing.py`

5. **tests/** (26 archivos) - Baja prioridad
   - Tests ya existentes solo verificar

#### 3.3 Template de Type Hints Modernos

**‚ùå Antes (deprecated)**:
```python
from typing import List, Dict, Optional, Union

def process_data(
    df: pl.DataFrame,
    columns: List[str],
    config: Optional[Dict[str, any]] = None
) -> Union[pl.DataFrame, None]:
    pass
```

**‚úÖ Despu√©s (PEP 604, PEP 585)**:
```python
import polars as pl

def process_data(
    df: pl.DataFrame,
    columns: list[str],
    config: dict[str, any] | None = None
) -> pl.DataFrame | None:
    pass
```

#### 3.4 Ejemplo de Refactorizaci√≥n: duckdb_utils.py

**Antes**:
```python
def get_stats_by_column(table_name, group_by_column, agg_column, db_path="data/steel.duckdb"):
    """Obtiene estad√≠sticas agregadas..."""
    from src.data.load_to_duckdb import query_to_dataframe
    conn = get_connection(db_path)
    # ...
```

**Despu√©s**:
```python
from pathlib import Path
from src.config.paths import DUCKDB_PATH

def get_stats_by_column(
    table_name: str,
    group_by_column: str,
    agg_column: str,
    db_path: Path | str = DUCKDB_PATH
) -> pl.DataFrame:
    """
    Obtiene estad√≠sticas agregadas agrupadas por una columna.
    
    Parameters
    ----------
    table_name : str
        Nombre de la tabla en DuckDB
    group_by_column : str
        Columna para agrupar
    agg_column : str
        Columna para agregar
    db_path : Path | str, default=DUCKDB_PATH
        Ruta a la base de datos DuckDB
        
    Returns
    -------
    pl.DataFrame
        DataFrame con estad√≠sticas (count, avg, min, max, std)
        
    Examples
    --------
    >>> from src.utils.duckdb_utils import get_stats_by_column
    >>> stats = get_stats_by_column('steel_cleaned', 'Load_Type', 'Usage_kWh')
    >>> print(stats)
    shape: (3, 6)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Load_Type ‚îÇ count ‚îÇ avg      ‚îÇ min      ‚îÇ max      ‚îÇ std      ‚îÇ
    ‚îÇ ---       ‚îÇ ---   ‚îÇ ---      ‚îÇ ---      ‚îÇ ---      ‚îÇ ---      ‚îÇ
    ‚îÇ str       ‚îÇ i64   ‚îÇ f64      ‚îÇ f64      ‚îÇ f64      ‚îÇ f64      ‚îÇ
    ‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
    ‚îÇ Light     ‚îÇ 12000 ‚îÇ 23.45    ‚îÇ 10.2     ‚îÇ 45.6     ‚îÇ 5.23     ‚îÇ
    ‚îÇ Medium    ‚îÇ 18000 ‚îÇ 47.89    ‚îÇ 30.1     ‚îÇ 78.9     ‚îÇ 8.45     ‚îÇ
    ‚îÇ Maximum   ‚îÇ 5040  ‚îÇ 95.67    ‚îÇ 70.4     ‚îÇ 120.3    ‚îÇ 12.34    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    Raises
    ------
    duckdb.Error
        Si la tabla no existe o las columnas son inv√°lidas
    """
    from src.data.load_to_duckdb import query_to_dataframe
    
    conn = get_connection(db_path)
    try:
        # ...
```

---

### Fase 4: Aplicaci√≥n de Principios SOLID (6-8 horas)

#### 4.1 Single Responsibility Principle (SRP)

**Caso: `ModelService` en src/api/services/model_service.py**

**Problema actual**:
```python
class ModelService:
    """Handles model loading AND inference AND metrics."""
    
    def load_model(self):
        """Load model from disk."""
        pass
    
    def predict(self, features):
        """Make predictions."""
        pass
    
    def get_metrics(self):
        """Get model metrics."""
        pass
    
    def validate_features(self, features):
        """Validate input features."""
        pass
```

**Refactorizaci√≥n SRP**:
```python
# src/api/services/model_loader.py
class ModelLoader:
    """
    Responsible ONLY for loading models from disk/MLflow.
    
    Single Responsibility: Model Loading
    """
    
    def __init__(self, model_type: str, model_path: Path):
        self.model_type = model_type
        self.model_path = model_path
    
    def load_from_disk(self) -> any:
        """Load model from local disk."""
        pass
    
    def load_from_mlflow(self, run_id: str) -> any:
        """Load model from MLflow registry."""
        pass


# src/api/services/predictor.py
class Predictor:
    """
    Responsible ONLY for making predictions.
    
    Single Responsibility: Inference
    """
    
    def __init__(self, model: any):
        self.model = model
    
    def predict_single(self, features: dict[str, any]) -> float:
        """Predict single instance."""
        pass
    
    def predict_batch(self, features: list[dict[str, any]]) -> list[float]:
        """Predict batch of instances."""
        pass


# src/api/services/feature_validator.py
class FeatureValidator:
    """
    Responsible ONLY for validating features.
    
    Single Responsibility: Validation
    """
    
    def __init__(self, schema: dict[str, any]):
        self.schema = schema
    
    def validate(self, features: dict[str, any]) -> tuple[bool, str | None]:
        """Validate features against schema."""
        pass


# src/api/services/model_service.py (SIMPLIFIED)
class ModelService:
    """
    Orchestrates model operations.
    
    Uses composition to delegate responsibilities.
    """
    
    def __init__(
        self,
        loader: ModelLoader,
        predictor: Predictor,
        validator: FeatureValidator
    ):
        self.loader = loader
        self.predictor = predictor
        self.validator = validator
    
    def predict(self, features: dict[str, any]) -> float:
        """Orchestrate prediction."""
        is_valid, error = self.validator.validate(features)
        if not is_valid:
            raise ValueError(error)
        return self.predictor.predict_single(features)
```

**Beneficios**:
- ‚úÖ Cada clase tiene una sola raz√≥n para cambiar
- ‚úÖ M√°s f√°cil de testear (mock individual de cada componente)
- ‚úÖ M√°s f√°cil de extender (agregar nuevo loader sin tocar predictor)

#### 4.2 Open/Closed Principle (OCP)

**Caso: Feature Engineering**

**Problema actual**:
```python
def create_temporal_features(df: pl.DataFrame) -> pl.DataFrame:
    """Create temporal features."""
    df = df.with_columns([
        (pl.col('NSM') // 3600).alias('hour'),
        (pl.col('NSM') % 3600 // 60).alias('minute'),
        # ... muchas m√°s features hardcodeadas
    ])
    return df
```

**Refactorizaci√≥n OCP**:
```python
from abc import ABC, abstractmethod

class FeatureTransformer(ABC):
    """
    Abstract base for feature transformers.
    
    Open for extension, closed for modification.
    """
    
    @abstractmethod
    def transform(self, df: pl.DataFrame) -> pl.DataFrame:
        """Transform dataframe."""
        pass
    
    @abstractmethod
    def get_feature_names(self) -> list[str]:
        """Get names of created features."""
        pass


class HourFeatureTransformer(FeatureTransformer):
    """Creates hour feature from NSM."""
    
    def transform(self, df: pl.DataFrame) -> pl.DataFrame:
        return df.with_columns(
            (pl.col('NSM') // 3600).cast(pl.Int32).alias('hour')
        )
    
    def get_feature_names(self) -> list[str]:
        return ['hour']


class CyclicTimeTransformer(FeatureTransformer):
    """Creates cyclic encoding of time features."""
    
    def transform(self, df: pl.DataFrame) -> pl.DataFrame:
        return df.with_columns([
            (np.sin(2 * np.pi * pl.col('hour') / 24)).alias('hour_sin'),
            (np.cos(2 * np.pi * pl.col('hour') / 24)).alias('hour_cos'),
        ])
    
    def get_feature_names(self) -> list[str]:
        return ['hour_sin', 'hour_cos']


class FeaturePipeline:
    """
    Composes multiple transformers.
    
    Easy to extend with new transformers without modifying existing code.
    """
    
    def __init__(self, transformers: list[FeatureTransformer]):
        self.transformers = transformers
    
    def transform(self, df: pl.DataFrame) -> pl.DataFrame:
        """Apply all transformers sequentially."""
        for transformer in self.transformers:
            df = transformer.transform(df)
        return df
    
    def get_feature_names(self) -> list[str]:
        """Get all created feature names."""
        names = []
        for transformer in self.transformers:
            names.extend(transformer.get_feature_names())
        return names


# Usage (EXTENDIBLE)
pipeline = FeaturePipeline([
    HourFeatureTransformer(),
    CyclicTimeTransformer(),
    # F√°cil agregar nuevo transformer sin modificar c√≥digo existente
    # WeekdayTransformer(),
    # LagFeatureTransformer(lags=[1, 7, 30]),
])
df_transformed = pipeline.transform(df)
```

**Beneficios**:
- ‚úÖ Agregar nuevos transformers no requiere modificar c√≥digo existente
- ‚úÖ Cada transformer es independiente y testeable
- ‚úÖ F√°cil activar/desactivar features espec√≠ficos

#### 4.3 Dependency Inversion Principle (DIP)

**Caso: Database Access**

**Problema actual** (depende de implementaci√≥n concreta):
```python
# src/api/services/feature_engineering.py
class FeatureService:
    def __init__(self):
        self.conn = duckdb.connect("data/steel.duckdb")  # Acoplamiento concreto
    
    def get_reference_data(self):
        return self.conn.execute("SELECT * FROM ref_data").pl()
```

**Refactorizaci√≥n DIP**:
```python
from abc import ABC, abstractmethod

# Abstraction (high-level)
class DataRepository(ABC):
    """
    Abstract repository for data access.
    
    High-level modules depend on this abstraction, not concrete implementations.
    """
    
    @abstractmethod
    def query(self, sql: str) -> pl.DataFrame:
        """Execute SQL query."""
        pass
    
    @abstractmethod
    def get_reference_data(self) -> pl.DataFrame:
        """Get reference dataset."""
        pass


# Concrete implementation (low-level)
class DuckDBRepository(DataRepository):
    """DuckDB implementation of data repository."""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.conn = duckdb.connect(str(db_path))
    
    def query(self, sql: str) -> pl.DataFrame:
        return self.conn.execute(sql).pl()
    
    def get_reference_data(self) -> pl.DataFrame:
        return self.query("SELECT * FROM steel_cleaned LIMIT 10000")


# High-level module (depends on abstraction)
class FeatureService:
    """
    Depends on DataRepository abstraction, not concrete implementation.
    """
    
    def __init__(self, repository: DataRepository):
        self.repository = repository  # Dependency injection
    
    def get_reference_data(self) -> pl.DataFrame:
        return self.repository.get_reference_data()


# Usage (FLEXIBLE)
# Producci√≥n: DuckDB
duckdb_repo = DuckDBRepository(DUCKDB_PATH)
feature_service = FeatureService(duckdb_repo)

# Testing: Mock repository
class MockRepository(DataRepository):
    def query(self, sql: str) -> pl.DataFrame:
        return pl.DataFrame({"col": [1, 2, 3]})
    
    def get_reference_data(self) -> pl.DataFrame:
        return pl.DataFrame({"Usage_kWh": [10.0, 20.0]})

mock_repo = MockRepository()
test_service = FeatureService(mock_repo)  # F√°cil de testear
```

**Beneficios**:
- ‚úÖ F√°cil cambiar de DuckDB a PostgreSQL sin modificar FeatureService
- ‚úÖ Tests unitarios sin necesidad de base de datos real
- ‚úÖ Inversi√≥n de control (IoC) facilita dependency injection

---

### Fase 5: Docstrings Google Format (3-4 horas)

#### 5.1 Template Docstring Completo

```python
def calculate_temporal_features(
    df: pl.DataFrame,
    time_column: str = "NSM",
    include_cyclic: bool = True
) -> pl.DataFrame:
    """
    Calculate temporal features from time column.
    
    Creates hour, minute, second, and optionally cyclic encodings
    (sine/cosine) of time features. Cyclic encoding is recommended
    for periodic patterns.
    
    Parameters
    ----------
    df : pl.DataFrame
        Input dataframe with time column
    time_column : str, default="NSM"
        Name of column containing seconds from midnight
    include_cyclic : bool, default=True
        Whether to include sin/cos cyclic encodings
        
    Returns
    -------
    pl.DataFrame
        DataFrame with additional temporal features:
        - hour (int): Hour of day (0-23)
        - minute (int): Minute of hour (0-59)
        - second (int): Second of minute (0-59)
        - hour_sin (float): Sine encoding of hour (if include_cyclic=True)
        - hour_cos (float): Cosine encoding of hour (if include_cyclic=True)
        
    Raises
    ------
    ValueError
        If time_column not found in dataframe
    TypeError
        If time_column is not numeric
        
    Examples
    --------
    >>> import polars as pl
    >>> df = pl.DataFrame({"NSM": [0, 3600, 7200]})
    >>> result = calculate_temporal_features(df)
    >>> print(result)
    shape: (3, 6)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ NSM   ‚îÇ hour ‚îÇ minute ‚îÇ second ‚îÇ hour_sin ‚îÇ hour_cos ‚îÇ
    ‚îÇ ---   ‚îÇ ---  ‚îÇ ---    ‚îÇ ---    ‚îÇ ---      ‚îÇ ---      ‚îÇ
    ‚îÇ i64   ‚îÇ i32  ‚îÇ i32    ‚îÇ i32    ‚îÇ f64      ‚îÇ f64      ‚îÇ
    ‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
    ‚îÇ 0     ‚îÇ 0    ‚îÇ 0      ‚îÇ 0      ‚îÇ 0.0      ‚îÇ 1.0      ‚îÇ
    ‚îÇ 3600  ‚îÇ 1    ‚îÇ 0      ‚îÇ 0      ‚îÇ 0.2588   ‚îÇ 0.9659   ‚îÇ
    ‚îÇ 7200  ‚îÇ 2    ‚îÇ 0      ‚îÇ 0      ‚îÇ 0.5      ‚îÇ 0.866    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    Without cyclic encoding:
    >>> result = calculate_temporal_features(df, include_cyclic=False)
    >>> print(result.columns)
    ['NSM', 'hour', 'minute', 'second']
    
    Notes
    -----
    Cyclic encoding preserves temporal continuity:
    - Hour 23 is close to hour 0 (end/start of day)
    - Uses sine/cosine to map circular time to 2D coordinates
    
    See Also
    --------
    create_cyclic_features : Create cyclic encodings for any periodic column
    extract_weekday_features : Extract day-of-week features
    
    References
    ----------
    .. [1] "Encoding Cyclical Features for Deep Learning"
           https://www.avanwyk.com/encoding-cyclical-features-for-deep-learning/
    """
    if time_column not in df.columns:
        raise ValueError(f"Column '{time_column}' not found in dataframe")
    
    # Implementation...
```

#### 5.2 Checklist por Funci√≥n

- [ ] Descripci√≥n concisa (1-2 l√≠neas)
- [ ] Descripci√≥n detallada (opcional, 1 p√°rrafo)
- [ ] Parameters con tipos y defaults
- [ ] Returns con tipo y descripci√≥n
- [ ] Raises con excepciones posibles
- [ ] Examples con c√≥digo ejecutable
- [ ] Notes (opcional) con detalles de implementaci√≥n
- [ ] See Also (opcional) con funciones relacionadas
- [ ] References (opcional) para algoritmos complejos

---

### Fase 6: MyPy Type Checking (2-3 horas)

#### 6.1 Configuraci√≥n Estricta de MyPy

```toml
[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true  # ‚¨ÖÔ∏è Ahora strict
disallow_incomplete_defs = true  # ‚¨ÖÔ∏è Ahora strict
check_untyped_defs = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
follow_imports = "normal"
ignore_missing_imports = true

# Per-module overrides
[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false  # Tests pueden ser menos estrictos

[[tool.mypy.overrides]]
module = "scripts.*"
disallow_untyped_defs = false  # Scripts pueden ser menos estrictos
```

#### 6.2 Ejecuci√≥n Incremental

```powershell
# Paso 1: Baseline (ver todos los errores)
poetry run mypy src/ > mypy_baseline.txt

# Paso 2: Arreglar m√≥dulo por m√≥dulo
poetry run mypy src/config/
poetry run mypy src/utils/duckdb_utils.py
poetry run mypy src/api/services/

# Paso 3: Validaci√≥n final
poetry run mypy src/ --no-error-summary
```

#### 6.3 Errores Comunes y Soluciones

**Error 1: Missing type annotations**
```python
# ‚ùå Error
def process(data):
    return data * 2

# ‚úÖ Fix
def process(data: int) -> int:
    return data * 2
```

**Error 2: Incompatible return type**
```python
# ‚ùå Error
def get_value() -> str:
    return None  # Type error

# ‚úÖ Fix
def get_value() -> str | None:
    return None
```

**Error 3: No implicit optional**
```python
# ‚ùå Error
def process(data: str = None):  # Implicit optional
    pass

# ‚úÖ Fix
def process(data: str | None = None):
    pass
```

---

### Fase 7: Testing y Validaci√≥n (3-4 horas)

#### 7.1 Test Suite Completo

```powershell
# 1. Linting (debe pasar sin errores)
poetry run ruff check src/ scripts/ tests/
# Expected: All checks passed!

# 2. Formateo (debe estar clean)
poetry run black --check src/ scripts/ tests/
# Expected: All files left unchanged

# 3. Type checking (debe pasar)
poetry run mypy src/
# Expected: Success: no issues found

# 4. Unit tests (coverage >70%)
poetry run pytest tests/unit/ -v --cov=src --cov-report=term
# Expected: ===== X passed in Y.Ys ===== (coverage ‚â•70%)

# 5. Integration tests
poetry run pytest tests/integration/ -v
# Expected: ===== X passed in Y.Ys =====

# 6. E2E tests
poetry run pytest tests/e2e/ -v
# Expected: ===== X passed in Y.Ys =====
```

#### 7.2 Validaci√≥n de Notebooks

```powershell
# Ejecutar notebooks clave para verificar compatibilidad
jupyter nbconvert --to notebook --execute `
    notebooks/exploratory/01_data_exploration.ipynb `
    --output /tmp/test_notebook.ipynb

# Verificar que imports funcionan
python -c "from src.config.paths import DUCKDB_PATH; print(DUCKDB_PATH)"
python -c "from src.config.constants import OUTLIER_Z_SCORE_THRESHOLD; print(OUTLIER_Z_SCORE_THRESHOLD)"
```

#### 7.3 Smoke Tests de API

```powershell
# Start API
Start-Job -ScriptBlock { poetry run uvicorn src.api.main:app --host 0.0.0.0 --port 8000 }
Start-Sleep -Seconds 5

# Health check
$response = Invoke-RestMethod -Uri "http://localhost:8000/health"
if ($response.status -ne "healthy") { throw "Health check failed" }

# Predict
$body = @{
    lagging_reactive_power = 23.45
    leading_reactive_power = 12.30
    co2 = 0.05
    lagging_power_factor = 0.85
    leading_power_factor = 0.92
    nsm = 36000
    day_of_week = 1
    load_type = "Medium"
} | ConvertTo-Json

$prediction = Invoke-RestMethod -Uri "http://localhost:8000/predict" -Method POST -Body $body -ContentType "application/json"
if ($null -eq $prediction.predicted_usage_kwh) { throw "Prediction failed" }

Write-Host "‚úÖ All smoke tests passed!"
```

---

## üìà M√©tricas de √âxito

### KPIs Cuantitativos

| M√©trica | Baseline | Objetivo | Medici√≥n |
|---------|----------|----------|----------|
| Ruff warnings | 1,864 | 0 | `poetry run ruff check` |
| MyPy errors | ~250 | 0 | `poetry run mypy src/` |
| Type hints coverage | ~70% | 100% | Manual audit |
| Docstrings coverage (public funcs) | ~60% | 100% | Manual audit |
| Test coverage | 72% | >75% | `pytest --cov` |
| Lines of config code | 0 | ~500 | `cloc src/config/` |
| Cyclomatic complexity (avg) | 8.5 | <7 | `radon cc src/` |

### KPIs Cualitativos

- [ ] **Mantenibilidad**: Nuevo desarrollador entiende estructura en <1 d√≠a
- [ ] **Extensibilidad**: Agregar nuevo modelo requiere <50 l√≠neas de c√≥digo
- [ ] **Testabilidad**: Tests unitarios cubren todos los componentes cr√≠ticos
- [ ] **Documentaci√≥n**: Toda funci√≥n p√∫blica autodocumentada
- [ ] **Consistencia**: C√≥digo sigue est√°ndares de AGENTS.md al 100%

---

## üóìÔ∏è Cronograma

| Fase | Duraci√≥n | Responsable | Hitos |
|------|----------|-------------|-------|
| **Fase 1**: Config centralizada | 2-3 hrs | Julian | src/config/ creado |
| **Fase 2**: Ruff auto-fixes | 1-2 hrs | Julian | 80% warnings eliminados |
| **Fase 3**: Type hints | 4-6 hrs | Julian | 100% funciones p√∫blicas anotadas |
| **Fase 4**: SOLID refactoring | 6-8 hrs | Julian + Arthur | 3 casos SOLID implementados |
| **Fase 5**: Docstrings | 3-4 hrs | Julian | 100% docstrings Google format |
| **Fase 6**: MyPy validation | 2-3 hrs | Julian | MyPy pasa sin errores |
| **Fase 7**: Testing | 3-4 hrs | Julian + Team | Todos los tests pasan |
| **TOTAL** | **22-30 hrs** | - | US-030 completada |

**Distribuci√≥n sugerida**:
- Semana 1 (10-12 hrs): Fases 1-3
- Semana 2 (12-18 hrs): Fases 4-7

---

## üîÑ Estrategia de Implementaci√≥n

### Approach: Incremental Refactoring

**‚ùå NO hacer**: "Big Bang" refactoring (todo de una vez)
**‚úÖ S√ç hacer**: Incremental refactoring con validaci√≥n continua

### Branch Strategy

```
main (protected)
  ‚Üì
us-030-refactoring (feature branch)
  ‚Üì
  ‚îú‚îÄ‚îÄ phase-1-config (sub-branch)
  ‚îú‚îÄ‚îÄ phase-2-ruff-fixes (sub-branch)
  ‚îú‚îÄ‚îÄ phase-3-type-hints (sub-branch)
  ‚îú‚îÄ‚îÄ phase-4-solid (sub-branch)
  ‚îî‚îÄ‚îÄ phase-5-docstrings (sub-branch)
```

**Proceso por fase**:
1. Crear sub-branch desde `us-030-refactoring`
2. Implementar cambios
3. Ejecutar tests localmente
4. Push y abrir PR a `us-030-refactoring`
5. Code review + merge
6. Repetir para siguiente fase

### Validaci√≥n Continua

Despu√©s de cada merge a `us-030-refactoring`:
```powershell
# Quick validation
poetry run ruff check src/
poetry run black --check src/
poetry run pytest tests/ -x  # Stop on first failure
```

---

## üö¶ Criterios de Aceptaci√≥n

### Funcionales

- [ ] **CA-1**: Todas las constantes hardcodeadas extra√≠das a `src/config/`
- [ ] **CA-2**: Ruff reporta 0 warnings en src/, scripts/, tests/
- [ ] **CA-3**: MyPy reporta 0 errores en src/ (con config strict)
- [ ] **CA-4**: 100% de funciones p√∫blicas tienen type hints completos
- [ ] **CA-5**: 100% de funciones p√∫blicas tienen docstrings Google format
- [ ] **CA-6**: Al menos 3 casos de SOLID implementados y documentados

### No Funcionales

- [ ] **CA-7**: Test coverage >75%
- [ ] **CA-8**: Todos los tests pasan (unit, integration, e2e)
- [ ] **CA-9**: Notebooks existentes siguen funcionando
- [ ] **CA-10**: API smoke tests pasan
- [ ] **CA-11**: Documentaci√≥n actualizada (README, AGENTS.md)
- [ ] **CA-12**: Commits siguen Conventional Commits

### Calidad

- [ ] **CA-13**: Code review aprobado por al menos 2 miembros del equipo
- [ ] **CA-14**: No se introdujeron bugs (regresiones detectadas en tests)
- [ ] **CA-15**: Performance no degradada (benchmarks iguales o mejores)

---

## üìö Recursos y Referencias

### Documentaci√≥n Interna
- `AGENTS.md` - Est√°ndares del proyecto
- `docs/us-resolved/us-028b.md` - Ejemplo de implementaci√≥n con excelencia
- `docs/us-resolved/us-025.md` - Ejemplo de refactorizaci√≥n bien hecha

### Herramientas
- [Ruff Documentation](https://docs.astral.sh/ruff/)
- [MyPy Documentation](https://mypy.readthedocs.io/)
- [Black Code Style](https://black.readthedocs.io/)
- [Google Python Style Guide - Docstrings](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)

### SOLID Principles
- [SOLID Principles Explained](https://www.digitalocean.com/community/conceptual-articles/s-o-l-i-d-the-first-five-principles-of-object-oriented-design)
- [Python SOLID Examples](https://realpython.com/solid-principles-python/)

---

## üéØ Checklist Final (Antes de Marcar US como Completada)

### C√≥digo
- [ ] Ruff: 0 warnings (`poetry run ruff check src/ scripts/ tests/`)
- [ ] Black: c√≥digo formateado (`poetry run black src/ scripts/ tests/`)
- [ ] MyPy: 0 errores en src/ (`poetry run mypy src/`)
- [ ] Tests: cobertura >75% (`poetry run pytest --cov`)
- [ ] src/config/ creado con 4 archivos (paths, constants, model_config, api_config)
- [ ] Type hints en 100% funciones p√∫blicas
- [ ] Docstrings Google en 100% funciones p√∫blicas
- [ ] Al menos 3 ejemplos de SOLID implementados

### Validaci√≥n
- [ ] Tests unitarios pasan
- [ ] Tests de integraci√≥n pasan
- [ ] Tests e2e pasan
- [ ] Notebooks validados (sample de 3 notebooks ejecutados)
- [ ] API smoke tests pasan
- [ ] Docker build exitoso
- [ ] No regresiones detectadas

### Documentaci√≥n
- [ ] us-030.md actualizado con resoluci√≥n
- [ ] AGENTS.md actualizado si hay nuevos est√°ndares
- [ ] README actualizado con src/config/
- [ ] Comentarios en PRs explicando cambios SOLID

### Git
- [ ] Commits siguen Conventional Commits
- [ ] PR creado de `us-030-refactoring` a `main`
- [ ] Code review aprobado por 2+ personas
- [ ] Merge sin conflictos

---

## üí° Notas Adicionales

### Consideraciones de Performance

Los cambios de refactorizaci√≥n NO deben degradar performance. Validar con benchmarks:

```python
# scripts/benchmark_refactoring.py
import time
import numpy as np
from src.api.services.model_service import ModelService

# Benchmark prediction latency
model_service = ModelService()
features = {...}  # Sample features

times = []
for _ in range(1000):
    start = time.perf_counter()
    prediction = model_service.predict(features)
    times.append(time.perf_counter() - start)

print(f"p50: {np.percentile(times, 50)*1000:.2f}ms")
print(f"p95: {np.percentile(times, 95)*1000:.2f}ms")
print(f"p99: {np.percentile(times, 99)*1000:.2f}ms")
```

**Target**: p95 latency <500ms (igual que baseline)

### Compatibility Checklist

Despu√©s de refactoring, validar:
- [ ] Notebooks existentes funcionan sin cambios
- [ ] Scripts en scripts/ funcionan sin cambios
- [ ] API endpoints responden igual que antes
- [ ] Tests existentes pasan sin modificaci√≥n
- [ ] DVC pipeline funciona

### Aprendizajes para el Equipo

Este US no solo mejora el c√≥digo, sino que establece un **precedente de calidad**:
1. **Toda nueva funci√≥n** debe seguir estos est√°ndares
2. **Pre-commit hooks** deben incluir ruff + black + mypy
3. **PR template** debe incluir checklist de calidad
4. **Code reviews** deben verificar SOLID

---

## üîÆ Mejoras Futuras (Post US-030)

### Automatizaci√≥n
- [ ] Pre-commit hooks con ruff, black, mypy
- [ ] GitHub Actions CI job para code quality
- [ ] Automatic docstring generation con GPT-4
- [ ] Complexity reports en PR comments

### M√©tricas Avanzadas
- [ ] SonarQube integration
- [ ] Code climate integration
- [ ] Technical debt dashboard
- [ ] Complexity trends over time

### Educaci√≥n del Equipo
- [ ] Workshop interno sobre SOLID
- [ ] Gu√≠a de code review con ejemplos
- [ ] Documentaci√≥n de arquitectura (C4 model)

---

## ‚úÖ Aprobaci√≥n del Plan

Este plan debe ser revisado y aprobado por:

- [ ] **Julian** (ML Engineer) - Implementador principal
- [ ] **Arthur** (MLOps) - Revisor de infraestructura y CI/CD
- [ ] **Dante** (Scrum Master) - Validaci√≥n de alcance y cronograma
- [ ] **Erick** (Data Scientist) - Validaci√≥n de impacto en notebooks
- [ ] **Juan** (Data Engineer) - Validaci√≥n de impacto en pipelines

**Una vez aprobado, crear el branch `us-030-refactoring` y comenzar implementaci√≥n.**

---

**Documento creado por**: Arthur (MLOps/SRE Engineer) con asistencia de IA
**Fecha**: 17 de Noviembre, 2025
**Versi√≥n**: 1.0 - Planeaci√≥n Completa
**Estado**: üìã PENDIENTE DE APROBACI√ìN

---

## üìù Anexo: Ejemplos de Refactorizaci√≥n por Archivo

### Anexo A: src/utils/duckdb_utils.py

**Cambios principales**:
1. Import de `src.config.paths import DUCKDB_PATH`
2. Type hints en todas las funciones
3. Docstrings completos
4. Sin trailing whitespace (W293, W291)
5. Imports ordenados (I001)

**Antes** (l√≠neas 1-20):
```python
"""
DuckDB Utilities
...
"""

import duckdb
import polars as pl
from pathlib import Path
from typing import Optional, Union, List, Dict
import logging

logger = logging.getLogger(__name__)


def get_connection(db_path: str = "data/steel.duckdb") -> duckdb.DuckDBPyConnection:
    """Obtiene una conexi√≥n..."""
    from src.data.load_to_duckdb import create_database
    return create_database(db_path)
```

**Despu√©s** (l√≠neas 1-25):
```python
"""
DuckDB Utilities.

Reusable functions for working with DuckDB in notebooks and scripts.
Following clean code and reusability principles.
"""

import logging
from pathlib import Path

import duckdb
import polars as pl

from src.config.paths import DUCKDB_PATH

logger = logging.getLogger(__name__)


def get_connection(db_path: Path | str = DUCKDB_PATH) -> duckdb.DuckDBPyConnection:
    """
    Get connection to DuckDB database.

    Simple wrapper for creating connections consistently across the project.

    Parameters
    ----------
    db_path : Path | str, default=DUCKDB_PATH
        Path to DuckDB database file

    Returns
    -------
    duckdb.DuckDBPyConnection
        Active database connection

    Examples
    --------
    >>> from src.utils.duckdb_utils import get_connection
    >>> conn = get_connection()
    >>> # Use connection...
    >>> conn.close()
    """
    from src.data.load_to_duckdb import create_database
    return create_database(str(db_path))
```

**Total de cambios**:
- +15 l√≠neas (docstrings extendidos)
- Tipo `Path | str` en lugar de `str`
- Import de constante DUCKDB_PATH
- Imports ordenados alfab√©ticamente

---

**FIN DEL PLAN US-030**

Este plan est√° listo para revisi√≥n y aprobaci√≥n del equipo. Una vez aprobado, se proceder√° con la implementaci√≥n fase por fase siguiendo la estrategia incremental propuesta.
