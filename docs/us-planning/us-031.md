# US-031: Endpoint /copilot/chat en FastAPI - Plan de ImplementaciÃ³n

**Estado**: ğŸ“‹ EN PLANIFICACIÃ“N  
**Fecha de CreaciÃ³n**: 17 de Noviembre, 2025  
**Responsable**: Julian (ML Engineer) + Arthur (MLOps)  
**Sprint**: Sprint 3 - Copilot & Deployment  
**Tipo**: Backend + AI Integration + LLM Orchestration

---

## ğŸ“‹ Resumen Ejecutivo

### Objetivo

Implementar un endpoint RESTful `/copilot/chat` en FastAPI que procese consultas en lenguaje natural, orqueste llamadas al LLM (Ollama/Gemini), integre predicciones del modelo de ML, y retorne respuestas contextualizadas para el frontend.

### Alcance

**Incluye**:
- âœ… Endpoint POST `/copilot/chat` con validaciÃ³n Pydantic
- âœ… System prompt especializado en optimizaciÃ³n energÃ©tica
- âœ… IntegraciÃ³n con Ollama (dev) y Gemini (prod) usando factory pattern de US-029
- âœ… OrquestaciÃ³n de predicciones del modelo de ML cuando sea necesario
- âœ… Parsing de intents (predicciÃ³n, anÃ¡lisis, what-if, explicaciÃ³n)
- âœ… GestiÃ³n de contexto conversacional (historial de mensajes)
- âœ… Manejo robusto de errores y timeouts
- âœ… Tests unitarios y de integraciÃ³n (>80% coverage)
- âœ… DocumentaciÃ³n OpenAPI automÃ¡tica

**No Incluye** (futuras iteraciones):
- âŒ RAG (Retrieval Augmented Generation) con datos histÃ³ricos
- âŒ Fine-tuning de modelos LLM
- âŒ Streaming de respuestas (SSE)
- âŒ AutenticaciÃ³n/autorizaciÃ³n de usuarios
- âŒ Rate limiting por usuario


### Valor de Negocio

- **Para operadores**: Interfaz conversacional intuitiva para consultar consumo energÃ©tico
- **Para el proyecto**: Diferenciador clave vs soluciones tradicionales de ML
- **Para el equipo**: IntegraciÃ³n completa de LLM + ML en producciÃ³n

---

## ğŸ¯ Criterios de AceptaciÃ³n

### Funcionales

| ID | Criterio | Prioridad | ValidaciÃ³n |
|----|----------|-----------|------------|
| **CA-F1** | Endpoint POST `/copilot/chat` responde con 200 OK | ğŸ”´ ALTA | curl/Postman |
| **CA-F2** | Acepta input: `{"message": "texto", "conversation_id": "uuid"}` | ğŸ”´ ALTA | Pydantic validation |
| **CA-F3** | Retorna: `{"response": "texto", "prediction_data": {...}, "conversation_id": "uuid"}` | ğŸ”´ ALTA | JSON schema |
| **CA-F4** | System prompt instruye al LLM sobre dominio energÃ©tico | ğŸ”´ ALTA | Prompt testing |
| **CA-F5** | Detecta intents: predicciÃ³n, anÃ¡lisis, what-if, explicaciÃ³n, general | ğŸ”´ ALTA | Intent parser tests |
| **CA-F6** | Orquesta predicciÃ³n del modelo cuando intent = "predicciÃ³n" | ğŸ”´ ALTA | Integration test |
| **CA-F7** | Mantiene contexto conversacional (Ãºltimos 10 mensajes) | ğŸŸ¡ MEDIA | Conversation tests |
| **CA-F8** | Usa Ollama en dev, Gemini en prod (vÃ­a LLM_PROVIDER) | ğŸ”´ ALTA | Factory pattern |
| **CA-F9** | Maneja errores del LLM con mensajes user-friendly | ğŸ”´ ALTA | Error handling tests |
| **CA-F10** | Timeout de 30s para LLM, 10s para predicciÃ³n | ğŸŸ¡ MEDIA | Timeout tests |

### No Funcionales

| ID | Criterio | Prioridad | ValidaciÃ³n |
|----|----------|-----------|------------|
| **CA-NF1** | Latencia p95 < 5s (Ollama) / < 2s (Gemini) | ğŸŸ¡ MEDIA | Benchmark script |
| **CA-NF2** | Test coverage > 80% en mÃ³dulos nuevos | ğŸ”´ ALTA | pytest --cov |
| **CA-NF3** | DocumentaciÃ³n OpenAPI completa en /docs | ğŸ”´ ALTA | Swagger UI |
| **CA-NF4** | Logging estructurado de requests/responses | ğŸ”´ ALTA | Log inspection |
| **CA-NF5** | Type hints 100% en cÃ³digo nuevo | ğŸ”´ ALTA | mypy |
| **CA-NF6** | Docstrings estilo Google en funciones pÃºblicas | ğŸ”´ ALTA | Code review |
| **CA-NF7** | Cumplimiento 100% con AGENTS.md | ğŸ”´ ALTA | Checklist |

### Calidad

| ID | Criterio | Prioridad | ValidaciÃ³n |
|----|----------|-----------|------------|
| **CA-Q1** | CÃ³digo formateado con Black | ğŸ”´ ALTA | black --check |
| **CA-Q2** | Sin warnings de Ruff en cÃ³digo nuevo | ğŸ”´ ALTA | ruff check |
| **CA-Q3** | Tests unitarios pasan (mocks de LLM) | ğŸ”´ ALTA | pytest unit/ |
| **CA-Q4** | Tests de integraciÃ³n pasan (Ollama real) | ğŸŸ¡ MEDIA | pytest integration/ |
| **CA-Q5** | Notebook de ejemplos funcional | ğŸŸ¡ MEDIA | Manual testing |

---

## ğŸ—ï¸ Arquitectura Propuesta

### Flujo de Datos

```
Frontend (Streamlit)
    â†“
POST /copilot/chat
    â†“
ChatRequest (Pydantic validation)
    â†“
CopilotService.process_message()
    â†“
    â”œâ”€â†’ IntentParser.parse(message)
    â”‚       â†“
    â”‚   Intent: prediction | analysis | what-if | explanation | general
    â”‚
    â”œâ”€â†’ ConversationManager.get_context(conversation_id)
    â”‚       â†“
    â”‚   Last 10 messages from cache/DB
    â”‚
    â”œâ”€â†’ PromptBuilder.build_prompt(intent, context, message)
    â”‚       â†“
    â”‚   System prompt + user message + context
    â”‚
    â”œâ”€â†’ LLMClient.chat(prompt)  [Factory: Ollama/Gemini]
    â”‚       â†“
    â”‚   LLM response (text)
    â”‚
    â”œâ”€â†’ [IF intent == "prediction"]
    â”‚   â”‚
    â”‚   â”œâ”€â†’ ParameterExtractor.extract(message)
    â”‚   â”‚       â†“
    â”‚   â”‚   {lagging_reactive_power: 23.45, ...}
    â”‚   â”‚
    â”‚   â””â”€â†’ ModelService.predict(features)
    â”‚           â†“
    â”‚       {predicted_usage_kwh: 44.456, ...}
    â”‚
    â””â”€â†’ ResponseFormatter.format(llm_response, prediction_data)
            â†“
        ChatResponse (JSON)
            â†“
        Frontend
```


### Estructura de CÃ³digo

```
src/api/
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ copilot.py                    # POST /copilot/chat endpoint (150 lÃ­neas)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ copilot_requests.py           # ChatRequest, ConversationMessage (100 lÃ­neas)
â”‚   â””â”€â”€ copilot_responses.py          # ChatResponse, PredictionData (120 lÃ­neas)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ copilot_service.py            # OrquestaciÃ³n principal (300 lÃ­neas)
â”‚   â”œâ”€â”€ intent_parser.py              # DetecciÃ³n de intents (200 lÃ­neas)
â”‚   â”œâ”€â”€ prompt_builder.py             # ConstrucciÃ³n de prompts (250 lÃ­neas)
â”‚   â”œâ”€â”€ parameter_extractor.py        # ExtracciÃ³n de parÃ¡metros (180 lÃ­neas)
â”‚   â”œâ”€â”€ conversation_manager.py       # GestiÃ³n de contexto (200 lÃ­neas)
â”‚   â””â”€â”€ response_formatter.py         # Formateo de respuestas (150 lÃ­neas)
â””â”€â”€ utils/
    â””â”€â”€ llm_utils.py                  # Helpers para LLM (100 lÃ­neas)

config/
â””â”€â”€ prompts/
    â”œâ”€â”€ system_prompt.txt             # System prompt principal (50 lÃ­neas)
    â”œâ”€â”€ prediction_prompt.txt         # Template para predicciones (30 lÃ­neas)
    â”œâ”€â”€ analysis_prompt.txt           # Template para anÃ¡lisis (30 lÃ­neas)
    â””â”€â”€ whatif_prompt.txt             # Template para what-if (30 lÃ­neas)

tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_copilot_endpoint.py     # Tests del endpoint (200 lÃ­neas)
â”‚   â”œâ”€â”€ test_intent_parser.py        # Tests de parsing (150 lÃ­neas)
â”‚   â”œâ”€â”€ test_prompt_builder.py       # Tests de prompts (150 lÃ­neas)
â”‚   â”œâ”€â”€ test_parameter_extractor.py  # Tests de extracciÃ³n (150 lÃ­neas)
â”‚   â””â”€â”€ test_conversation_manager.py # Tests de contexto (150 lÃ­neas)
â””â”€â”€ integration/
    â””â”€â”€ test_copilot_integration.py  # Tests end-to-end (250 lÃ­neas)

notebooks/
â””â”€â”€ experimental/
    â””â”€â”€ copilot_usage_examples.ipynb # Ejemplos de uso (10 secciones)
```

**Total estimado**: ~2,500 lÃ­neas de cÃ³digo + tests + documentaciÃ³n

---

## ğŸ’¡ DiseÃ±o Detallado

### 1. Endpoint `/copilot/chat`

**Archivo**: `src/api/routes/copilot.py`

```python
from fastapi import APIRouter, HTTPException, Depends
from src.api.models.copilot_requests import ChatRequest
from src.api.models.copilot_responses import ChatResponse
from src.api.services.copilot_service import CopilotService
import logging

router = APIRouter(prefix="/copilot", tags=["Copilot"])
logger = logging.getLogger(__name__)

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    Process natural language queries about energy consumption.
    
    Supports multiple intents:
    - Prediction: "What will be the consumption tomorrow at 10am?"
    - Analysis: "Why was there a spike last Friday?"
    - What-if: "How much would I save if I shift production to 2am?"
    - Explanation: "Explain the factors affecting consumption"
    - General: "What is power factor?"
    
    Parameters
    ----------
    request : ChatRequest
        User message and optional conversation_id
        
    Returns
    -------
    ChatResponse
        LLM response with optional prediction data
        
    Raises
    ------
    HTTPException
        400: Invalid request
        500: LLM error
        503: Service unavailable
    """
    try:
        copilot_service = CopilotService()
        response = await copilot_service.process_message(
            message=request.message,
            conversation_id=request.conversation_id,
            context=request.context
        )
        return response
    except ValueError as e:
        logger.error(f"Validation error: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except TimeoutError as e:
        logger.error(f"Timeout error: {e}")
        raise HTTPException(status_code=504, detail="Request timeout")
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error")
```


### 2. Modelos Pydantic

**Archivo**: `src/api/models/copilot_requests.py`

```python
from pydantic import BaseModel, Field, field_validator
from typing import Literal
import uuid

class ConversationMessage(BaseModel):
    """Single message in conversation history."""
    role: Literal["user", "assistant", "system"]
    content: str = Field(..., min_length=1, max_length=10000)
    timestamp: str | None = None

class ChatRequest(BaseModel):
    """Request model for /copilot/chat endpoint."""
    
    message: str = Field(
        ...,
        min_length=1,
        max_length=2000,
        description="User query in natural language",
        examples=["What will be the consumption tomorrow at 10am with Medium load?"]
    )
    
    conversation_id: str | None = Field(
        default=None,
        description="UUID for conversation tracking. Auto-generated if not provided."
    )
    
    context: list[ConversationMessage] | None = Field(
        default=None,
        max_length=10,
        description="Previous messages for context (max 10)"
    )
    
    @field_validator("conversation_id")
    @classmethod
    def validate_conversation_id(cls, v: str | None) -> str:
        """Generate UUID if not provided."""
        if v is None:
            return str(uuid.uuid4())
        # Validate UUID format
        try:
            uuid.UUID(v)
            return v
        except ValueError:
            raise ValueError(f"Invalid conversation_id format: {v}")
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "message": "What will be the consumption tomorrow at 10am?",
                    "conversation_id": "550e8400-e29b-41d4-a716-446655440000"
                }
            ]
        }
    }
```

**Archivo**: `src/api/models/copilot_responses.py`

```python
from pydantic import BaseModel, Field
from typing import Literal

class PredictionData(BaseModel):
    """Prediction data when intent is 'prediction'."""
    predicted_usage_kwh: float = Field(..., description="Predicted energy consumption")
    confidence_score: float | None = Field(None, ge=0.0, le=1.0)
    model_version: str
    features_used: dict[str, float]
    
class ChatResponse(BaseModel):
    """Response model for /copilot/chat endpoint."""
    
    response: str = Field(
        ...,
        description="LLM-generated response to user query"
    )
    
    conversation_id: str = Field(
        ...,
        description="UUID for conversation tracking"
    )
    
    intent: Literal["prediction", "analysis", "what-if", "explanation", "general"] = Field(
        ...,
        description="Detected intent of user query"
    )
    
    prediction_data: PredictionData | None = Field(
        default=None,
        description="Prediction results if intent is 'prediction'"
    )
    
    sources: list[str] | None = Field(
        default=None,
        description="Data sources used (for future RAG implementation)"
    )
    
    processing_time_ms: float = Field(
        ...,
        description="Total processing time in milliseconds"
    )
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "response": "Based on the model, the predicted consumption...",
                    "conversation_id": "550e8400-e29b-41d4-a716-446655440000",
                    "intent": "prediction",
                    "prediction_data": {
                        "predicted_usage_kwh": 44.456,
                        "model_version": "stacking_ensemble_v1",
                        "features_used": {"lagging_reactive_power": 23.45}
                    },
                    "processing_time_ms": 2345.67
                }
            ]
        }
    }
```


### 3. CopilotService - OrquestaciÃ³n Principal

**Archivo**: `src/api/services/copilot_service.py`

```python
from src.llm import get_llm_client
from src.api.services.intent_parser import IntentParser
from src.api.services.prompt_builder import PromptBuilder
from src.api.services.parameter_extractor import ParameterExtractor
from src.api.services.conversation_manager import ConversationManager
from src.api.services.response_formatter import ResponseFormatter
from src.api.services.model_service import ModelService
from src.api.models.copilot_responses import ChatResponse
import logging
import time

logger = logging.getLogger(__name__)

class CopilotService:
    """
    Main orchestration service for copilot functionality.
    
    Coordinates:
    - Intent detection
    - Prompt building
    - LLM interaction
    - Model prediction (when needed)
    - Response formatting
    """
    
    def __init__(self):
        self.llm_client = get_llm_client()  # Factory pattern from US-029
        self.intent_parser = IntentParser()
        self.prompt_builder = PromptBuilder()
        self.parameter_extractor = ParameterExtractor()
        self.conversation_manager = ConversationManager()
        self.response_formatter = ResponseFormatter()
        self.model_service = ModelService()  # From US-020
        
    async def process_message(
        self,
        message: str,
        conversation_id: str,
        context: list[dict] | None = None
    ) -> ChatResponse:
        """
        Process user message and generate response.
        
        Parameters
        ----------
        message : str
            User query in natural language
        conversation_id : str
            UUID for conversation tracking
        context : list[dict] | None
            Previous messages for context
            
        Returns
        -------
        ChatResponse
            Formatted response with LLM output and optional prediction
        """
        start_time = time.time()
        
        try:
            # Step 1: Detect intent
            intent = self.intent_parser.parse(message)
            logger.info(f"Detected intent: {intent} for message: {message[:50]}...")
            
            # Step 2: Get conversation context
            if context is None:
                context = self.conversation_manager.get_context(conversation_id)
            
            # Step 3: Build prompt
            prompt = self.prompt_builder.build_prompt(
                intent=intent,
                message=message,
                context=context
            )
            
            # Step 4: Get LLM response
            llm_response = self.llm_client.chat(prompt)
            logger.info(f"LLM response received: {len(llm_response)} chars")
            
            # Step 5: If prediction intent, call model
            prediction_data = None
            if intent == "prediction":
                try:
                    # Extract parameters from message
                    params = self.parameter_extractor.extract(message)
                    logger.info(f"Extracted parameters: {params}")
                    
                    # Make prediction
                    prediction = self.model_service.predict(params)
                    prediction_data = {
                        "predicted_usage_kwh": prediction["predicted_usage_kwh"],
                        "model_version": prediction["model_version"],
                        "features_used": params
                    }
                except Exception as e:
                    logger.error(f"Prediction failed: {e}")
                    # Continue without prediction data
            
            # Step 6: Format response
            processing_time = (time.time() - start_time) * 1000
            
            response = self.response_formatter.format(
                llm_response=llm_response,
                intent=intent,
                prediction_data=prediction_data,
                conversation_id=conversation_id,
                processing_time_ms=processing_time
            )
            
            # Step 7: Save to conversation history
            self.conversation_manager.add_message(
                conversation_id=conversation_id,
                role="user",
                content=message
            )
            self.conversation_manager.add_message(
                conversation_id=conversation_id,
                role="assistant",
                content=response.response
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            raise
```


### 4. IntentParser - DetecciÃ³n de Intenciones

**Archivo**: `src/api/services/intent_parser.py`

```python
import re
from typing import Literal

Intent = Literal["prediction", "analysis", "what-if", "explanation", "general"]

class IntentParser:
    """
    Parses user message to detect intent.
    
    Uses keyword matching and regex patterns.
    """
    
    INTENT_PATTERNS = {
        "prediction": [
            r"\b(predict|forecast|estimate|will be|consumption|usage)\b",
            r"\b(tomorrow|next|future|at \d+am|at \d+pm)\b",
            r"\b(how much|what will|cuÃ¡nto|quÃ© serÃ¡)\b"
        ],
        "what-if": [
            r"\b(what if|if I|suppose|scenario|cambio|shift)\b",
            r"\b(save|reduce|optimize|ahorrar|reducir)\b",
            r"\b(change|modify|adjust|cambiar)\b"
        ],
        "analysis": [
            r"\b(why|reason|cause|por quÃ©|causa)\b",
            r"\b(spike|peak|high|low|pico|alto|bajo)\b",
            r"\b(analyze|analysis|trend|patrÃ³n)\b"
        ],
        "explanation": [
            r"\b(explain|what is|how does|quÃ© es|cÃ³mo funciona)\b",
            r"\b(factor|affect|influence|factor|afecta)\b",
            r"\b(definition|meaning|significado)\b"
        ]
    }
    
    def parse(self, message: str) -> Intent:
        """
        Detect intent from user message.
        
        Parameters
        ----------
        message : str
            User query
            
        Returns
        -------
        Intent
            Detected intent: prediction, analysis, what-if, explanation, or general
        """
        message_lower = message.lower()
        
        # Score each intent
        scores = {intent: 0 for intent in self.INTENT_PATTERNS.keys()}
        
        for intent, patterns in self.INTENT_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, message_lower):
                    scores[intent] += 1
        
        # Get intent with highest score
        max_score = max(scores.values())
        
        if max_score == 0:
            return "general"
        
        # Return intent with highest score
        for intent, score in scores.items():
            if score == max_score:
                return intent
        
        return "general"
```

### 5. PromptBuilder - ConstrucciÃ³n de Prompts

**Archivo**: `src/api/services/prompt_builder.py`

```python
from pathlib import Path
from typing import Literal

Intent = Literal["prediction", "analysis", "what-if", "explanation", "general"]

class PromptBuilder:
    """
    Builds prompts for LLM based on intent and context.
    """
    
    def __init__(self, prompts_dir: Path = Path("config/prompts")):
        self.prompts_dir = prompts_dir
        self.system_prompt = self._load_system_prompt()
        
    def _load_system_prompt(self) -> str:
        """Load system prompt from file."""
        system_prompt_path = self.prompts_dir / "system_prompt.txt"
        if system_prompt_path.exists():
            return system_prompt_path.read_text(encoding="utf-8")
        return self._get_default_system_prompt()
    
    def _get_default_system_prompt(self) -> str:
        """Default system prompt if file not found."""
        return """You are an AI assistant specialized in energy optimization for the steel industry.

Your role:
- Help operators understand energy consumption patterns
- Provide predictions based on ML models
- Explain factors affecting consumption
- Suggest optimization strategies

Guidelines:
- Be concise and technical but accessible
- Use specific numbers when available
- Explain your reasoning
- If you don't have data, say so clearly
- Focus on actionable insights

Context:
- Steel plant in South Korea
- Data from 2018, 15-minute intervals
- Key factors: reactive power, CO2, power factor, load type, time
- Target: Usage_kWh (energy consumption)
"""
    
    def build_prompt(
        self,
        intent: Intent,
        message: str,
        context: list[dict] | None = None
    ) -> list[dict]:
        """
        Build prompt messages for LLM.
        
        Parameters
        ----------
        intent : Intent
            Detected intent
        message : str
            User message
        context : list[dict] | None
            Previous conversation messages
            
        Returns
        -------
        list[dict]
            Messages in format [{"role": "system", "content": "..."}, ...]
        """
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        # Add intent-specific instructions
        intent_instruction = self._get_intent_instruction(intent)
        if intent_instruction:
            messages.append({
                "role": "system",
                "content": intent_instruction
            })
        
        # Add conversation context
        if context:
            messages.extend(context[-10:])  # Last 10 messages
        
        # Add current user message
        messages.append({
            "role": "user",
            "content": message
        })
        
        return messages
    
    def _get_intent_instruction(self, intent: Intent) -> str | None:
        """Get intent-specific instruction."""
        instructions = {
            "prediction": "The user wants a prediction. Extract parameters and provide a forecast.",
            "what-if": "The user wants to explore a scenario. Compare current vs proposed situation.",
            "analysis": "The user wants to understand why something happened. Analyze patterns.",
            "explanation": "The user wants to learn. Explain concepts clearly.",
            "general": None
        }
        return instructions.get(intent)
```


### 6. ParameterExtractor - ExtracciÃ³n de ParÃ¡metros

**Archivo**: `src/api/services/parameter_extractor.py`

```python
import re
from datetime import datetime, timedelta

class ParameterExtractor:
    """
    Extracts prediction parameters from natural language.
    
    Handles:
    - Time expressions (tomorrow, 10am, next Monday)
    - Load types (Light, Medium, Maximum)
    - Numerical values (reactive power, CO2, etc.)
    """
    
    LOAD_TYPE_MAPPING = {
        "light": "Light",
        "medium": "Medium",
        "maximum": "Maximum",
        "max": "Maximum",
        "high": "Maximum",
        "low": "Light"
    }
    
    def extract(self, message: str) -> dict:
        """
        Extract prediction parameters from message.
        
        Parameters
        ----------
        message : str
            User message
            
        Returns
        -------
        dict
            Extracted parameters with defaults for missing values
        """
        params = {}
        
        # Extract time
        nsm = self._extract_time(message)
        if nsm is not None:
            params["nsm"] = nsm
        else:
            params["nsm"] = 36000  # Default: 10am
        
        # Extract day of week
        day_of_week = self._extract_day_of_week(message)
        params["day_of_week"] = day_of_week
        
        # Extract load type
        load_type = self._extract_load_type(message)
        params["load_type"] = load_type
        
        # Use default values for other parameters
        params.update({
            "lagging_reactive_power": 23.45,
            "leading_reactive_power": 12.30,
            "co2": 0.05,
            "lagging_power_factor": 0.85,
            "leading_power_factor": 0.92
        })
        
        return params
    
    def _extract_time(self, message: str) -> int | None:
        """Extract time and convert to NSM (seconds from midnight)."""
        message_lower = message.lower()
        
        # Pattern: "10am", "2pm", "14:00"
        time_pattern = r"(\d{1,2})(?::(\d{2}))?\s*(am|pm)?"
        match = re.search(time_pattern, message_lower)
        
        if match:
            hour = int(match.group(1))
            minute = int(match.group(2)) if match.group(2) else 0
            period = match.group(3)
            
            # Convert to 24-hour format
            if period == "pm" and hour != 12:
                hour += 12
            elif period == "am" and hour == 12:
                hour = 0
            
            # Convert to NSM
            nsm = hour * 3600 + minute * 60
            return nsm
        
        return None
    
    def _extract_day_of_week(self, message: str) -> int:
        """Extract day of week (0=Monday, 6=Sunday)."""
        message_lower = message.lower()
        
        # Check for relative days
        if "tomorrow" in message_lower:
            tomorrow = datetime.now() + timedelta(days=1)
            return tomorrow.weekday()
        elif "today" in message_lower:
            return datetime.now().weekday()
        
        # Check for specific days
        days = {
            "monday": 0, "tuesday": 1, "wednesday": 2,
            "thursday": 3, "friday": 4, "saturday": 5, "sunday": 6,
            "lunes": 0, "martes": 1, "miÃ©rcoles": 2,
            "jueves": 3, "viernes": 4, "sÃ¡bado": 5, "domingo": 6
        }
        
        for day_name, day_num in days.items():
            if day_name in message_lower:
                return day_num
        
        # Default: Monday
        return 0
    
    def _extract_load_type(self, message: str) -> str:
        """Extract load type."""
        message_lower = message.lower()
        
        for keyword, load_type in self.LOAD_TYPE_MAPPING.items():
            if keyword in message_lower:
                return load_type
        
        # Default: Medium
        return "Medium"
```


### 7. ConversationManager - GestiÃ³n de Contexto

**Archivo**: `src/api/services/conversation_manager.py`

```python
from collections import defaultdict
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ConversationManager:
    """
    Manages conversation history and context.
    
    Uses in-memory storage (dict) for MVP.
    Future: Redis or database for persistence.
    """
    
    def __init__(self, max_messages: int = 10):
        self.conversations: dict[str, list[dict]] = defaultdict(list)
        self.max_messages = max_messages
        
    def get_context(self, conversation_id: str) -> list[dict]:
        """
        Get conversation context.
        
        Parameters
        ----------
        conversation_id : str
            Conversation UUID
            
        Returns
        -------
        list[dict]
            Last N messages in format [{"role": "user", "content": "..."}]
        """
        messages = self.conversations.get(conversation_id, [])
        return messages[-self.max_messages:]
    
    def add_message(
        self,
        conversation_id: str,
        role: str,
        content: str
    ) -> None:
        """
        Add message to conversation history.
        
        Parameters
        ----------
        conversation_id : str
            Conversation UUID
        role : str
            Message role: "user", "assistant", or "system"
        content : str
            Message content
        """
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        }
        
        self.conversations[conversation_id].append(message)
        
        # Trim to max_messages
        if len(self.conversations[conversation_id]) > self.max_messages * 2:
            self.conversations[conversation_id] = \
                self.conversations[conversation_id][-self.max_messages * 2:]
        
        logger.debug(f"Added message to conversation {conversation_id}: {role}")
    
    def clear_conversation(self, conversation_id: str) -> None:
        """Clear conversation history."""
        if conversation_id in self.conversations:
            del self.conversations[conversation_id]
            logger.info(f"Cleared conversation {conversation_id}")
    
    def get_all_conversations(self) -> dict[str, list[dict]]:
        """Get all conversations (for debugging)."""
        return dict(self.conversations)
```

### 8. ResponseFormatter - Formateo de Respuestas

**Archivo**: `src/api/services/response_formatter.py`

```python
from src.api.models.copilot_responses import ChatResponse, PredictionData
from typing import Literal

Intent = Literal["prediction", "analysis", "what-if", "explanation", "general"]

class ResponseFormatter:
    """
    Formats LLM response into ChatResponse model.
    """
    
    def format(
        self,
        llm_response: str,
        intent: Intent,
        prediction_data: dict | None,
        conversation_id: str,
        processing_time_ms: float
    ) -> ChatResponse:
        """
        Format response for API.
        
        Parameters
        ----------
        llm_response : str
            Raw LLM response
        intent : Intent
            Detected intent
        prediction_data : dict | None
            Prediction results if available
        conversation_id : str
            Conversation UUID
        processing_time_ms : float
            Processing time in milliseconds
            
        Returns
        -------
        ChatResponse
            Formatted response
        """
        # Build prediction data model if available
        prediction_model = None
        if prediction_data:
            prediction_model = PredictionData(**prediction_data)
        
        return ChatResponse(
            response=llm_response,
            conversation_id=conversation_id,
            intent=intent,
            prediction_data=prediction_model,
            processing_time_ms=processing_time_ms
        )
```

---

## ğŸ§ª Plan de Testing

### Tests Unitarios

**Archivo**: `tests/unit/test_copilot_endpoint.py`

```python
import pytest
from fastapi.testclient import TestClient
from src.api.main import app
from unittest.mock import patch, MagicMock

client = TestClient(app)

class TestCopilotEndpoint:
    """Tests for /copilot/chat endpoint."""
    
    @patch("src.api.services.copilot_service.CopilotService.process_message")
    def test_chat_success(self, mock_process):
        """Test successful chat request."""
        mock_process.return_value = {
            "response": "The predicted consumption is 44.5 kWh",
            "conversation_id": "test-uuid",
            "intent": "prediction",
            "prediction_data": {
                "predicted_usage_kwh": 44.5,
                "model_version": "v1"
            },
            "processing_time_ms": 1234.56
        }
        
        response = client.post("/copilot/chat", json={
            "message": "What will be the consumption tomorrow at 10am?"
        })
        
        assert response.status_code == 200
        data = response.json()
        assert "response" in data
        assert data["intent"] == "prediction"
    
    def test_chat_empty_message(self):
        """Test validation error for empty message."""
        response = client.post("/copilot/chat", json={
            "message": ""
        })
        
        assert response.status_code == 422  # Validation error
    
    def test_chat_message_too_long(self):
        """Test validation error for message too long."""
        response = client.post("/copilot/chat", json={
            "message": "x" * 3000  # Exceeds 2000 char limit
        })
        
        assert response.status_code == 422
```

**Archivo**: `tests/unit/test_intent_parser.py`

```python
import pytest
from src.api.services.intent_parser import IntentParser

class TestIntentParser:
    """Tests for IntentParser."""
    
    def setup_method(self):
        self.parser = IntentParser()
    
    def test_parse_prediction_intent(self):
        """Test detection of prediction intent."""
        messages = [
            "What will be the consumption tomorrow?",
            "Predict usage at 10am",
            "How much energy will we use next Monday?"
        ]
        
        for msg in messages:
            intent = self.parser.parse(msg)
            assert intent == "prediction", f"Failed for: {msg}"
    
    def test_parse_whatif_intent(self):
        """Test detection of what-if intent."""
        messages = [
            "What if I shift production to 2am?",
            "How much would I save if I change the load?",
            "Suppose we reduce the power factor"
        ]
        
        for msg in messages:
            intent = self.parser.parse(msg)
            assert intent == "what-if", f"Failed for: {msg}"
    
    def test_parse_analysis_intent(self):
        """Test detection of analysis intent."""
        messages = [
            "Why was there a spike last Friday?",
            "Analyze the consumption pattern",
            "What caused the high usage?"
        ]
        
        for msg in messages:
            intent = self.parser.parse(msg)
            assert intent == "analysis", f"Failed for: {msg}"
    
    def test_parse_general_intent(self):
        """Test fallback to general intent."""
        messages = [
            "Hello",
            "Thanks",
            "Random text without keywords"
        ]
        
        for msg in messages:
            intent = self.parser.parse(msg)
            assert intent == "general", f"Failed for: {msg}"
```


### Tests de IntegraciÃ³n

**Archivo**: `tests/integration/test_copilot_integration.py`

```python
import pytest
from fastapi.testclient import TestClient
from src.api.main import app
import os

client = TestClient(app)

@pytest.mark.integration
@pytest.mark.skipif(
    os.getenv("OLLAMA_HOST") is None,
    reason="Ollama not available"
)
class TestCopilotIntegration:
    """Integration tests with real Ollama."""
    
    def test_prediction_query_end_to_end(self):
        """Test full prediction flow with real LLM."""
        response = client.post("/copilot/chat", json={
            "message": "What will be the consumption tomorrow at 10am with Medium load?"
        })
        
        assert response.status_code == 200
        data = response.json()
        
        # Verify response structure
        assert "response" in data
        assert "conversation_id" in data
        assert "intent" in data
        assert data["intent"] == "prediction"
        
        # Verify prediction data
        assert "prediction_data" in data
        assert data["prediction_data"] is not None
        assert "predicted_usage_kwh" in data["prediction_data"]
        assert data["prediction_data"]["predicted_usage_kwh"] > 0
    
    def test_conversation_context(self):
        """Test conversation context is maintained."""
        # First message
        response1 = client.post("/copilot/chat", json={
            "message": "What is power factor?"
        })
        
        assert response1.status_code == 200
        conv_id = response1.json()["conversation_id"]
        
        # Second message with context
        response2 = client.post("/copilot/chat", json={
            "message": "How does it affect consumption?",
            "conversation_id": conv_id
        })
        
        assert response2.status_code == 200
        assert response2.json()["conversation_id"] == conv_id
    
    def test_latency_requirement(self):
        """Test latency is within acceptable range."""
        import time
        
        start = time.time()
        response = client.post("/copilot/chat", json={
            "message": "Explain energy consumption"
        })
        elapsed = (time.time() - start) * 1000
        
        assert response.status_code == 200
        
        # Latency should be < 5s for Ollama
        assert elapsed < 5000, f"Latency too high: {elapsed}ms"
```

---

## ğŸ“Š EstimaciÃ³n de Esfuerzo

### Desglose por Componente

| Componente | LÃ­neas | Complejidad | Tiempo (hrs) |
|------------|--------|-------------|--------------|
| **Endpoint /copilot/chat** | 150 | Media | 2 |
| **Modelos Pydantic** | 220 | Baja | 1.5 |
| **CopilotService** | 300 | Alta | 4 |
| **IntentParser** | 200 | Media | 3 |
| **PromptBuilder** | 250 | Media | 3 |
| **ParameterExtractor** | 180 | Alta | 4 |
| **ConversationManager** | 200 | Media | 2.5 |
| **ResponseFormatter** | 150 | Baja | 1 |
| **System Prompts** | 140 | Media | 2 |
| **Tests Unitarios** | 800 | Media | 6 |
| **Tests IntegraciÃ³n** | 250 | Media | 3 |
| **Notebook Ejemplos** | 300 | Baja | 2 |
| **DocumentaciÃ³n** | 500 | Baja | 2 |
| **Debugging & Ajustes** | - | - | 4 |
| **Total** | ~3,640 | - | **40 hrs** |

### DistribuciÃ³n por Rol

| Rol | Tareas | Tiempo |
|-----|--------|--------|
| **Julian (ML Engineer)** | CopilotService, IntentParser, ParameterExtractor, Prompts | 16 hrs |
| **Arthur (MLOps)** | Endpoint, Pydantic models, Tests, IntegraciÃ³n con US-029 | 14 hrs |
| **Dante (Software Engineer)** | ConversationManager, ResponseFormatter, Notebook | 6 hrs |
| **Erick (Data Scientist)** | ValidaciÃ³n de prompts, Testing de intents | 4 hrs |

**Total**: 40 horas (~1 semana con 4 personas)

---

## ğŸ”§ Dependencias TÃ©cnicas

### Dependencias de US Anteriores

| US | Componente Requerido | Estado |
|----|---------------------|--------|
| **US-020** | FastAPI endpoints + ModelService | âœ… Completado |
| **US-029** | Ollama + Llama 3.2 + LLM clients | âœ… Completado |
| **US-030** | RefactorizaciÃ³n + SOLID | âœ… Completado |

### Nuevas Dependencias

```toml
# pyproject.toml - Ya incluidas en US-029
[tool.poetry.dependencies]
ollama = "^0.3.0"
google-generativeai = "^0.8.0"
langchain = "^0.3.0"  # Opcional, para futuro RAG
```

### Variables de Entorno

```bash
# .env
# LLM Configuration (from US-029)
LLM_PROVIDER=ollama  # ollama | gemini
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=30
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=1000

GEMINI_API_KEY=your-api-key
GEMINI_MODEL=gemini-2.0-flash-exp
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=1000

# Copilot Configuration (NEW)
COPILOT_MAX_CONTEXT_MESSAGES=10
COPILOT_PREDICTION_TIMEOUT=10
COPILOT_LLM_TIMEOUT=30
```

---

## ğŸ¯ Riesgos y Mitigaciones

### Riesgos Identificados

| Riesgo | Probabilidad | Impacto | MitigaciÃ³n |
|--------|--------------|---------|------------|
| **Latencia alta de Ollama** | Alta | Medio | Usar Gemini en prod, optimizar prompts |
| **ExtracciÃ³n de parÃ¡metros imprecisa** | Media | Alto | ValidaciÃ³n robusta, defaults sensatos |
| **LLM genera respuestas incorrectas** | Media | Alto | System prompt bien diseÃ±ado, validaciÃ³n |
| **Contexto conversacional se pierde** | Baja | Medio | ConversationManager con persistencia |
| **IntegraciÃ³n con ModelService falla** | Baja | Alto | Try-except, continuar sin predicciÃ³n |
| **Timeouts frecuentes** | Media | Medio | Timeouts configurables, retry logic |

### Plan de Contingencia

1. **Si Ollama es muy lento**: Cambiar a Gemini incluso en dev
2. **Si extracciÃ³n falla**: Usar defaults y notificar al usuario
3. **Si LLM no responde**: Mensaje de error user-friendly
4. **Si predicciÃ³n falla**: Retornar solo respuesta del LLM

---

## ğŸ“š DocumentaciÃ³n a Crear

### 1. README del MÃ³dulo

**Archivo**: `src/api/services/README.md`

Contenido:
- Arquitectura del copilot
- Flujo de datos
- CÃ³mo agregar nuevos intents
- CÃ³mo modificar prompts
- Troubleshooting

### 2. GuÃ­a de Prompts

**Archivo**: `docs/guides/PROMPT_ENGINEERING.md`

Contenido:
- System prompt design
- Intent-specific prompts
- Best practices
- Ejemplos de queries

### 3. Notebook de Ejemplos

**Archivo**: `notebooks/experimental/copilot_usage_examples.ipynb`

Secciones:
1. Setup y configuraciÃ³n
2. Ejemplo: PredicciÃ³n simple
3. Ejemplo: AnÃ¡lisis de patrones
4. Ejemplo: Escenario what-if
5. Ejemplo: ExplicaciÃ³n de conceptos
6. Ejemplo: ConversaciÃ³n multi-turno
7. Manejo de errores
8. Benchmarking de latencia
9. Conclusiones

### 4. DocumentaciÃ³n de ResoluciÃ³n

**Archivo**: `docs/us-resolved/us-031.md`

Contenido (post-implementaciÃ³n):
- Resumen ejecutivo
- Arquitectura implementada
- Decisiones tÃ©cnicas
- Problemas resueltos
- MÃ©tricas de calidad
- Lecciones aprendidas


---

## âœ… Checklist de ImplementaciÃ³n

### Fase 1: Setup y Estructura (4 hrs)

- [ ] Crear estructura de carpetas (`routes/`, `services/`, `models/`, `config/prompts/`)
- [ ] Crear modelos Pydantic (`ChatRequest`, `ChatResponse`, `PredictionData`)
- [ ] Crear system prompt base en `config/prompts/system_prompt.txt`
- [ ] Actualizar `.env.example` con variables de copilot
- [ ] Crear tests bÃ¡sicos de estructura

### Fase 2: Componentes Core (16 hrs)

- [ ] Implementar `IntentParser` con patterns de keywords
- [ ] Implementar `PromptBuilder` con templates
- [ ] Implementar `ParameterExtractor` con regex
- [ ] Implementar `ConversationManager` con dict in-memory
- [ ] Implementar `ResponseFormatter`
- [ ] Tests unitarios para cada componente (>80% coverage)

### Fase 3: OrquestaciÃ³n (8 hrs)

- [ ] Implementar `CopilotService` con flujo completo
- [ ] Integrar con `get_llm_client()` de US-029
- [ ] Integrar con `ModelService` de US-020
- [ ] Manejo de errores y timeouts
- [ ] Logging estructurado
- [ ] Tests de integraciÃ³n

### Fase 4: Endpoint FastAPI (4 hrs)

- [ ] Crear endpoint POST `/copilot/chat` en `routes/copilot.py`
- [ ] Registrar router en `main.py`
- [ ] ValidaciÃ³n de requests con Pydantic
- [ ] Error handlers (400, 500, 503, 504)
- [ ] DocumentaciÃ³n OpenAPI
- [ ] Tests del endpoint

### Fase 5: Testing y ValidaciÃ³n (6 hrs)

- [ ] Ejecutar tests unitarios (target: >80% coverage)
- [ ] Ejecutar tests de integraciÃ³n con Ollama
- [ ] Validar latencia (p95 < 5s Ollama, < 2s Gemini)
- [ ] Validar detecciÃ³n de intents (>90% accuracy)
- [ ] Validar extracciÃ³n de parÃ¡metros
- [ ] Smoke tests end-to-end

### Fase 6: DocumentaciÃ³n (4 hrs)

- [ ] Crear `src/api/services/README.md`
- [ ] Crear `docs/guides/PROMPT_ENGINEERING.md`
- [ ] Crear notebook `copilot_usage_examples.ipynb`
- [ ] Actualizar `docs/us-resolved/us-031.md`
- [ ] Actualizar README principal del proyecto

### Fase 7: Refinamiento (2 hrs)

- [ ] Code review
- [ ] Formatear con Black
- [ ] Verificar con Ruff (0 warnings)
- [ ] Verificar type hints con MyPy
- [ ] Verificar cumplimiento con AGENTS.md

---

## ğŸ“ Cumplimiento con AGENTS.md

### CÃ³digo

- âœ… **CÃ³digo en inglÃ©s**: Todos los archivos Python en inglÃ©s
- âœ… **DocumentaciÃ³n en espaÃ±ol**: README, guÃ­as y docstrings en espaÃ±ol
- âœ… **Type hints 100%**: Todas las funciones pÃºblicas con type hints
- âœ… **Docstrings estilo Google**: Todas las funciones documentadas
- âœ… **Sin cÃ³digo duplicado**: ReutilizaciÃ³n de componentes de US-029 y US-020
- âœ… **Sin magic numbers**: ConfiguraciÃ³n en .env y config/
- âœ… **Sin emojis en cÃ³digo**: Solo en documentaciÃ³n markdown
- âœ… **Logging estructurado**: Logger en lugar de prints
- âœ… **Manejo de excepciones**: Try-except con logging apropiado

### Testing

- âœ… **Tests unitarios**: >80% coverage en mÃ³dulos nuevos
- âœ… **Tests de integraciÃ³n**: Con Ollama real
- âœ… **Pytest como framework**: EstÃ¡ndar del proyecto
- âœ… **Mocks apropiados**: Para evitar llamadas reales en unit tests

### Arquitectura

- âœ… **SeparaciÃ³n de concerns**: Services, routes, models separados
- âœ… **SOLID principles**: SRP en cada service, DIP con factory pattern
- âœ… **ReutilizaciÃ³n**: Uso de componentes existentes (US-029, US-020)
- âœ… **Extensibilidad**: FÃ¡cil agregar nuevos intents o providers

---

## ğŸ“ˆ MÃ©tricas de Ã‰xito

### Funcionales

| MÃ©trica | Target | ValidaciÃ³n |
|---------|--------|------------|
| **DetecciÃ³n de intents** | >90% accuracy | Manual testing con 50 queries |
| **ExtracciÃ³n de parÃ¡metros** | >85% accuracy | Unit tests |
| **Predicciones correctas** | 100% cuando params vÃ¡lidos | Integration tests |
| **Respuestas coherentes** | >95% | Manual review |

### No Funcionales

| MÃ©trica | Target | ValidaciÃ³n |
|---------|--------|------------|
| **Latencia p95 (Ollama)** | <5s | Benchmark script |
| **Latencia p95 (Gemini)** | <2s | Benchmark script |
| **Test coverage** | >80% | pytest --cov |
| **Uptime** | >99% | Health checks |
| **Error rate** | <5% | Logging analysis |

### Calidad

| MÃ©trica | Target | ValidaciÃ³n |
|---------|--------|------------|
| **Ruff warnings** | 0 | ruff check |
| **Black compliance** | 100% | black --check |
| **Type hints** | 100% | mypy |
| **Docstrings** | 100% | Code review |

---

## ğŸš€ Plan de Deployment

### Desarrollo

1. Implementar en rama `feature/us-031-copilot-chat`
2. Tests locales con Ollama
3. Code review
4. Merge a `develop`

### Staging

1. Deploy a Cloud Run (staging)
2. Configurar `LLM_PROVIDER=gemini`
3. Tests de integraciÃ³n en staging
4. ValidaciÃ³n de latencia

### ProducciÃ³n

1. Merge a `main`
2. Deploy automÃ¡tico vÃ­a Cloud Build
3. Smoke tests post-deploy
4. Monitoreo de mÃ©tricas

---

## ğŸ”® Trabajo Futuro (Post-US-031)

### Mejoras Inmediatas

1. **Streaming de respuestas**: SSE para respuestas en tiempo real
2. **Persistencia de conversaciones**: Redis o PostgreSQL
3. **Rate limiting**: Por usuario/IP
4. **AutenticaciÃ³n**: JWT tokens

### Mejoras a Mediano Plazo

1. **RAG Implementation**: IntegraciÃ³n con datos histÃ³ricos
2. **Fine-tuning**: Llama 3.2 fine-tuned con datos del dominio
3. **Multi-idioma**: Soporte para inglÃ©s y espaÃ±ol
4. **Confidence scores**: Scores de confianza en respuestas

### Mejoras a Largo Plazo

1. **Feedback loop**: Usuarios califican respuestas
2. **A/B testing**: Comparar diferentes prompts
3. **Analytics dashboard**: MÃ©tricas de uso del copilot
4. **Voice interface**: IntegraciÃ³n con speech-to-text

---

## ğŸ“ Contactos y Recursos

### Equipo

- **Julian (ML Engineer)**: Responsable de CopilotService, IntentParser, Prompts
- **Arthur (MLOps)**: Responsable de Endpoint, Tests, IntegraciÃ³n
- **Dante (Software Engineer)**: Responsable de ConversationManager, Frontend
- **Erick (Data Scientist)**: ValidaciÃ³n de prompts y testing

### Referencias

- **US-020**: FastAPI Endpoints (`docs/us-resolved/us-020.md`)
- **US-029**: Ollama + Llama 3.2 Setup (`docs/us-resolved/us-029.md`)
- **US-030**: RefactorizaciÃ³n y SOLID (`docs/us-resolved/us-030.md`)
- **AGENTS.md**: EstÃ¡ndares del proyecto
- **plan_context.md**: Contexto general del proyecto

### Recursos Externos

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Gemini API Documentation](https://ai.google.dev/docs)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [LangChain Documentation](https://python.langchain.com/)

---

## ğŸ‰ ConclusiÃ³n

Esta planificaciÃ³n define una implementaciÃ³n completa y robusta del endpoint `/copilot/chat` que:

1. âœ… **Cumple todos los criterios de aceptaciÃ³n** definidos en la US
2. âœ… **Sigue el estÃ¡ndar de excelencia** de US-020, US-025 y US-029
3. âœ… **Integra componentes existentes** (US-029 LLM clients, US-020 ModelService)
4. âœ… **Aplica principios SOLID** (SRP, DIP) de US-030
5. âœ… **Cumple 100% con AGENTS.md** (cÃ³digo en inglÃ©s, docs en espaÃ±ol, type hints, etc.)
6. âœ… **Incluye testing robusto** (>80% coverage, unit + integration)
7. âœ… **EstÃ¡ bien documentado** (README, guÃ­as, notebook de ejemplos)
8. âœ… **Es extensible** (fÃ¡cil agregar intents, providers, features)

### PrÃ³ximos Pasos

1. **AprobaciÃ³n del plan** por el equipo
2. **AsignaciÃ³n de tareas** segÃºn distribuciÃ³n por rol
3. **Inicio de implementaciÃ³n** en rama `feature/us-031-copilot-chat`
4. **Daily standups** para seguimiento de progreso
5. **Code review** al completar cada fase
6. **Merge y deployment** tras validaciÃ³n completa

**EstimaciÃ³n total**: 40 horas (~1 semana con 4 personas)

---

**Documento creado por**: Arthur (MLOps Engineer) con asistencia de AI  
**Fecha**: 17 de Noviembre, 2025  
**VersiÃ³n**: 1.0  
**Estado**: ğŸ“‹ LISTO PARA APROBACIÃ“N

---

**Â¿Aprobado para implementaciÃ³n?** â¬œ SÃ  â¬œ NO  â¬œ REQUIERE AJUSTES

**Comentarios del equipo**:
```
[Espacio para feedback del equipo]
```

