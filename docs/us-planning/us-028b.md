# US-028b: Evidently AI - Data Drift Detection & Model Monitoring

**Estado**: üìã En Planeaci√≥n  
**Fecha de Creaci√≥n**: 16 de Noviembre, 2025  
**Responsable**: Arthur (MLOps/SRE Engineer)  
**Sprint**: Sprint 2 - MLOps Automation  
**Tipo**: MLOps + Monitoring + Data Quality

---

## üìã Historia de Usuario

**Como** ML Engineer  
**Quiero** detectar drift en datos de producci√≥n  
**Para que** sepa cu√°ndo re-entrenar el modelo

---

## üéØ Objetivos

### Objetivo Principal
Implementar un sistema automatizado de monitoreo de drift usando **Evidently AI** que detecte cambios en la distribuci√≥n de datos de entrada, predicciones y performance del modelo en producci√≥n, generando reportes HTML autom√°ticos y alertas cuando se detecte drift severo.

### Objetivos Espec√≠ficos
1. **Monitoreo Semanal Automatizado**: Script que se ejecute semanalmente para analizar drift
2. **Reportes HTML Visuales**: Generaci√≥n autom√°tica de reportes con visualizaciones interactivas
3. **Detecci√≥n Multi-Dimensional**: Monitorear data drift, target drift, prediction drift y model performance
4. **Sistema de Alertas**: Notificaciones por email cuando se detecte drift severo
5. **Almacenamiento Hist√≥rico**: Guardar reportes en `reports/monitoring/` para an√°lisis temporal

---

## üîç An√°lisis de Herramientas: Evidently AI vs Deepchecks vs Arize

### Opci√≥n 1: Evidently AI (SELECCIONADA)

**Ventajas**:
- ‚úÖ **Ya instalado en el proyecto**: `evidently = "^0.7.14"` en pyproject.toml
- ‚úÖ **Activamente mantenido**: √öltima actualizaci√≥n Nov 2024, releases frecuentes
- ‚úÖ **Open Source**: Sin costos adicionales, control total del c√≥digo
- ‚úÖ **Especializado en drift**: Core competency en drift detection y monitoring
- ‚úÖ **Reportes HTML interactivos**: Visualizaciones ricas sin necesidad de servidor
- ‚úÖ **Python-first**: Integraci√≥n nativa con Polars, Pandas, sklearn
- ‚úÖ **Offline-first**: Funciona sin conexi√≥n a internet
- ‚úÖ **Customizable**: F√°cil extender con custom metrics
- ‚úÖ **Versionable**: Reportes HTML se pueden versionar con Git/DVC
- ‚úÖ **Sin vendor lock-in**: No dependemos de un servicio externo
- ‚úÖ **Documentaci√≥n excelente**: Tutoriales completos y actualizados
- ‚úÖ **Comunidad activa**: 4.8k stars en GitHub, muy activo
- ‚úÖ **Evidently UI**: Dashboard open source disponible si lo necesitamos
- ‚úÖ **Presets listos**: DataDriftPreset, TargetDriftPreset, RegressionPreset

**Desventajas**:
- ‚ö†Ô∏è Requiere implementar sistema de alertas manualmente (email)
- ‚ö†Ô∏è No tiene dashboard en tiempo real built-in (solo reportes est√°ticos)

**Casos de uso ideales**:
- ‚úÖ Proyectos con presupuesto limitado
- ‚úÖ Equipos que prefieren control total sobre sus datos
- ‚úÖ Ambientes donde la privacidad de datos es cr√≠tica
- ‚úÖ Desarrollo local y CI/CD pipelines

### Opci√≥n 2: Deepchecks

**Ventajas**:
- ‚úÖ **Suite completa de testing**: 200+ checks predefinidos
- ‚úÖ **Data validation**: Detecta duplicados, leaks, inconsistencias

**Desventajas**:
- ‚ùå **NO est√° instalado**: Requiere agregar nueva dependencia (~50MB)
- ‚ùå **√öltima actualizaci√≥n**: Hace 1 a√±o (menos activo que Evidently)
- ‚ùå **Menos mantenido**: Comunidad m√°s peque√±a
- ‚ùå **M√°s complejo**: Curva de aprendizaje mayor
- ‚ùå **Overkill**: 200+ checks cuando solo necesitamos drift monitoring

### Opci√≥n 3: Arize AI

**Ventajas**:
- ‚úÖ **Dashboard en tiempo real**: Visualizaci√≥n continua

**Desventajas**:
- ‚ùå **Requiere cuenta cloud**: Dependencia de servicio externo
- ‚ùå **Costo adicional**: Free tier limitado
- ‚ùå **Vendor lock-in**: Migrar es complejo
- ‚ùå **No alineado con presupuesto**: Proyecto tiene l√≠mite de $50 USD

---

## üèÜ Decisi√≥n: Evidently AI

**Recomendaci√≥n**: Usar **Evidently AI** por las siguientes razones:

### Razones T√©cnicas
1. **Ya est√° instalado**: `evidently = "^0.7.14"` - 0 overhead
2. **Activamente mantenido**: √öltima release Nov 2024, muy activo
3. **Especializado en drift**: Es su core competency
4. **Reportes de calidad**: HTML interactivos con visualizaciones ricas
5. **Presets listos**: DataDriftPreset, TargetDriftPreset ya implementados
6. **F√°cil de usar**: API simple y directa

### Razones Estrat√©gicas
7. **Alineado con presupuesto**: $0 adicionales
8. **Filosof√≠a del proyecto**: Open source, control total, versionable
9. **Cumple todos los criterios**: Reportes HTML, m√©tricas, alertas
10. **Futuro-proof**: Si necesitamos dashboard, Evidently UI est√° disponible

### Razones de Mantenimiento
11. **Comunidad activa**: 4.8k stars, releases frecuentes
12. **Documentaci√≥n actualizada**: Tutoriales y ejemplos al d√≠a
13. **Menos dependencias**: Ya est√° en el proyecto
14. **Simplicidad**: Menos moving parts, menos puntos de falla

**Comparaci√≥n de actividad**:
- **Evidently**: √öltima release Nov 2024, commits semanales
- **Deepchecks**: √öltima release hace 1 a√±o, menos activo

**Decisi√≥n final**: Evidently es la opci√≥n correcta para este proyecto.

---

## üìä Criterios de Aceptaci√≥n

### Funcionales

#### 1. Script de Monitoreo Semanal
- [ ] Script Python ejecutable: `src/monitoring/evidently_monitor.py`
- [ ] Configurable v√≠a `config/monitoring_config.yaml`
- [ ] Ejecutable manualmente o v√≠a cron/scheduler
- [ ] Logging estructurado de todas las operaciones
- [ ] Manejo robusto de errores y excepciones

#### 2. Reporte HTML Generado Autom√°ticamente
- [ ] Reporte HTML interactivo con visualizaciones de Evidently
- [ ] Guardado en `reports/monitoring/drift_report_YYYY-MM-DD.html`
- [ ] Incluye timestamp y metadata del an√°lisis
- [ ] Visualizaciones: histogramas, KDE plots, drift scores, heatmaps
- [ ] Navegable sin servidor web (standalone HTML)
- [ ] Secci√≥n de summary con m√©tricas clave

#### 3. M√©tricas Monitoreadas con Evidently

**Data Drift** (usando DataDriftPreset):
- [ ] PSI (Population Stability Index) por feature
- [ ] KS Test (Kolmogorov-Smirnov) por feature
- [ ] Wasserstein Distance para features num√©ricas
- [ ] Chi-squared test para features categ√≥ricas
- [ ] Drift score agregado (0-1 scale)
- [ ] Identificaci√≥n de features con mayor drift
- [ ] Share of drifted features

**Target Drift** (usando TargetDriftPreset):
- [ ] Distribuci√≥n de `Usage_kWh` (target) en producci√≥n vs entrenamiento
- [ ] Statistical tests (KS, Wasserstein)
- [ ] Drift score para target
- [ ] Visualizaci√≥n de distribuciones

**Prediction Drift**:
- [ ] Distribuci√≥n de predicciones en producci√≥n vs entrenamiento
- [ ] Cambios en media, mediana, std de predicciones
- [ ] Drift score para predicciones
- [ ] Correlation entre predictions y target

**Model Performance** (usando RegressionPreset):
- [ ] RMSE en producci√≥n (si tenemos ground truth)
- [ ] MAE en producci√≥n
- [ ] R¬≤ score
- [ ] MAPE (Mean Absolute Percentage Error)
- [ ] Comparaci√≥n con m√©tricas de entrenamiento
- [ ] Degradaci√≥n de performance (%)
- [ ] Error distribution analysis

#### 4. Alertas por Email si Drift Severo
- [ ] Sistema de alertas configurable (thresholds)
- [ ] Email enviado cuando:
  - `drift_score > 0.7` (configurable)
  - `share_of_drifted_features > 0.5`
  - `performance_degradation > 0.15` (15%)
- [ ] Email incluye:
  - Resumen de drift detectado
  - Features afectados con drift scores
  - M√©tricas de performance
  - Link a reporte HTML completo
- [ ] Configuraci√≥n SMTP v√≠a variables de entorno
- [ ] Fallback si email falla (log warning, no crash)
- [ ] Template HTML para emails profesionales

#### 5. Reportes Guardados en `reports/monitoring/`
- [ ] Estructura de carpetas: `reports/monitoring/YYYY/MM/`
- [ ] Naming convention: `drift_report_YYYY-MM-DD_HH-MM-SS.html`
- [ ] JSON con m√©tricas: `drift_metrics_YYYY-MM-DD_HH-MM-SS.json`
- [ ] CSV hist√≥rico: `metrics_history.csv` (append mode)
- [ ] Retenci√≥n configurable (ej: √∫ltimos 90 d√≠as)
- [ ] Versionado con DVC si reportes > 1MB

### No Funcionales

#### Performance
- [ ] An√°lisis completo en < 5 minutos para 10,000 registros
- [ ] Uso de memoria < 2GB durante an√°lisis
- [ ] Procesamiento eficiente con Polars ‚Üí Pandas conversion

#### Calidad de C√≥digo
- [ ] Type hints en 100% de funciones
- [ ] Docstrings estilo Google
- [ ] Code coverage > 70% en tests
- [ ] Black formatted (line-length=100)
- [ ] Sin warnings de Ruff
- [ ] Sin emojis en c√≥digo (solo en docs)

#### Documentaci√≥n
- [ ] README en `src/monitoring/README.md`
- [ ] Notebook de ejemplo: `notebooks/experimental/evidently_monitoring_demo.ipynb`
- [ ] Gu√≠a de configuraci√≥n de alertas por email
- [ ] Documentaci√≥n de thresholds y c√≥mo ajustarlos
- [ ] Ejemplos de interpretaci√≥n de reportes

#### Testing
- [ ] Tests unitarios para funciones de c√°lculo
- [ ] Tests de integraci√≥n para generaci√≥n de reportes
- [ ] Test de env√≠o de email (mock SMTP)
- [ ] Test con datos sint√©ticos (drift simulado)

---

## üèóÔ∏è Arquitectura Propuesta

### Componentes del Sistema

```
src/monitoring/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ evidently_monitor.py       # Script principal (CLI)
‚îú‚îÄ‚îÄ data_loader.py             # Carga de datos de referencia y producci√≥n
‚îú‚îÄ‚îÄ drift_analyzer.py          # An√°lisis de drift con Evidently
‚îú‚îÄ‚îÄ report_generator.py        # Generaci√≥n y guardado de reportes
‚îú‚îÄ‚îÄ alert_manager.py           # Sistema de alertas (email)
‚îú‚îÄ‚îÄ metrics_tracker.py         # Tracking de m√©tricas hist√≥ricas
‚îî‚îÄ‚îÄ config.py                  # Configuraci√≥n y thresholds

reports/monitoring/
‚îú‚îÄ‚îÄ 2025/
‚îÇ   ‚îî‚îÄ‚îÄ 11/
‚îÇ       ‚îú‚îÄ‚îÄ drift_report_2025-11-16_10-30-00.html
‚îÇ       ‚îú‚îÄ‚îÄ drift_metrics_2025-11-16_10-30-00.json
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ reference_data/
‚îÇ   ‚îî‚îÄ‚îÄ train_data_sample.parquet  # Datos de referencia
‚îî‚îÄ‚îÄ metrics_history.csv            # Hist√≥rico de m√©tricas

config/
‚îî‚îÄ‚îÄ monitoring_config.yaml     # Configuraci√≥n de thresholds y alertas

tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_drift_analyzer.py
‚îÇ   ‚îú‚îÄ‚îÄ test_alert_manager.py
‚îÇ   ‚îî‚îÄ‚îÄ test_metrics_tracker.py
‚îî‚îÄ‚îÄ integration/
    ‚îî‚îÄ‚îÄ test_monitoring_pipeline.py
```

### Flujo de Datos con Evidently

```
1. Initialization
   ‚îú‚îÄ‚îÄ Load config from monitoring_config.yaml
   ‚îú‚îÄ‚îÄ Load reference data (training set sample)
   ‚îî‚îÄ‚îÄ Initialize Evidently Report with presets
        ‚Üì
2. Data Loading
   ‚îú‚îÄ‚îÄ Reference Data: train_data_sample.parquet
   ‚îú‚îÄ‚îÄ Production Data: √∫ltimas N predicciones de API/logs
   ‚îú‚îÄ‚îÄ Convert Polars ‚Üí Pandas (requerido por Evidently)
   ‚îî‚îÄ‚îÄ Validate schemas match
        ‚Üì
3. Drift Analysis (Evidently)
   ‚îú‚îÄ‚îÄ Create Report with presets:
   ‚îÇ   ‚îú‚îÄ‚îÄ DataDriftPreset()
   ‚îÇ   ‚îú‚îÄ‚îÄ TargetDriftPreset()
   ‚îÇ   ‚îî‚îÄ‚îÄ RegressionPreset()
   ‚îÇ
   ‚îú‚îÄ‚îÄ Run analysis:
   ‚îÇ   ‚îú‚îÄ‚îÄ Calculate drift scores per feature
   ‚îÇ   ‚îú‚îÄ‚îÄ Statistical tests (PSI, KS, Wasserstein)
   ‚îÇ   ‚îú‚îÄ‚îÄ Target drift analysis
   ‚îÇ   ‚îú‚îÄ‚îÄ Prediction drift analysis
   ‚îÇ   ‚îî‚îÄ‚îÄ Performance metrics (RMSE, MAE, R¬≤)
   ‚îÇ
   ‚îî‚îÄ‚îÄ Extract results:
       ‚îú‚îÄ‚îÄ Drift scores
       ‚îú‚îÄ‚îÄ Share of drifted features
       ‚îú‚îÄ‚îÄ Performance degradation
       ‚îî‚îÄ‚îÄ Feature-level details
        ‚Üì
4. Report Generation
   ‚îú‚îÄ‚îÄ Save HTML report (Evidently built-in)
   ‚îú‚îÄ‚îÄ Extract metrics to JSON
   ‚îú‚îÄ‚îÄ Append to metrics_history.csv
   ‚îî‚îÄ‚îÄ Save in reports/monitoring/YYYY/MM/
        ‚Üì
5. Alert Evaluation
   ‚îú‚îÄ‚îÄ Evaluate alert conditions:
   ‚îÇ   ‚îú‚îÄ‚îÄ drift_score > threshold
   ‚îÇ   ‚îú‚îÄ‚îÄ share_of_drifted_features > threshold
   ‚îÇ   ‚îî‚îÄ‚îÄ performance_degradation > threshold
   ‚îÇ
   ‚îú‚îÄ‚îÄ If alert triggered:
   ‚îÇ   ‚îú‚îÄ‚îÄ Generate email with summary
   ‚îÇ   ‚îú‚îÄ‚îÄ Include top drifted features
   ‚îÇ   ‚îú‚îÄ‚îÄ Attach link to HTML report
   ‚îÇ   ‚îî‚îÄ‚îÄ Send via SMTP
   ‚îÇ
   ‚îî‚îÄ‚îÄ Log alert status
        ‚Üì
6. Cleanup & Archival
   ‚îú‚îÄ‚îÄ Delete reports older than retention_days
   ‚îú‚îÄ‚îÄ Compress old reports (gzip)
   ‚îî‚îÄ‚îÄ Update metrics dashboard (if exists)
```

---

## üí° Implementaci√≥n Detallada con Evidently AI

### 1. Estructura de Datos

#### Reference Data (Training Set Sample)
```python
# File: reports/monitoring/reference_data/train_data_sample.parquet
# Columns: todas las features + target (Usage_kWh) + predictions
# Size: 5,000-10,000 registros (representativo de distribuci√≥n)
# Stratified sampling para mantener distribuci√≥n de Load_Type

import polars as pl

# Load full training data
train_df = pl.read_parquet("data/processed/train_set.parquet")

# Stratified sample
train_sample = train_df.sample(n=10000, stratify="Load_Type", seed=42)

# Add predictions from trained model
import joblib
model = joblib.load("models/ensembles/ensemble_lightgbm_v1.pkl")
X = train_sample.drop("Usage_kWh").to_pandas()
predictions = model.predict(X)
train_sample = train_sample.with_columns(
    pl.Series("predictions", predictions)
)

# Save reference data
train_sample.write_parquet("reports/monitoring/reference_data/train_data_sample.parquet")
```

#### Production Data
```python
# File: Generado desde API logs o database
# Columns: mismas que reference data
# Timeframe: √∫ltima semana (configurable)
# Source: API predictions logged en production
```

### 2. Drift Analysis con Evidently

```python
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, TargetDriftPreset, RegressionPreset
from evidently.metrics import (
    DatasetDriftMetric,
    DatasetMissingValuesMetric,
    ColumnDriftMetric,
)

class DriftAnalyzer:
    """
    Drift analyzer using Evidently AI.
    """
    
    def __init__(self, config: dict):
        self.config = config
        self.column_mapping = self._create_column_mapping()
    
    def _create_column_mapping(self):
        """Create Evidently column mapping."""
        from evidently.pipeline.column_mapping import ColumnMapping
        
        return ColumnMapping(
            target="Usage_kWh",
            prediction="predictions",
            numerical_features=[
                "lagging_reactive_power",
                "leading_reactive_power",
                "co2",
                "lagging_power_factor",
                "leading_power_factor",
                "nsm",
            ],
            categorical_features=["Load_Type", "day_of_week"],
        )
    
    def analyze_drift(
        self,
        reference_data: pd.DataFrame,
        current_data: pd.DataFrame
    ) -> Report:
        """
        Analyze drift between reference and current data.
        
        Parameters
        ----------
        reference_data : pd.DataFrame
            Training data sample
        current_data : pd.DataFrame
            Production data
            
        Returns
        -------
        Report
            Evidently report with drift analysis
        """
        # Create report with presets
        report = Report(metrics=[
            DataDriftPreset(),
            TargetDriftPreset(),
            RegressionPreset(),
            
            # Additional custom metrics
            DatasetDriftMetric(),
            DatasetMissingValuesMetric(),
            
            # Per-feature drift
            ColumnDriftMetric(column_name="lagging_reactive_power"),
            ColumnDriftMetric(column_name="leading_reactive_power"),
            ColumnDriftMetric(column_name="co2"),
            ColumnDriftMetric(column_name="Load_Type"),
        ])
        
        # Run analysis
        report.run(
            reference_data=reference_data,
            current_data=current_data,
            column_mapping=self.column_mapping
        )
        
        return report
    
    def extract_metrics(self, report: Report) -> dict:
        """
        Extract key metrics from Evidently report.
        
        Parameters
        ----------
        report : Report
            Evidently report
            
        Returns
        -------
        dict
            Dictionary with key metrics
        """
        results = report.as_dict()
        
        # Extract drift metrics
        dataset_drift = results["metrics"][0]["result"]
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "drift_score": dataset_drift.get("drift_score", 0.0),
            "share_of_drifted_features": dataset_drift.get("share_of_drifted_columns", 0.0),
            "number_of_drifted_features": dataset_drift.get("number_of_drifted_columns", 0),
            "drifted_features": dataset_drift.get("drift_by_columns", {}),
        }
        
        # Extract performance metrics if available
        if len(results["metrics"]) > 2:
            regression_metrics = results["metrics"][2]["result"]
            metrics.update({
                "rmse": regression_metrics.get("current", {}).get("rmse"),
                "mae": regression_metrics.get("current", {}).get("mae"),
                "r2_score": regression_metrics.get("current", {}).get("r2_score"),
            })
        
        return metrics
```

### 3. Configuraci√≥n YAML

```yaml
# config/monitoring_config.yaml
monitoring:
  reference_data:
    path: "reports/monitoring/reference_data/train_data_sample.parquet"
    size: 10000
    stratify_column: "Load_Type"
  
  production_data:
    source: "api_logs"  # or "database", "file"
    timeframe_days: 7
    min_samples: 1000
  
  drift_detection:
    thresholds:
      drift_score: 0.7              # Alert if > 0.7
      share_of_drifted_features: 0.5  # Alert if > 50% features drifted
      feature_drift_score: 0.5      # Alert if individual feature > 0.5
      target_drift_score: 0.6
      prediction_drift_score: 0.6
    
    statistical_tests:
      psi_threshold: 0.2            # PSI > 0.2 indica drift
      ks_pvalue_threshold: 0.05     # p-value < 0.05 indica drift
      wasserstein_threshold: 0.1
  
  performance:
    thresholds:
      rmse_degradation: 0.15        # Alert if RMSE increases > 15%
      mae_degradation: 0.15
      r2_degradation: 0.10
  
  reporting:
    output_dir: "reports/monitoring"
    retention_days: 90
    compress_old_reports: true
    save_json_metrics: true
    save_csv_history: true
  
  alerts:
    enabled: true
    channels:
      - email
    
    email:
      smtp_server: "smtp.gmail.com"
      smtp_port: 587
      use_tls: true
      sender: "${SMTP_SENDER_EMAIL}"
      password: "${SMTP_SENDER_PASSWORD}"
      recipients:
        - "arthur@team37.com"
        - "mlops-team@team37.com"
      subject_template: "[ALERT] Data Drift Detected - {date}"
      html_template: true
```

### 4. Sistema de Alertas

```python
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders
from dataclasses import dataclass
from enum import Enum

class AlertSeverity(Enum):
    INFO = "INFO"
    WARNING = "WARNING"
    CRITICAL = "CRITICAL"

@dataclass
class AlertContext:
    severity: AlertSeverity
    drift_score: float
    share_of_drifted_features: float
    drifted_features: list[str]
    performance_degradation: float
    report_path: str
    timestamp: str

class AlertManager:
    """
    Alert manager for drift monitoring.
    """
    
    def __init__(self, config: dict):
        self.config = config
        self.smtp_config = config["alerts"]["email"]
    
    def evaluate_alert_conditions(self, metrics: dict) -> AlertContext:
        """
        Evaluate if alert should be triggered.
        
        Parameters
        ----------
        metrics : dict
            Metrics from drift analysis
            
        Returns
        -------
        AlertContext
            Alert context with severity and details
        """
        drift_score = metrics.get("drift_score", 0.0)
        share_drifted = metrics.get("share_of_drifted_features", 0.0)
        
        # Calculate performance degradation
        current_rmse = metrics.get("rmse", 0.0)
        reference_rmse = self._get_reference_rmse()
        degradation = (current_rmse - reference_rmse) / reference_rmse if reference_rmse > 0 else 0.0
        
        # Determine severity
        severity = self._determine_severity(drift_score, share_drifted, degradation)
        
        # Get drifted features
        drifted_features = [
            f"{feat}: {score:.3f}"
            for feat, score in metrics.get("drifted_features", {}).items()
            if score > self.config["drift_detection"]["thresholds"]["feature_drift_score"]
        ]
        
        return AlertContext(
            severity=severity,
            drift_score=drift_score,
            share_of_drifted_features=share_drifted,
            drifted_features=drifted_features,
            performance_degradation=degradation,
            report_path=metrics.get("report_path", ""),
            timestamp=metrics.get("timestamp", "")
        )
    
    def _determine_severity(
        self,
        drift_score: float,
        share_drifted: float,
        degradation: float
    ) -> AlertSeverity:
        """Determine alert severity based on metrics."""
        thresholds = self.config["drift_detection"]["thresholds"]
        perf_threshold = self.config["performance"]["thresholds"]["rmse_degradation"]
        
        # Critical conditions
        if (drift_score > thresholds["drift_score"] or
            share_drifted > thresholds["share_of_drifted_features"] or
            degradation > perf_threshold):
            return AlertSeverity.CRITICAL
        
        # Warning conditions
        if (drift_score > thresholds["drift_score"] * 0.7 or
            share_drifted > thresholds["share_of_drifted_features"] * 0.7):
            return AlertSeverity.WARNING
        
        return AlertSeverity.INFO
    
    def send_alert(self, context: AlertContext) -> bool:
        """
        Send alert via email.
        
        Parameters
        ----------
        context : AlertContext
            Alert context with details
            
        Returns
        -------
        bool
            True if alert sent successfully
        """
        if context.severity == AlertSeverity.INFO:
            return True  # No alert needed
        
        try:
            msg = self._create_email_message(context)
            self._send_email(msg)
            logger.info(f"Alert sent successfully: {context.severity.value}")
            return True
        except Exception as e:
            logger.error(f"Failed to send alert: {e}")
            return False
    
    def _create_email_message(self, context: AlertContext) -> MIMEMultipart:
        """Create email message with HTML template."""
        msg = MIMEMultipart("alternative")
        msg["Subject"] = self.smtp_config["subject_template"].format(
            date=context.timestamp
        )
        msg["From"] = self.smtp_config["sender"]
        msg["To"] = ", ".join(self.smtp_config["recipients"])
        
        # HTML body
        html_body = f"""
        <html>
          <body>
            <h2 style="color: {'red' if context.severity == AlertSeverity.CRITICAL else 'orange'};">
              Data Drift Alert - {context.severity.value}
            </h2>
            
            <h3>Summary</h3>
            <ul>
              <li><strong>Timestamp:</strong> {context.timestamp}</li>
              <li><strong>Drift Score:</strong> {context.drift_score:.3f}</li>
              <li><strong>Drifted Features:</strong> {context.share_of_drifted_features:.1%}</li>
              <li><strong>Performance Degradation:</strong> {context.performance_degradation:.2%}</li>
            </ul>
            
            <h3>Drifted Features ({len(context.drifted_features)})</h3>
            <ul>
              {''.join(f'<li>{feat}</li>' for feat in context.drifted_features[:10])}
            </ul>
            
            <h3>Action Required</h3>
            <p>
              {'<strong>IMMEDIATE:</strong> Review and potentially retrain model' 
               if context.severity == AlertSeverity.CRITICAL 
               else 'Review report and monitor'}
            </p>
            
            <p>
              <a href="file://{context.report_path}">View Full Report</a>
            </p>
          </body>
        </html>
        """
        
        msg.attach(MIMEText(html_body, "html"))
        return msg
    
    def _send_email(self, msg: MIMEMultipart):
        """Send email via SMTP."""
        with smtplib.SMTP(
            self.smtp_config["smtp_server"],
            self.smtp_config["smtp_port"]
        ) as server:
            if self.smtp_config.get("use_tls", True):
                server.starttls()
            server.login(
                self.smtp_config["sender"],
                self.smtp_config["password"]
            )
            server.send_message(msg)
```

---

## üìö Dependencias

### Dependencias Existentes (Ya Instaladas)
- ‚úÖ `evidently = "^0.7.14"` - Drift detection y reportes (YA INSTALADO)
- ‚úÖ `polars = "^1.35.1"` - Data processing
- ‚úÖ `pandas = "^2.3.3"` - Requerido por Evidently
- ‚úÖ `scikit-learn = "^1.7.2"` - M√©tricas de performance
- ‚úÖ `python-dotenv = "^1.1.1"` - Variables de entorno
- ‚úÖ `mlflow = "^3.5.1"` - Para cargar m√©tricas de entrenamiento

### Dependencias Nuevas (Requeridas)

#### PyYAML (para configuraci√≥n)
```bash
poetry add pyyaml
```
- **Versi√≥n**: `^6.0.1`
- **Tama√±o**: ~1MB
- **Uso**: Parser de archivos YAML de configuraci√≥n

### Librer√≠as Est√°ndar (No Requieren Instalaci√≥n)
- ‚úÖ `smtplib` - Env√≠o de emails
- ‚úÖ `email` - Construcci√≥n de emails
- ‚úÖ `logging` - Logging estructurado
- ‚úÖ `pathlib` - Manejo de paths
- ‚úÖ `datetime` - Timestamps
- ‚úÖ `json` - Serializaci√≥n JSON

### Comando de Instalaci√≥n

```bash
# Solo necesitamos agregar PyYAML
poetry add pyyaml

# Verificar que Evidently ya est√° instalado
poetry run python -c "import evidently; print(evidently.__version__)"
```

### Tama√±o Total de Dependencias Nuevas
- **PyYAML**: ~1MB
- **Total**: ~1MB adicional (m√≠nimo impacto)

**Impacto en Docker**: Imagen aumentar√° ~1MB (despreciable)

---

## üöÄ Plan de Implementaci√≥n

### Fase 1: Setup y Estructura (1 hora)
1. **Crear estructura de carpetas**
   ```
   src/monitoring/
   reports/monitoring/reference_data/
   config/
   ```

2. **Configuraci√≥n inicial**
   - Crear `config/monitoring_config.yaml`
   - Crear `.env.example` con variables SMTP
   - Instalar PyYAML: `poetry add pyyaml`

3. **Crear archivos base con docstrings**
   - `src/monitoring/__init__.py`
   - Estructura de m√≥dulos

### Fase 2: Reference Data Preparation (1 hora)
1. **Generar reference data**
   - Script para extraer sample del training set
   - Stratified sampling por `Load_Type`
   - Agregar predictions del modelo
   - Guardar en `reports/monitoring/reference_data/`

2. **Validaci√≥n de reference data**
   - Verificar distribuciones
   - Verificar completitud de features
   - Documentar estad√≠sticas

### Fase 3: Data Loader (1.5 horas)
1. **Implementar `data_loader.py`**
   - Funci√≥n para cargar reference data (Parquet)
   - Funci√≥n para cargar production data (API logs, CSV, DB)
   - Convertir Polars ‚Üí Pandas (requerido por Evidently)
   - Validaci√≥n de schemas
   - Manejo de errores

2. **Tests unitarios**
   - Test de carga de reference data
   - Test de conversi√≥n Polars ‚Üí Pandas
   - Test de validaci√≥n de schemas

### Fase 4: Drift Analyzer (3 horas)
1. **Implementar `drift_analyzer.py`**
   - Clase `DriftAnalyzer`
   - Configurar Evidently Report con presets:
     - DataDriftPreset()
     - TargetDriftPreset()
     - RegressionPreset()
   - Column mapping para Evidently
   - M√©todo `analyze_drift()`
   - M√©todo `extract_metrics()`

2. **Tests unitarios**
   - Test con datos sint√©ticos (sin drift)
   - Test con datos sint√©ticos (con drift)
   - Test de extracci√≥n de m√©tricas

### Fase 5: Report Generator (1.5 horas)
1. **Implementar `report_generator.py`**
   - Guardar HTML report de Evidently
   - Extraer m√©tricas a JSON
   - Append m√©tricas a CSV hist√≥rico
   - Organizaci√≥n por fecha (YYYY/MM/)
   - Naming convention con timestamp
   - Cleanup de reportes antiguos

2. **Tests**
   - Test de guardado de reportes
   - Test de organizaci√≥n de carpetas
   - Test de cleanup

### Fase 6: Alert Manager (2 horas)
1. **Implementar `alert_manager.py`**
   - Clase `AlertManager`
   - Evaluar conditions de drift
   - Determinar severidad (INFO/WARNING/CRITICAL)
   - Generar email HTML con resumen
   - Env√≠o v√≠a SMTP
   - Manejo de errores SMTP

2. **Tests con mock SMTP**
   - Test de evaluaci√≥n de conditions
   - Test de determinaci√≥n de severidad
   - Test de env√≠o de email (mocked)

### Fase 7: Metrics Tracker (1 hora)
1. **Implementar `metrics_tracker.py`**
   - Extraer m√©tricas de Evidently report
   - Append a `metrics_history.csv`
   - Calcular trends (drift increasing/decreasing)
   - Estad√≠sticas hist√≥ricas

2. **Tests**
   - Test de append a CSV
   - Test de c√°lculo de trends

### Fase 8: Config Parser (1 hora)
1. **Implementar `config.py`**
   - Parser de YAML config
   - Validaci√≥n de configuraci√≥n
   - Defaults sensatos
   - Manejo de variables de entorno

2. **Tests**
   - Test de parsing de YAML
   - Test de validaci√≥n
   - Test de defaults

### Fase 9: Script Principal (2 horas)
1. **Implementar `evidently_monitor.py` (CLI)**
   - Argparse para configuraci√≥n
   - Orquestaci√≥n del pipeline:
     1. Load config
     2. Load data
     3. Analyze drift
     4. Generate reports
     5. Evaluate alerts
     6. Send notifications
     7. Cleanup
   - Logging estructurado
   - Manejo de excepciones
   - Exit codes apropiados

2. **CLI interface**
   ```bash
   python -m src.monitoring.evidently_monitor \
     --config config/monitoring_config.yaml \
     --production-data data/production/latest.parquet \
     --send-alerts
   ```

### Fase 10: Testing (2 horas)
1. **Tests unitarios**
   - `tests/unit/test_drift_analyzer.py`
   - `tests/unit/test_alert_manager.py`
   - `tests/unit/test_metrics_tracker.py`
   - `tests/unit/test_config.py`

2. **Tests de integraci√≥n**
   - `tests/integration/test_monitoring_pipeline.py`
   - Test end-to-end con datos sint√©ticos
   - Test con drift simulado
   - Test de alertas

3. **Validaci√≥n**
   - Coverage > 70%
   - Black formatting
   - Ruff linting
   - No diagnostics

### Fase 11: Documentaci√≥n (2 horas)
1. **README en `src/monitoring/README.md`**
   - Instalaci√≥n y configuraci√≥n
   - Uso del script
   - Interpretaci√≥n de reportes de Evidently
   - Configuraci√≥n de alertas
   - Troubleshooting
   - Ejemplos de uso

2. **Notebook de ejemplo**
   - `notebooks/experimental/evidently_monitoring_demo.ipynb`
   - Demo interactivo de Evidently
   - Visualizaci√≥n de reportes
   - Simulaci√≥n de drift
   - Interpretaci√≥n de m√©tricas

3. **Documentaci√≥n de resoluci√≥n**
   - `docs/us-resolved/us-028b.md`
   - Decisiones t√©cnicas
   - Problemas resueltos
   - Lecciones aprendidas

**Tiempo Total Estimado**: 18 horas

**Distribuci√≥n**:
- Setup y config: 2 horas
- Data loading y preparation: 2.5 horas
- Drift analysis: 3 horas
- Reports y alerts: 3.5 horas
- Metrics tracking y config: 2 horas
- Script principal: 2 horas
- Testing: 2 horas
- Documentaci√≥n: 2 horas

---

## üéØ Entregables

### C√≥digo Principal
1. `src/monitoring/evidently_monitor.py` - Script principal (CLI)
2. `src/monitoring/data_loader.py` - Carga de datos y conversi√≥n
3. `src/monitoring/drift_analyzer.py` - An√°lisis de drift con Evidently
4. `src/monitoring/report_generator.py` - Generaci√≥n y guardado de reportes
5. `src/monitoring/alert_manager.py` - Sistema de alertas por email
6. `src/monitoring/metrics_tracker.py` - Tracking de m√©tricas hist√≥ricas
7. `src/monitoring/config.py` - Parser de configuraci√≥n
8. `src/monitoring/__init__.py` - Package init

### Configuraci√≥n
9. `config/monitoring_config.yaml` - Configuraci√≥n de thresholds y alertas
10. `.env.example` - Template de variables de entorno

### Tests
11. `tests/unit/test_drift_analyzer.py` - Tests de drift analysis
12. `tests/unit/test_alert_manager.py` - Tests de alertas
13. `tests/unit/test_metrics_tracker.py` - Tests de metrics tracking
14. `tests/unit/test_config.py` - Tests de configuraci√≥n
15. `tests/integration/test_monitoring_pipeline.py` - Tests end-to-end

### Documentaci√≥n
16. `src/monitoring/README.md` - Gu√≠a completa de uso
17. `notebooks/experimental/evidently_monitoring_demo.ipynb` - Demo interactivo
18. `docs/us-resolved/us-028b.md` - Documentaci√≥n de resoluci√≥n

### Datos y Reportes
19. `reports/monitoring/reference_data/train_data_sample.parquet` - Training sample
20. `reports/monitoring/metrics_history.csv` - Hist√≥rico de m√©tricas
21. `reports/monitoring/2025/11/drift_report_YYYY-MM-DD.html` - Reporte ejemplo
22. `reports/monitoring/2025/11/drift_metrics_YYYY-MM-DD.json` - M√©tricas JSON

### Scripts Auxiliares (Opcional)
23. `scripts/generate_reference_data.py` - Script para generar reference data
24. `scripts/simulate_drift.py` - Script para simular drift en datos de prueba

**Total**: 24 archivos principales

**L√≠neas de c√≥digo estimadas**: ~2,500 l√≠neas
- Drift analyzer: ~400 l√≠neas
- Alert manager: ~300 l√≠neas
- Report generator: ~250 l√≠neas
- Data loader: ~200 l√≠neas
- Metrics tracker: ~200 l√≠neas
- Config parser: ~150 l√≠neas
- Script principal: ~300 l√≠neas
- Tests: ~700 l√≠neas

---

## ‚ö†Ô∏è Riesgos y Mitigaciones

### Riesgo 1: Datos de Producci√≥n No Disponibles
**Probabilidad**: Media  
**Impacto**: Alto  
**Mitigaci√≥n**: 
- Crear datos sint√©ticos de producci√≥n para testing
- Documentar c√≥mo integrar con API logs o database
- Proveer script de ejemplo para generar production data

### Riesgo 2: SMTP Bloqueado o No Configurado
**Probabilidad**: Alta  
**Impacto**: Medio  
**Mitigaci√≥n**:
- Hacer alertas por email opcionales (flag `--no-email`)
- Proveer alternativas: Slack webhook, Discord, log file
- Documentar configuraci√≥n de Gmail App Passwords

### Riesgo 3: Reportes HTML Muy Grandes
**Probabilidad**: Baja  
**Impacto**: Medio  
**Mitigaci√≥n**:
- Limitar tama√±o de datasets en reportes (sampling)
- Comprimir reportes antiguos (gzip)
- Implementar retenci√≥n autom√°tica (90 d√≠as)

### Riesgo 4: False Positives en Drift Detection
**Probabilidad**: Media  
**Impacto**: Medio  
**Mitigaci√≥n**:
- Thresholds configurables y bien documentados
- M√∫ltiples m√©tricas (PSI + KS test + Wasserstein) para confirmar drift
- Documentar c√≥mo ajustar thresholds seg√∫n el caso de uso

### Riesgo 5: Conversi√≥n Polars ‚Üí Pandas
**Probabilidad**: Baja  
**Impacto**: Bajo  
**Mitigaci√≥n**:
- Evidently requiere Pandas, conversi√≥n es necesaria
- Conversi√≥n es eficiente para datasets < 100k registros
- Documentar limitaciones de tama√±o

---

## üîÑ Integraci√≥n con Sistema Existente

### Integraci√≥n con API (US-020)
```python
# En src/api/main.py, loggear predicciones para monitoreo
from src.monitoring.data_logger import log_prediction

@app.post("/predict")
async def predict(request: PredictRequest):
    prediction = model_service.predict(features)
    
    # Log prediction for monitoring
    log_prediction(
        features=request.dict(),
        prediction=prediction,
        timestamp=datetime.now()
    )
    
    return prediction
```

### Integraci√≥n con MLflow (US-013, US-015)
```python
# Cargar m√©tricas de entrenamiento desde MLflow
import mlflow

def load_training_metrics(run_id: str) -> dict:
    """Load training metrics from MLflow for comparison."""
    client = mlflow.tracking.MlflowClient()
    run = client.get_run(run_id)
    return {
        "rmse": run.data.metrics.get("test_rmse"),
        "mae": run.data.metrics.get("test_mae"),
        "r2": run.data.metrics.get("test_r2"),
    }
```

### Integraci√≥n con DVC (US-007)
```python
# Versionar reference data con DVC
# reports/monitoring/reference_data/train_data_sample.parquet.dvc
```

### Integraci√≥n con Cloud Run (US-025)
```python
# Ejecutar monitoring como Cloud Run Job (scheduled)
# O como parte del pipeline de deployment
```

---

## üìä M√©tricas de √âxito

| M√©trica | Target | Validaci√≥n |
|---------|--------|------------|
| **Funcionalidad** | 100% criterios cumplidos | Checklist de aceptaci√≥n |
| **Code Coverage** | > 70% | pytest --cov |
| **Type Hints** | 100% | mypy |
| **Docstrings** | 100% funciones | Manual review |
| **Performance** | < 5 min para 10k registros | Benchmark |
| **Reportes HTML** | Generados correctamente | Visual inspection |
| **Alertas Email** | Enviadas cuando drift > threshold | Test con SMTP real |
| **Documentaci√≥n** | README + Notebook | Peer review |
| **Dependencias nuevas** | Solo PyYAML (~1MB) | poetry show |

---

## ‚úÖ Checklist de Cumplimiento con AGENTS.md

- [ ] C√≥digo en ingl√©s, documentaci√≥n en espa√±ol
- [ ] Funciones reutilizables en `src/monitoring/`
- [ ] Docstrings estilo Google en todas las funciones
- [ ] Type hints en 100% de funciones
- [ ] Tests unitarios con coverage > 70%
- [ ] Formateado con Black (line-length=100)
- [ ] Sin warnings de Ruff
- [ ] Logging estructurado (no prints)
- [ ] Manejo de excepciones apropiado
- [ ] Sin c√≥digo duplicado (DRY)
- [ ] Sin magic numbers (constantes en config)
- [ ] Paths relativos (no hardcoded)
- [ ] Sin emojis en c√≥digo (solo en docs)
- [ ] Sin separadores decorativos en c√≥digo
- [ ] Comentarios concisos y t√©cnicos
- [ ] Conventional commits

---

## üìà Roadmap Futuro (Post US-028b)

### Sprint 3 (Opcional)
1. **Evidently UI**: Dashboard interactivo self-hosted
2. **Automated Retraining**: Trigger autom√°tico de retraining cuando drift > threshold
3. **Slack Integration**: Alertas en canales de equipo
4. **Grafana Dashboard**: Visualizaci√≥n de m√©tricas en tiempo real
5. **A/B Testing**: Comparar performance de m√∫ltiples modelos

### Producci√≥n
1. **Scheduled Execution**: Cloud Scheduler para ejecuci√≥n semanal
2. **Cloud Storage**: Guardar reportes en GCS en lugar de local
3. **BigQuery Integration**: Almacenar m√©tricas de drift en BigQuery
4. **Monitoring Dashboard**: Streamlit app para visualizar hist√≥rico de drift
5. **Multi-model Support**: Monitorear m√∫ltiples modelos simult√°neamente

---

## üéâ Conclusi√≥n

Este plan implementa un sistema robusto de monitoreo de drift usando **Evidently AI**, cumpliendo todos los criterios de aceptaci√≥n y alineado con:

### Ventajas de la Soluci√≥n

- ‚úÖ **Ya instalado**: `evidently = "^0.7.14"` - 0 overhead
- ‚úÖ **Activamente mantenido**: √öltima release Nov 2024, muy activo
- ‚úÖ **Especializado en drift**: Core competency de Evidently
- ‚úÖ **Reportes de calidad**: HTML interactivos con visualizaciones ricas
- ‚úÖ **Presets listos**: DataDriftPreset, TargetDriftPreset, RegressionPreset
- ‚úÖ **Open source**: $0 adicionales, control total
- ‚úÖ **F√°cil de usar**: API simple y directa
- ‚úÖ **Comunidad activa**: 4.8k stars, releases frecuentes

### Alineaci√≥n con Proyecto

- ‚úÖ **Presupuesto**: Solo ~1MB adicional (PyYAML)
- ‚úÖ **Filosof√≠a MLOps**: Monitoring y validaci√≥n continua
- ‚úÖ **Est√°ndar AGENTS.md**: C√≥digo limpio, documentado, testeado
- ‚úÖ **Excelencia de US anteriores**: Mismo nivel de detalle y calidad
- ‚úÖ **Stack tecnol√≥gico**: Compatible con Polars, Pandas, sklearn

### Capacidades del Sistema

El sistema implementado ser√°:

1. **Robusto**: M√∫ltiples m√©tricas de drift (PSI, KS, Wasserstein)
2. **Inteligente**: Alertas basadas en severidad y m√∫ltiples thresholds
3. **Automatizable**: Script ejecutable v√≠a cron, scheduler o CI/CD
4. **Escalable**: Puede procesar datasets grandes con sampling
5. **Mantenible**: C√≥digo modular con separaci√≥n de concerns
6. **Extensible**: F√°cil agregar custom metrics
7. **Versionable**: Reportes y configs versionables con Git/DVC
8. **Completo**: Cubre data drift, target drift, prediction drift y performance

### Comparaci√≥n con Alternativas

| Aspecto | Evidently | Deepchecks | Arize |
|---------|-----------|------------|-------|
| **Instalado** | ‚úÖ S√≠ | ‚ùå No | ‚ùå No |
| **Mantenimiento** | ‚úÖ Activo (Nov 2024) | ‚ö†Ô∏è Menos activo (1 a√±o) | ‚úÖ Activo |
| **Especializaci√≥n** | ‚úÖ Drift detection | ‚ö†Ô∏è Testing general | ‚úÖ Monitoring |
| **Costo** | $0 | $0 | $$ |
| **Complejidad** | ‚úÖ Baja | ‚ö†Ô∏è Media | ‚ö†Ô∏è Alta |
| **Control** | ‚úÖ Total | ‚úÖ Total | ‚ùå Limitado |
| **Tiempo impl.** | 18h | 22h | 25h+ |

**Decisi√≥n**: Evidently ofrece el mejor balance entre capacidades, mantenimiento y simplicidad.

---

## üìã Pr√≥ximos Pasos

1. **Aprobaci√≥n del plan** por el equipo
2. **Instalaci√≥n de PyYAML**: `poetry add pyyaml`
3. **Inicio de implementaci√≥n**: Seguir plan de 18 horas en 11 fases
4. **Revisi√≥n intermedia**: Despu√©s de Fase 5 (drift analyzer implementado)
5. **Testing y validaci√≥n**: Fase 10 con datos reales
6. **Documentaci√≥n y entrega**: Fase 11

**Tiempo estimado de entrega**: 2-3 d√≠as de trabajo efectivo

---

**Documento creado por**: Arthur (MLOps/SRE Engineer)  
**Fecha**: 16 de Noviembre, 2025  
**Versi√≥n**: 3.0 (Revertido a Evidently AI - herramienta m√°s activa)  
**Estado**: üìã PENDIENTE DE APROBACI√ìN

---

## üìö Referencias

### Evidently AI
- **Documentaci√≥n oficial**: https://docs.evidentlyai.com/
- **GitHub**: https://github.com/evidentlyai/evidently (4.8k stars)
- **Tutoriales**: https://docs.evidentlyai.com/user-guide/tutorials
- **API Reference**: https://docs.evidentlyai.com/reference/api-reference
- **√öltima release**: v0.7.14 (Noviembre 2024)

### Comparaciones
- **Evidently vs Deepchecks**: https://docs.evidentlyai.com/get-started/compare
- **ML Monitoring Best Practices**: https://madewithml.com/courses/mlops/monitoring/

### Proyecto
- **US-020**: FastAPI Endpoints (integraci√≥n con API)
- **US-025**: Cloud Run Deployment (deployment de monitoring)
- **AGENTS.md**: Est√°ndares del proyecto
