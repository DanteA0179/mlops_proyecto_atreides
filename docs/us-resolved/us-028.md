# US-028: Sistema de Monitoreo de Modelo en Producci√≥n (MVP)

**Estado**: ‚úÖ COMPLETADO
**Fecha de planeaci√≥n**: Noviembre 2025
**Fecha de completaci√≥n**: Noviembre 12, 2025
**Tipo**: MLOps Monitoring + Model Observability
**Prioridad**: Alta (Sprint 2)

---

## üìã Resumen Ejecutivo

Se implementar√° un **sistema de monitoreo MVP ultra-simple** para el modelo Stacking Ensemble (ensemble_lightgbm_v1.pkl) en producci√≥n. El sistema utilizar√° **Evidently AI** para detectar data drift, generando un reporte HTML b√°sico.

**Problema a Resolver**: El modelo en producci√≥n (US-020) no tiene visibilidad sobre cambios en la distribuci√≥n de datos de entrada (data drift).

**Soluci√≥n MVP Ultra-Simple**:
1. Captura predicciones y features de entrada (CSV simple)
2. Script que genera 1 reporte de Evidently (Data Drift)
3. Dashboard HTML b√°sico

**Alcance MVP**: Sistema funcional m√≠nimo para detectar drift. Sin alertas, sin performance metrics complejos, sin middleware. Solo lo esencial para tener visibilidad b√°sica del modelo en producci√≥n.

**Tiempo de Implementaci√≥n**: 8 horas

---

## üéØ Objetivos del MVP

### ‚úÖ Objetivo 1: Captura B√°sica de Datos
- Loggear predicciones directamente en endpoint de API (append a CSV)
- Capturar: features de entrada, predicci√≥n, timestamp
- Archivo CSV simple: `data/monitoring/predictions.csv`

### ‚úÖ Objetivo 2: Dataset de Referencia
- Preparar archivo de referencia desde training data
- 1,000 muestras aleatorias del training set
- Guardar como: `data/monitoring/reference_data.csv`

### ‚úÖ Objetivo 3: Reporte de Data Drift
- 1 script que genera reporte HTML de Evidently
- Detectar drift en features principales
- Comparar producci√≥n vs referencia
- Output: `reports/monitoring/drift_report.html`

---

## üèóÔ∏è Arquitectura Simplificada

### Flujo de Datos

```
API /predict ‚Üí Log to CSV ‚Üí (manual) Run script ‚Üí HTML Report
```

### Estructura de C√≥digo

```
src/monitoring/
‚îú‚îÄ‚îÄ __init__.py                      # M√≥dulo simple
‚îî‚îÄ‚îÄ log_prediction.py                # Funci√≥n simple de logging (50 l√≠neas)

scripts/
‚îú‚îÄ‚îÄ prepare_reference_data.py        # Genera reference_data.csv (50 l√≠neas)
‚îî‚îÄ‚îÄ generate_drift_report.py         # Genera reporte Evidently (100 l√≠neas)

data/monitoring/
‚îú‚îÄ‚îÄ reference_data.csv               # 1,000 muestras de training
‚îî‚îÄ‚îÄ predictions.csv                  # Predicciones de producci√≥n (append)

reports/monitoring/
‚îî‚îÄ‚îÄ drift_report.html                # Reporte HTML de Evidently

tests/unit/
‚îî‚îÄ‚îÄ test_monitoring.py               # Tests b√°sicos (5 tests)
```

**Total de archivos**: 5 archivos Python (~250 l√≠neas totales)

---

## üí° Componentes a Implementar (Ultra-Simplificados)

### 1. Funci√≥n de Logging Simple

**Archivo**: `src/monitoring/log_prediction.py` (~50 l√≠neas)

```python
import csv
from pathlib import Path
from datetime import datetime

def log_prediction(features: dict, prediction: float) -> None:
    """
    Append prediction to CSV file.

    Parameters
    ----------
    features : dict
        Input features (18 features after engineering)
    prediction : float
        Model prediction
    """
    log_file = Path("data/monitoring/predictions.csv")

    # Create file with header if doesn't exist
    if not log_file.exists():
        log_file.parent.mkdir(parents=True, exist_ok=True)
        with open(log_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp'] + list(features.keys()) + ['prediction'])

    # Append prediction
    with open(log_file, 'a', newline='') as f:
        writer = csv.writer(f)
        row = [datetime.now().isoformat()] + list(features.values()) + [prediction]
        writer.writerow(row)
```

**Integraci√≥n en API** (modificar `src/api/routes/predict.py`):
```python
from src.monitoring.log_prediction import log_prediction

@router.post("/predict")
async def predict(request: PredictionRequest):
    # ... existing code ...

    # Log prediction (simple append)
    try:
        log_prediction(features_dict, predicted_usage)
    except Exception as e:
        logger.warning(f"Failed to log prediction: {e}")

    return response
```

### 2. Script para Preparar Datos de Referencia

**Archivo**: `scripts/prepare_reference_data.py` (~50 l√≠neas)

```python
import pandas as pd
from pathlib import Path

def prepare_reference_data():
    """Sample 1000 rows from training data as reference."""
    # Load training data
    df_train = pd.read_parquet("data/processed/steel_preprocessed_train.parquet")

    # Sample 1000 rows
    df_ref = df_train.sample(n=1000, random_state=42)

    # Save as CSV
    output_path = Path("data/monitoring/reference_data.csv")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df_ref.to_csv(output_path, index=False)

    print(f"Reference data saved: {output_path}")
    print(f"Shape: {df_ref.shape}")

if __name__ == "__main__":
    prepare_reference_data()
```

### 3. Script para Generar Reporte de Drift

**Archivo**: `scripts/generate_drift_report.py` (~100 l√≠neas)

```python
import pandas as pd
from pathlib import Path
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

def generate_drift_report():
    """Generate Evidently data drift report."""
    # Load reference data
    reference_df = pd.read_csv("data/monitoring/reference_data.csv")

    # Load production data
    production_df = pd.read_csv("data/monitoring/predictions.csv")

    # Remove timestamp column
    production_df = production_df.drop('timestamp', axis=1)

    # Create Evidently report
    report = Report(metrics=[
        DataDriftPreset(),
    ])

    # Run report
    report.run(
        reference_data=reference_df,
        current_data=production_df
    )

    # Save HTML report
    output_path = Path("reports/monitoring/drift_report.html")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    report.save_html(str(output_path))

    print(f"Report generated: {output_path}")

if __name__ == "__main__":
    generate_drift_report()
```

---

## üîß Pasos de Implementaci√≥n Simplificados

### Paso 1: Preparar Dataset de Referencia (30 min)

```bash
# Ejecutar script para crear reference_data.csv
poetry run python scripts/prepare_reference_data.py
```

Este script toma 1,000 muestras aleatorias del training set.

### Paso 2: Implementar Logging en API (1 hora)

1. Crear `src/monitoring/log_prediction.py` (funci√≥n simple de 50 l√≠neas)
2. Modificar `src/api/routes/predict.py` para llamar a `log_prediction()`
3. Testear con 10 predicciones, verificar CSV creado

### Paso 3: Crear Script de Reporte (1 hora)

1. Crear `scripts/generate_drift_report.py` (~100 l√≠neas)
2. Usar `DataDriftPreset` de Evidently
3. Generar HTML report

### Paso 4: Testing B√°sico (1 hora)

1. Hacer 50 predicciones a trav√©s de la API
2. Ejecutar script de reporte: `poetry run python scripts/generate_drift_report.py`
3. Abrir HTML y verificar visualizaciones

### Paso 5: Documentaci√≥n (30 min)

1. Crear README simple en `src/monitoring/`
2. Documentar uso de scripts

---

## üìÅ Archivos a Crear

### C√≥digo Fuente (~200 l√≠neas total)

| Archivo | L√≠neas | Prop√≥sito |
|---------|--------|-----------|
| `src/monitoring/__init__.py` | 5 | Exports del m√≥dulo |
| `src/monitoring/log_prediction.py` | 50 | Funci√≥n simple de logging |

### Scripts (~150 l√≠neas total)

| Archivo | L√≠neas | Prop√≥sito |
|---------|--------|-----------|
| `scripts/prepare_reference_data.py` | 50 | Preparar dataset de referencia |
| `scripts/generate_drift_report.py` | 100 | Generar reporte Evidently |

### Tests (~100 l√≠neas)

| Archivo | L√≠neas | Prop√≥sito |
|---------|--------|-----------|
| `tests/unit/test_monitoring.py` | 100 | Tests b√°sicos (5 tests) |

### Documentaci√≥n (~50 l√≠neas)

| Archivo | L√≠neas | Prop√≥sito |
|---------|--------|-----------|
| `src/monitoring/README.md` | 50 | README simple del m√≥dulo |

**Total Estimado**: ~500 l√≠neas (c√≥digo + tests + docs)

---

## üß™ Estrategia de Testing (Simplificada)

### Unit Tests B√°sicos

**Test Coverage Target**: >70% (suficiente para MVP)

**Archivo**: `tests/unit/test_monitoring.py`

**Tests** (5 tests b√°sicos):
1. `test_log_prediction_creates_file` - Verifica que se crea CSV
2. `test_log_prediction_appends` - Verifica que appends funcionan
3. `test_prepare_reference_data` - Verifica script de referencia
4. `test_generate_drift_report` - Verifica que reporte se genera
5. `test_csv_has_correct_columns` - Verifica schema del CSV

### Manual Testing

**Checklist m√≠nimo**:
- [ ] Hacer 20 predicciones a trav√©s de la API
- [ ] Verificar que se cre√≥ `data/monitoring/predictions.csv`
- [ ] Ejecutar script de generaci√≥n de reporte
- [ ] Abrir `drift_report.html` y verificar visualizaciones
- [ ] Verificar que muestra drift detection

---

## üìä M√©tricas de √âxito (Simplificadas)

### M√©tricas de Calidad M√≠nimas

| M√©trica | Target |
|---------|--------|
| Test Coverage | >70% |
| Tests passing | 100% (5 tests) |
| Ruff checks | 0 errors cr√≠ticos |
| Prediction Logging funciona | S√≠ |
| Reporte se genera | S√≠ |

### M√©tricas de Monitoreo

| M√©trica | Descripci√≥n |
|---------|-------------|
| **Data Drift Detection** | Detecta drift en features principales |
| **Reporte HTML** | Se genera y es accesible |
| **Predicciones loggeadas** | 100% de predicciones se capturan |

---

## ‚úÖ Criterios de Aceptaci√≥n (MVP)

### Funcionales M√≠nimos

- [x] Predicciones se loggean en CSV autom√°ticamente
- [x] Dataset de referencia generado (1,000 muestras)
- [x] Script `generate_drift_report.py` funciona
- [x] Reporte HTML se genera correctamente
- [x] Reporte muestra data drift en features
- [x] Sistema maneja >20 predicciones sin errores (26 predicciones validadas)

### No Funcionales M√≠nimos

- [x] Tests b√°sicos (8 tests) pasan
- [x] Coverage 100% (supera requisito de >70%)
- [x] C√≥digo cumple con AGENTS.md b√°sico (docstrings, type hints)
- [x] README completo en src/monitoring/

### Documentaci√≥n M√≠nima

- [x] README.md en src/monitoring/ con ejemplos b√°sicos
- [x] Docstrings en funciones principales
- [x] Comentarios en c√≥digo complejo

---

## ‚è±Ô∏è Estimaci√≥n de Esfuerzo (MVP Ultra-Simple)

### Desarrollo (Total: 8 horas)

| Tarea | Horas |
|-------|-------|
| **Preparar dataset de referencia** | 0.5h |
| - Script simple (50 l√≠neas) | |
| **Implementar logging en API** | 2h |
| - Crear `log_prediction.py` (50 l√≠neas) | 1h |
| - Modificar endpoint `/predict` | 0.5h |
| - Testear logging manualmente | 0.5h |
| **Script de reporte Evidently** | 2h |
| - Crear `generate_drift_report.py` (100 l√≠neas) | 1.5h |
| - Testear generaci√≥n de reporte | 0.5h |
| **Tests unitarios b√°sicos** | 2h |
| - 5 tests b√°sicos | 1.5h |
| - Ejecutar y verificar coverage >70% | 0.5h |
| **Documentaci√≥n m√≠nima** | 1h |
| - README en src/monitoring/ | 0.5h |
| - Docstrings b√°sicos | 0.5h |
| **Buffer y QA** | 0.5h |
| - Bug fixes menores | |

**Total: 8 horas (1 d√≠a de trabajo completo)**

---

## ‚ö†Ô∏è Riesgos y Mitigaci√≥n (Simplificados)

### Riesgo 1: Logging Lento

**Riesgo**: CSV append puede ser lento con muchas predicciones.

**Mitigaci√≥n**:
- Para MVP, es aceptable (bajo volumen)
- Futuro: migrar a Parquet con buffering

### Riesgo 2: CSV Crece Mucho

**Riesgo**: Archivo CSV crece indefinidamente.

**Mitigaci√≥n**:
- Documentar limpieza manual peri√≥dica
- Futuro: implementar rotaci√≥n autom√°tica

### Riesgo 3: Evidently Versi√≥n

**Riesgo**: Breaking changes en Evidently.

**Mitigaci√≥n**:
- Pin version: `evidently==0.4.30`
- Documentar versi√≥n compatible

---

## üîÑ Mejoras Futuras (Post-MVP)

Si el MVP funciona bien, considerar:

1. **Alertas autom√°ticas** cuando drift > threshold
2. **Parquet con buffering** en lugar de CSV simple
3. **Rotaci√≥n autom√°tica** de archivos
4. **Dashboard Streamlit** con m√©tricas en vivo
5. **Performance metrics** con ground truth capturado

---

## üìö Referencias

### Documentaci√≥n

- [Evidently AI Docs](https://docs.evidentlyai.com/)
- [Evidently DataDriftPreset](https://docs.evidentlyai.com/reference/all-metrics)

### C√≥digo Relacionado

- US-020: FastAPI Endpoints (`src/api/`)
- US-012: Preprocessing Pipeline

### Dependencias

- `evidently==0.4.30` (agregar a pyproject.toml)

---

## üéØ Alcance del MVP Ultra-Simple

### ‚úÖ Incluido en MVP

- [x] Logging b√°sico de predicciones a CSV
- [x] Dataset de referencia (1,000 muestras)
- [x] 1 reporte HTML de data drift
- [x] Script simple para generar reporte
- [x] 5 tests b√°sicos (>70% coverage)
- [x] README m√≠nimo

### ‚ùå NO Incluido en MVP

- [ ] Alertas autom√°ticas
- [ ] Dashboard en tiempo real
- [ ] Performance metrics (requiere ground truth)
- [ ] Rotaci√≥n autom√°tica de archivos
- [ ] Buffering en memoria
- [ ] Parquet (solo CSV simple)
- [ ] Multiple reportes (solo data drift)
- [ ] Target drift report
- [ ] Integration con Prometheus

---

## üìù Pasos para Implementaci√≥n (8 horas)

### Orden de Implementaci√≥n

1. **Agregar dependencia** (5 min)
   ```bash
   poetry add evidently==0.4.30
   ```

2. **Crear `scripts/prepare_reference_data.py`** (30 min)
   - Samplear 1,000 rows de training data
   - Guardar como CSV

3. **Crear `src/monitoring/log_prediction.py`** (1 hora)
   - Funci√≥n simple para append a CSV
   - Testear manualmente

4. **Modificar `src/api/routes/predict.py`** (30 min)
   - Importar y llamar `log_prediction()`
   - Hacer 20 predicciones de prueba

5. **Crear `scripts/generate_drift_report.py`** (2 horas)
   - Leer CSVs
   - Crear reporte Evidently
   - Testear generaci√≥n

6. **Crear tests b√°sicos** (2 horas)
   - `tests/unit/test_monitoring.py`
   - 5 tests simples

7. **Crear README** (1 hora)
   - `src/monitoring/README.md`
   - Ejemplos de uso

---

## üéâ Definici√≥n de "Done" (MVP)

La US-028 MVP se considerar√° completa cuando:

1. ‚úÖ C√≥digo implementado (5 archivos, 502 l√≠neas) - **COMPLETADO**
2. ‚úÖ Tests b√°sicos pasando (8 tests, 100% coverage) - **COMPLETADO** (super√≥ requisito >70%)
3. ‚úÖ 20+ predicciones loggeadas exitosamente en CSV - **COMPLETADO** (26 predicciones)
4. ‚úÖ Reporte HTML generado y verificado visualmente - **COMPLETADO** (3.5MB)
5. ‚úÖ README con ejemplos de uso - **COMPLETADO** (230 l√≠neas)
6. ‚úÖ C√≥digo cumple AGENTS.md b√°sico (docstrings, type hints) - **COMPLETADO**
7. ‚úÖ Ruff clean (0 errores cr√≠ticos) - **COMPLETADO**
8. ‚úÖ Peer review aprobado - **PENDIENTE**
9. ‚úÖ Merged to main branch - **PENDIENTE**

---

**Documento creado por**: MLOps Team - Proyecto Atreides
**Fecha de planeaci√≥n**: Noviembre 2025
**Fecha de completaci√≥n**: Noviembre 12, 2025
**Versi√≥n**: 2.0 (MVP Ultra-Simplificado)
**Estado**: ‚úÖ COMPLETADO - Implementado y validado
**Tiempo real**: 8 horas (seg√∫n estimaci√≥n)

---

## üìã Aprobaci√≥n y Sign-Off

**Implementado por**:
- [x] Julian Diaz Lombardo (ML Engineer) - Implementaci√≥n y testing
- [x] Claude AI - Asistencia de desarrollo

**Aprobaci√≥n pendiente**:
- [ ] Peer review
- [ ] Merge to main branch

**Sprint**: Sprint 2
**Branch**: Evidently

---

## üí¨ Resumen

Este MVP ultra-simplificado proporciona **lo m√≠nimo funcional** para tener visibilidad del modelo en producci√≥n:

- **CSV logging simple**: Sin overhead de performance
- **1 reporte de drift**: Data drift en features
- **Implementaci√≥n r√°pida**: 8 horas total
- **Extensible**: F√°cil de mejorar despu√©s

El sistema permite detectar cuando la distribuci√≥n de datos en producci√≥n difiere del training, que es el problema m√°s cr√≠tico a monitorear.

**¬øListo para implementar?** üöÄ
