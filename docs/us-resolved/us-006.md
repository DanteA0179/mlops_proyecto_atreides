# US-006: Pipeline de Limpieza de Datos

## ðŸ“‹ Resumen Ejecutivo

**Estado**: âœ… COMPLETADO
**Fecha**: 2025-01-17 (actualizado con correcciones)
**Objetivo**: Crear un pipeline reproducible que limpie el dataset sucio y genere un dataset listo para EDA
**Calidad Final**: 100% (34,910 filas, 0 nulos, 0 valores "NAN")

---

## ðŸŽ¯ Objetivos y Criterios de AceptaciÃ³n

### Objetivo Principal
Crear un script reproducible que limpie el dataset sucio y genere un dataset listo para EDA y anÃ¡lisis posterior.

### Criterios de AceptaciÃ³n Cumplidos
- âœ… Script `src/data/clean_data.py` completado y funcional
- âœ… Output: `data/processed/steel_cleaned.parquet` (34,910 filas, 11 columnas)
- âœ… ValidaciÃ³n: 100% limpio (0 nulos, 0 "NAN" strings)
- âœ… Versionado con DVC configurado (tag: data-v1.0)
- âœ… Tests unitarios implementados (19 tests, 67.47% coverage)
- âœ… Notebook exploratorio con proceso completo documentado
- âœ… Funciones reutilizables en `src/utils/`

---

## ðŸ“ Archivos Implementados

### Scripts Principales
- **`src/data/clean_data.py`** - Pipeline principal de limpieza (463 lÃ­neas)
  - Proceso completo de limpieza en 6 pasos
  - Logging detallado
  - ValidaciÃ³n automÃ¡tica
  - GeneraciÃ³n de reportes

### Funciones Reutilizables
- **`src/utils/data_cleaning.py`** - Funciones de limpieza reutilizables
  - `convert_data_types()` - ConversiÃ³n de tipos (incluye String â†’ Float64 â†’ Int64)
  - `correct_range_violations()` - CorrecciÃ³n de rangos con capping
  - `treat_outliers()` - Tratamiento de outliers
  - `remove_duplicates()` - EliminaciÃ³n de duplicados
  - `validate_cleaned_data()` - ValidaciÃ³n contra referencia

- **`src/utils/data_quality.py`** - AnÃ¡lisis de calidad
  - `analyze_nulls()` - AnÃ¡lisis de valores nulos (maneja DataFrames vacÃ­os)
  - Funciones de comparaciÃ³n de esquemas

### Tests Unitarios
- **`tests/test_clean_data.py`** - 19 tests, todos pasando âœ…
  - TestConvertDataTypes (5 tests)
  - TestAnalyzeNulls (3 tests)
  - TestCorrectRangeViolations (3 tests)
  - TestTreatOutliers (2 tests)
  - TestRemoveDuplicates (3 tests)
  - TestValidateCleanedData (3 tests)
  - Coverage: 67.47% en data_cleaning.py

### Notebooks
- **`notebooks/exploratory/01_data_cleaning.ipynb`**
  - Proceso completo documentado en 12 secciones
  - Visualizaciones de calidad de datos
  - AnÃ¡lisis detallado de cada paso
  - ValidaciÃ³n de resultados

### Scripts de ValidaciÃ³n
- **`scripts/validate_cleaning_quality.py`**
  - ValidaciÃ³n completa de calidad
  - ComparaciÃ³n alineada por fecha
  - AnÃ¡lisis estadÃ­stico
  - Reporte detallado de diferencias

### DocumentaciÃ³n
- **`docs/us-006/README.md`** - DocumentaciÃ³n tÃ©cnica
- **`docs/us-006/COMPLETION_SUMMARY.md`** - Resumen de completaciÃ³n
- **`docs/us-006/GIT_COMMIT_GUIDE.md`** - GuÃ­a de commit

### Versionado de Datos (DVC)
- **`data/processed/steel_cleaned.parquet.dvc`** - Archivo DVC
  - Dataset versionado
  - Listo para push a remote storage
  - Tag: `data-v1.0`

### Reportes Generados
- **`reports/data_cleaning_log.json`** - Log detallado en JSON
- **`reports/data_cleaning_report.md`** - Reporte en Markdown

---

## ðŸ”§ Pipeline de Limpieza (6 Pasos)

### PASO 1: ConversiÃ³n de Tipos
**FunciÃ³n**: `convert_data_types()`

- Convierte columnas de String a tipos numÃ©ricos apropiados
- Manejo especial para NSM: String â†’ Float64 â†’ Int64 (evita error de conversiÃ³n directa)
- Elimina columna `mixed_type_col` (no Ãºtil)

**Columnas procesadas**:
- NSM: String â†’ Int64 (con paso intermedio Float64)
- Usage_kWh, CO2, Reactive Powers, Power Factors: String â†’ Float64

### PASO 2: Manejo Profesional de Nulos
**FunciÃ³n**: `handle_nulls_professional()`

**Estrategia por prioridad**:
1. **Fecha** (crÃ­tica): Eliminar filas con date nulo o "NAN" string
2. **Filas con >3 nulos**: Eliminar (demasiado incompletas)
3. **Columnas numÃ©ricas**: InterpolaciÃ³n lineal
4. **Valores restantes**: Forward fill â†’ Backward fill
5. **Columnas categÃ³ricas**: Llenar con moda (DESPUÃ‰S de limpiar "NAN")

**Filtros de date crÃ­ticos** (corregidos en versiÃ³n final):
```python
df = df.filter(
    (pl.col('date').is_not_null()) &
    (pl.col('date').str.strip_chars().str.to_uppercase() != "NAN") &
    (pl.col('date').str.strip_chars() != "") &
    (pl.col('date').str.strip_chars().str.to_uppercase() != "N/A")
)
```

### PASO 3: Limpieza de Columnas CategÃ³ricas
**FunciÃ³n**: `clean_categorical_columns()`

**Orden correcto de operaciones** (crÃ­tico para evitar bug "NAN"/"Nan"):
1. **Strip whitespace** primero
2. **Detectar y reemplazar "NAN" con null** (ANTES de titlecase, usando `.to_uppercase() == "NAN"`)
3. **Aplicar titlecase** solo a valores vÃ¡lidos
4. **Llenar nulls con moda**

**Columnas procesadas**:
- WeekStatus: "Weekday", "Weekend"
- Day_of_week: Monday - Sunday
- Load_Type: "Light", "Medium", "Maximum"

**CÃ³digo corregido**:
```python
for col in categorical_cols:
    # STEP 1: Strip whitespace
    df = df.with_columns(pl.col(col).str.strip_chars().alias(col))

    # STEP 2: Replace "NAN" strings with null (BEFORE titlecase)
    df = df.with_columns(
        pl.when(
            (pl.col(col).str.to_uppercase() == "NAN") |
            (pl.col(col).str.to_uppercase() == "N/A") |
            (pl.col(col) == "") |
            (pl.col(col).is_null())
        )
        .then(None)
        .otherwise(pl.col(col))
        .alias(col)
    )

    # STEP 3: NOW apply titlecase to remaining valid values
    df = df.with_columns(pl.col(col).str.to_titlecase().alias(col))

    # STEP 4: Fill nulls with mode
    if nulls_detected > 0:
        mode_value = df[col].mode()[0]
        df = df.with_columns(pl.col(col).fill_null(mode_value).alias(col))
```

### PASO 4: CorrecciÃ³n de Violaciones de Rango
**FunciÃ³n**: `correct_range_violations()`

**MÃ©todo**: Capping (no null, para preservar datos)

**Rangos aplicados**:
- **Power Factor**: 0-100
- **NSM**: 0-86400 (segundos en un dÃ­a)
- **Valores negativos**: Convertir a absolutos (energÃ­a no puede ser negativa)

### PASO 5: Tratamiento de Outliers
**FunciÃ³n**: `treat_outliers()`

**ConfiguraciÃ³n**:
- **Percentiles**: 0.5% y 99.5%
- **MÃ©todo**: Capping
- **Columnas**: Usage_kWh, Reactive Power, CO2
- **Estado**: Deshabilitado por defecto para preservar calidad

### PASO 6: EliminaciÃ³n de Duplicados
**FunciÃ³n**: `remove_duplicates()`

- 467 duplicados eliminados
- Keep: first
- DeduplicaciÃ³n por todas las columnas

---

## ðŸ“Š Resultados Finales

### MÃ©tricas de Dataset Limpio

| MÃ©trica | Valor |
|---------|-------|
| **Filas** | 34,910 |
| **Columnas** | 11 |
| **Nulos** | 0 |
| **Duplicados** | 0 |
| **"NAN" strings** | 0 |
| **"Nan" strings** | 0 |
| **Calidad** | 100% limpio |

### Esquema del Dataset

```python
{
    'date': String,
    'Usage_kWh': Float64,
    'Lagging_Current_Reactive.Power_kVarh': Float64,
    'Leading_Current_Reactive_Power_kVarh': Float64,
    'CO2(tCO2)': Float64,
    'Lagging_Current_Power_Factor': Float64,
    'Leading_Current_Power_Factor': Float64,
    'NSM': Int64,
    'WeekStatus': String,
    'Day_of_week': String,
    'Load_Type': String
}
```

### Validaciones de Calidad Pasadas

```
âœ… Schema match: PASS
âœ… Row count: 34,910 (correcto)
âœ… No nulls: PASS (0 nulos)
âœ… No duplicates: PASS (0 duplicados)
âœ… No "NAN" strings: PASS (0 encontrados)
âœ… Categorical values clean: PASS
âœ… Date column clean: PASS
```

---

## ðŸ› Bugs Corregidos (VersiÃ³n Final)

### Bug CrÃ­tico: "NAN" â†’ "Nan" Conversion

**Problema Original**:
- El cÃ³digo aplicaba `.to_titlecase()` ANTES de detectar "NAN" strings
- Esto convertÃ­a "NAN" â†’ "Nan", que no era detectado por el filtro
- Resultado: 34,933 filas con valores "Nan" en categorÃ­as

**Causa RaÃ­z**:
```python
# âŒ ORDEN INCORRECTO (bug)
1. Strip â†’ "NAN"
2. Titlecase â†’ "Nan"
3. Detect "NAN" â†’ No encuentra "Nan"
```

**SoluciÃ³n Implementada**:
```python
# âœ… ORDEN CORRECTO (fix)
1. Strip â†’ "NAN"
2. Detect and replace "NAN" with null (using .to_uppercase() == "NAN")
3. Apply titlecase to valid values only
4. Fill nulls with mode
```

**Archivos Corregidos**:
1. `src/data/clean_data.py` - FunciÃ³n `clean_categorical_columns()`
2. `notebooks/exploratory/01_data_cleaning.ipynb` - Cell 10

**Resultado**: 34,910 filas limpias (eliminÃ³ 23 filas con "NAN" real)

---

## ðŸ’¡ Lecciones Aprendidas

### DesafÃ­os Enfrentados

1. **NSM con formato decimal**
   - Problema: ConversiÃ³n directa String â†’ Int64 fallaba
   - SoluciÃ³n: ConversiÃ³n en dos pasos (String â†’ Float64 â†’ Int64)

2. **Columnas categÃ³ricas con "NAN" strings**
   - Problema: `.to_titlecase()` convertÃ­a "NAN" â†’ "Nan" antes de detectarlo
   - SoluciÃ³n: Detectar "NAN" ANTES de aplicar transformaciones de texto

3. **Date column con " NAN " (con espacios)**
   - Problema: Filtro solo buscaba nulls, no strings "NAN"
   - SoluciÃ³n: Agregar filtro `.str.to_uppercase() != "NAN"` despuÃ©s de `.str.strip_chars()`

4. **SincronizaciÃ³n Notebook vs Script**
   - Problema: Notebook y script Python tenÃ­an lÃ³gica de limpieza diferente
   - SoluciÃ³n: Sincronizar ambos con la misma lÃ³gica corregida

5. **Balance entre limpieza y preservaciÃ³n**
   - Problema: Capping vs eliminaciÃ³n de outliers
   - SoluciÃ³n: Capping para preservar datos, outlier treatment deshabilitado

### Soluciones Aplicadas

1. **ConversiÃ³n en dos pasos para NSM**
2. **Limpieza de strings con orden correcto**: strip â†’ detect invalid â†’ transform valid â†’ fill nulls
3. **Capping en lugar de null** para preservar datos
4. **ValidaciÃ³n rigurosa** en mÃºltiples puntos del pipeline

### Mejoras Futuras

1. âœ… Ajustar percentiles de outliers (completado - disabled)
2. âœ… Implementar tests unitarios (completado - 19 tests)
3. âœ… Agregar versionado con DVC (completado)
4. â³ Documentar decisiones de negocio sobre outliers
5. â³ Aumentar coverage de tests a >80%
6. â³ Agregar CI/CD para tests automÃ¡ticos

---

## ðŸš€ Uso del Pipeline

### OpciÃ³n 1: Ejecutar Script Principal

```bash
# Desde la raÃ­z del proyecto
python src/data/clean_data.py
```

**Output esperado**:
```
âœ… VALIDATION SUCCESSFUL
Final dataset shape: (34,910, 11)
Nulls: 0
Duplicates: 0
No 'NAN' strings found
Quality: 100% clean
```

### OpciÃ³n 2: Validar Calidad

```bash
python scripts/validate_cleaning_quality.py
```

### OpciÃ³n 3: Explorar en Notebook

```bash
# Desde notebooks/exploratory/
poetry run jupyter lab 01_data_cleaning.ipynb
```

**Nota**: Reiniciar kernel y limpiar outputs antes de ejecutar para evitar cache.

---

## ðŸ“¦ Outputs Generados

### Dataset Limpio
- **`data/processed/steel_cleaned.parquet`**
  - 34,910 filas Ã— 11 columnas
  - 0 nulos
  - 0 duplicados
  - 0 valores "NAN"

### Reportes
- **`reports/data_cleaning_log.json`** - Log detallado del proceso en formato JSON
- **`reports/data_cleaning_report.md`** - Reporte legible en Markdown

### Versionado DVC
- **`data/processed/steel_cleaned.parquet.dvc`** - Archivo de tracking
- **`.gitignore`** - Actualizado para DVC

---

## ðŸ§ª Testing

### Ejecutar Tests

```bash
# Todos los tests
poetry run pytest tests/test_clean_data.py -v

# Con coverage
poetry run pytest tests/test_clean_data.py --cov=src.utils.data_cleaning --cov-report=term-missing

# Test especÃ­fico
poetry run pytest tests/test_clean_data.py::TestConvertDataTypes::test_convert_nsm_string_to_int -v
```

### Resultados de Tests

```
tests/test_clean_data.py::TestConvertDataTypes::test_convert_nsm_string_to_int PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_convert_usage_string_to_float PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_remove_mixed_type_column PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_handle_invalid_numeric_strings PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_preserve_existing_numeric_types PASSED
tests/test_clean_data.py::TestAnalyzeNulls::test_analyze_nulls_basic PASSED
tests/test_clean_data.py::TestAnalyzeNulls::test_analyze_nulls_empty_dataframe PASSED
tests/test_clean_data.py::TestAnalyzeNulls::test_analyze_nulls_no_nulls PASSED
tests/test_clean_data.py::TestCorrectRangeViolations::test_correct_power_factor_range PASSED
tests/test_clean_data.py::TestCorrectRangeViolations::test_correct_nsm_range PASSED
tests/test_clean_data.py::TestCorrectRangeViolations::test_handle_negative_values PASSED
tests/test_clean_data.py::TestTreatOutliers::test_treat_outliers_capping PASSED
tests/test_clean_data.py::TestTreatOutliers::test_outliers_preserve_non_outliers PASSED
tests/test_clean_data.py::TestRemoveDuplicates::test_remove_duplicates_basic PASSED
tests/test_clean_data.py::TestRemoveDuplicates::test_no_duplicates PASSED
tests/test_clean_data.py::TestRemoveDuplicates::test_keep_first_duplicate PASSED
tests/test_clean_data.py::TestValidateCleanedData::test_validation_success PASSED
tests/test_clean_data.py::TestValidateCleanedData::test_validation_row_count_mismatch PASSED
tests/test_clean_data.py::TestValidateCleanedData::test_validation_quality_threshold PASSED

======================== 19 passed in 4.24s ========================
```

**Coverage**: 67.47% (objetivo: >60% âœ…)

---

## ðŸ“š Referencias

### Archivos del Proyecto
- Dataset original: `data/raw/steel_energy_original.csv`
- Dataset sucio: `data/raw/steel_energy_modified.csv`
- Dataset limpio: `data/processed/steel_cleaned.parquet`
- Notebook: `notebooks/exploratory/01_data_cleaning.ipynb`
- Script principal: `src/data/clean_data.py`
- Tests: `tests/test_clean_data.py`

### DocumentaciÃ³n Relacionada
- [US-007: DuckDB Loader](us-007.md) - Carga de datos para EDA
- [US-008: EDA Exhaustivo](us-008.md) - AnÃ¡lisis exploratorio
- [US-009: Time Series Analysis](us-009.md) - AnÃ¡lisis temporal

---

## âœ… Estado Final

**US-006: âœ… COMPLETADA**

- Script principal: âœ… Funcional
- Tests unitarios: âœ… 19/19 passing (67.47% coverage)
- Versionado DVC: âœ… Configurado
- DocumentaciÃ³n: âœ… Completa
- Calidad: âœ… 100% limpio (34,910 filas, 0 nulos, 0 "NAN")
- Notebook: âœ… Sincronizado con script

**Listo para producciÃ³n** ðŸš€

---

**Autor**: Data Engineering Team - Proyecto Atreides
**Fecha de CompletaciÃ³n**: 2025-01-17
**VersiÃ³n**: 2.0 (con correcciones de "NAN" bug)
**PrÃ³ximo Paso**: US-007 (DuckDB Loader) y US-008 (EDA Exhaustivo)
