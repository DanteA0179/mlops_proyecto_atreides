# US-006: Pipeline de Limpieza de Datos

## 📋 Resumen Ejecutivo

**Estado**: ✅ COMPLETADO
**Fecha**: 2025-01-17 (actualizado con correcciones)
**Objetivo**: Crear un pipeline reproducible que limpie el dataset sucio y genere un dataset listo para EDA
**Calidad Final**: 100% (34,910 filas, 0 nulos, 0 valores "NAN")

---

## 🎯 Objetivos y Criterios de Aceptación

### Objetivo Principal
Crear un script reproducible que limpie el dataset sucio y genere un dataset listo para EDA y análisis posterior.

### Criterios de Aceptación Cumplidos
- ✅ Script `src/data/clean_data.py` completado y funcional
- ✅ Output: `data/processed/steel_cleaned.parquet` (34,910 filas, 11 columnas)
- ✅ Validación: 100% limpio (0 nulos, 0 "NAN" strings)
- ✅ Versionado con DVC configurado (tag: data-v1.0)
- ✅ Tests unitarios implementados (19 tests, 67.47% coverage)
- ✅ Notebook exploratorio con proceso completo documentado
- ✅ Funciones reutilizables en `src/utils/`

---

## 📁 Archivos Implementados

### Scripts Principales
- **`src/data/clean_data.py`** - Pipeline principal de limpieza (463 líneas)
  - Proceso completo de limpieza en 6 pasos
  - Logging detallado
  - Validación automática
  - Generación de reportes

### Funciones Reutilizables
- **`src/utils/data_cleaning.py`** - Funciones de limpieza reutilizables
  - `convert_data_types()` - Conversión de tipos (incluye String → Float64 → Int64)
  - `correct_range_violations()` - Corrección de rangos con capping
  - `treat_outliers()` - Tratamiento de outliers
  - `remove_duplicates()` - Eliminación de duplicados
  - `validate_cleaned_data()` - Validación contra referencia

- **`src/utils/data_quality.py`** - Análisis de calidad
  - `analyze_nulls()` - Análisis de valores nulos (maneja DataFrames vacíos)
  - Funciones de comparación de esquemas

### Tests Unitarios
- **`tests/test_clean_data.py`** - 19 tests, todos pasando ✅
  - TestConvertDataTypes (5 tests)
  - TestAnalyzeNulls (3 tests)
  - TestCorrectRangeViolations (3 tests)
  - TestTreatOutliers (2 tests)
  - TestRemoveDuplicates (3 tests)
  - TestValidateCleanedData (3 tests)
  - Coverage: 67.47% en data_cleaning.py

### Notebooks
- **`notebooks/exploratory/01_data_cleaning.ipynb`**
  - Proceso completo documentado en 12 secciones
  - Visualizaciones de calidad de datos
  - Análisis detallado de cada paso
  - Validación de resultados

### Scripts de Validación
- **`scripts/validate_cleaning_quality.py`**
  - Validación completa de calidad
  - Comparación alineada por fecha
  - Análisis estadístico
  - Reporte detallado de diferencias

### Documentación
- **`docs/us-006/README.md`** - Documentación técnica
- **`docs/us-006/COMPLETION_SUMMARY.md`** - Resumen de completación
- **`docs/us-006/GIT_COMMIT_GUIDE.md`** - Guía de commit

### Versionado de Datos (DVC)
- **`data/processed/steel_cleaned.parquet.dvc`** - Archivo DVC
  - Dataset versionado
  - Listo para push a remote storage
  - Tag: `data-v1.0`

### Reportes Generados
- **`reports/data_cleaning_log.json`** - Log detallado en JSON
- **`reports/data_cleaning_report.md`** - Reporte en Markdown

---

## 🔧 Pipeline de Limpieza (6 Pasos)

### PASO 1: Conversión de Tipos
**Función**: `convert_data_types()`

- Convierte columnas de String a tipos numéricos apropiados
- Manejo especial para NSM: String → Float64 → Int64 (evita error de conversión directa)
- Elimina columna `mixed_type_col` (no útil)

**Columnas procesadas**:
- NSM: String → Int64 (con paso intermedio Float64)
- Usage_kWh, CO2, Reactive Powers, Power Factors: String → Float64

### PASO 2: Manejo Profesional de Nulos
**Función**: `handle_nulls_professional()`

**Estrategia por prioridad**:
1. **Fecha** (crítica): Eliminar filas con date nulo o "NAN" string
2. **Filas con >3 nulos**: Eliminar (demasiado incompletas)
3. **Columnas numéricas**: Interpolación lineal
4. **Valores restantes**: Forward fill → Backward fill
5. **Columnas categóricas**: Llenar con moda (DESPUÉS de limpiar "NAN")

**Filtros de date críticos** (corregidos en versión final):
```python
df = df.filter(
    (pl.col('date').is_not_null()) &
    (pl.col('date').str.strip_chars().str.to_uppercase() != "NAN") &
    (pl.col('date').str.strip_chars() != "") &
    (pl.col('date').str.strip_chars().str.to_uppercase() != "N/A")
)
```

### PASO 3: Limpieza de Columnas Categóricas
**Función**: `clean_categorical_columns()`

**Orden correcto de operaciones** (crítico para evitar bug "NAN"/"Nan"):
1. **Strip whitespace** primero
2. **Detectar y reemplazar "NAN" con null** (ANTES de titlecase, usando `.to_uppercase() == "NAN"`)
3. **Aplicar titlecase** solo a valores válidos
4. **Llenar nulls con moda**

**Columnas procesadas**:
- WeekStatus: "Weekday", "Weekend"
- Day_of_week: Monday - Sunday
- Load_Type: "Light", "Medium", "Maximum"

**Código corregido**:
```python
for col in categorical_cols:
    # STEP 1: Strip whitespace
    df = df.with_columns(pl.col(col).str.strip_chars().alias(col))

    # STEP 2: Replace "NAN" strings with null (BEFORE titlecase)
    df = df.with_columns(
        pl.when(
            (pl.col(col).str.to_uppercase() == "NAN") |
            (pl.col(col).str.to_uppercase() == "N/A") |
            (pl.col(col) == "") |
            (pl.col(col).is_null())
        )
        .then(None)
        .otherwise(pl.col(col))
        .alias(col)
    )

    # STEP 3: NOW apply titlecase to remaining valid values
    df = df.with_columns(pl.col(col).str.to_titlecase().alias(col))

    # STEP 4: Fill nulls with mode
    if nulls_detected > 0:
        mode_value = df[col].mode()[0]
        df = df.with_columns(pl.col(col).fill_null(mode_value).alias(col))
```

### PASO 4: Corrección de Violaciones de Rango
**Función**: `correct_range_violations()`

**Método**: Capping (no null, para preservar datos)

**Rangos aplicados**:
- **Power Factor**: 0-100
- **NSM**: 0-86400 (segundos en un día)
- **Valores negativos**: Convertir a absolutos (energía no puede ser negativa)

### PASO 5: Tratamiento de Outliers
**Función**: `treat_outliers()`

**Configuración**:
- **Percentiles**: 0.5% y 99.5%
- **Método**: Capping
- **Columnas**: Usage_kWh, Reactive Power, CO2
- **Estado**: Deshabilitado por defecto para preservar calidad

### PASO 6: Eliminación de Duplicados
**Función**: `remove_duplicates()`

- 467 duplicados eliminados
- Keep: first
- Deduplicación por todas las columnas

---

## 📊 Resultados Finales

### Métricas de Dataset Limpio

| Métrica | Valor |
|---------|-------|
| **Filas** | 34,910 |
| **Columnas** | 11 |
| **Nulos** | 0 |
| **Duplicados** | 0 |
| **"NAN" strings** | 0 |
| **"Nan" strings** | 0 |
| **Calidad** | 100% limpio |

### Esquema del Dataset

```python
{
    'date': String,
    'Usage_kWh': Float64,
    'Lagging_Current_Reactive.Power_kVarh': Float64,
    'Leading_Current_Reactive_Power_kVarh': Float64,
    'CO2(tCO2)': Float64,
    'Lagging_Current_Power_Factor': Float64,
    'Leading_Current_Power_Factor': Float64,
    'NSM': Int64,
    'WeekStatus': String,
    'Day_of_week': String,
    'Load_Type': String
}
```

### Validaciones de Calidad Pasadas

```
✅ Schema match: PASS
✅ Row count: 34,910 (correcto)
✅ No nulls: PASS (0 nulos)
✅ No duplicates: PASS (0 duplicados)
✅ No "NAN" strings: PASS (0 encontrados)
✅ Categorical values clean: PASS
✅ Date column clean: PASS
```

---

## 🐛 Bugs Corregidos (Versión Final)

### Bug Crítico: "NAN" → "Nan" Conversion

**Problema Original**:
- El código aplicaba `.to_titlecase()` ANTES de detectar "NAN" strings
- Esto convertía "NAN" → "Nan", que no era detectado por el filtro
- Resultado: 34,933 filas con valores "Nan" en categorías

**Causa Raíz**:
```python
# ❌ ORDEN INCORRECTO (bug)
1. Strip → "NAN"
2. Titlecase → "Nan"
3. Detect "NAN" → No encuentra "Nan"
```

**Solución Implementada**:
```python
# ✅ ORDEN CORRECTO (fix)
1. Strip → "NAN"
2. Detect and replace "NAN" with null (using .to_uppercase() == "NAN")
3. Apply titlecase to valid values only
4. Fill nulls with mode
```

**Archivos Corregidos**:
1. `src/data/clean_data.py` - Función `clean_categorical_columns()`
2. `notebooks/exploratory/01_data_cleaning.ipynb` - Cell 10

**Resultado**: 34,910 filas limpias (eliminó 23 filas con "NAN" real)

---

## 💡 Lecciones Aprendidas

### Desafíos Enfrentados

1. **NSM con formato decimal**
   - Problema: Conversión directa String → Int64 fallaba
   - Solución: Conversión en dos pasos (String → Float64 → Int64)

2. **Columnas categóricas con "NAN" strings**
   - Problema: `.to_titlecase()` convertía "NAN" → "Nan" antes de detectarlo
   - Solución: Detectar "NAN" ANTES de aplicar transformaciones de texto

3. **Date column con " NAN " (con espacios)**
   - Problema: Filtro solo buscaba nulls, no strings "NAN"
   - Solución: Agregar filtro `.str.to_uppercase() != "NAN"` después de `.str.strip_chars()`

4. **Sincronización Notebook vs Script**
   - Problema: Notebook y script Python tenían lógica de limpieza diferente
   - Solución: Sincronizar ambos con la misma lógica corregida

5. **Balance entre limpieza y preservación**
   - Problema: Capping vs eliminación de outliers
   - Solución: Capping para preservar datos, outlier treatment deshabilitado

### Soluciones Aplicadas

1. **Conversión en dos pasos para NSM**
2. **Limpieza de strings con orden correcto**: strip → detect invalid → transform valid → fill nulls
3. **Capping en lugar de null** para preservar datos
4. **Validación rigurosa** en múltiples puntos del pipeline

### Mejoras Futuras

1. ✅ Ajustar percentiles de outliers (completado - disabled)
2. ✅ Implementar tests unitarios (completado - 19 tests)
3. ✅ Agregar versionado con DVC (completado)
4. ⏳ Documentar decisiones de negocio sobre outliers
5. ⏳ Aumentar coverage de tests a >80%
6. ⏳ Agregar CI/CD para tests automáticos

---

## 🚀 Uso del Pipeline

### Opción 1: Ejecutar Script Principal

```bash
# Desde la raíz del proyecto
python src/data/clean_data.py
```

**Output esperado**:
```
✅ VALIDATION SUCCESSFUL
Final dataset shape: (34,910, 11)
Nulls: 0
Duplicates: 0
No 'NAN' strings found
Quality: 100% clean
```

### Opción 2: Validar Calidad

```bash
python scripts/validate_cleaning_quality.py
```

### Opción 3: Explorar en Notebook

```bash
# Desde notebooks/exploratory/
poetry run jupyter lab 01_data_cleaning.ipynb
```

**Nota**: Reiniciar kernel y limpiar outputs antes de ejecutar para evitar cache.

---

## 📦 Outputs Generados

### Dataset Limpio
- **`data/processed/steel_cleaned.parquet`**
  - 34,910 filas × 11 columnas
  - 0 nulos
  - 0 duplicados
  - 0 valores "NAN"

### Reportes
- **`reports/data_cleaning_log.json`** - Log detallado del proceso en formato JSON
- **`reports/data_cleaning_report.md`** - Reporte legible en Markdown

### Versionado DVC
- **`data/processed/steel_cleaned.parquet.dvc`** - Archivo de tracking
- **`.gitignore`** - Actualizado para DVC

---

## 🧪 Testing

### Ejecutar Tests

```bash
# Todos los tests
poetry run pytest tests/test_clean_data.py -v

# Con coverage
poetry run pytest tests/test_clean_data.py --cov=src.utils.data_cleaning --cov-report=term-missing

# Test específico
poetry run pytest tests/test_clean_data.py::TestConvertDataTypes::test_convert_nsm_string_to_int -v
```

### Resultados de Tests

```
tests/test_clean_data.py::TestConvertDataTypes::test_convert_nsm_string_to_int PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_convert_usage_string_to_float PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_remove_mixed_type_column PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_handle_invalid_numeric_strings PASSED
tests/test_clean_data.py::TestConvertDataTypes::test_preserve_existing_numeric_types PASSED
tests/test_clean_data.py::TestAnalyzeNulls::test_analyze_nulls_basic PASSED
tests/test_clean_data.py::TestAnalyzeNulls::test_analyze_nulls_empty_dataframe PASSED
tests/test_clean_data.py::TestAnalyzeNulls::test_analyze_nulls_no_nulls PASSED
tests/test_clean_data.py::TestCorrectRangeViolations::test_correct_power_factor_range PASSED
tests/test_clean_data.py::TestCorrectRangeViolations::test_correct_nsm_range PASSED
tests/test_clean_data.py::TestCorrectRangeViolations::test_handle_negative_values PASSED
tests/test_clean_data.py::TestTreatOutliers::test_treat_outliers_capping PASSED
tests/test_clean_data.py::TestTreatOutliers::test_outliers_preserve_non_outliers PASSED
tests/test_clean_data.py::TestRemoveDuplicates::test_remove_duplicates_basic PASSED
tests/test_clean_data.py::TestRemoveDuplicates::test_no_duplicates PASSED
tests/test_clean_data.py::TestRemoveDuplicates::test_keep_first_duplicate PASSED
tests/test_clean_data.py::TestValidateCleanedData::test_validation_success PASSED
tests/test_clean_data.py::TestValidateCleanedData::test_validation_row_count_mismatch PASSED
tests/test_clean_data.py::TestValidateCleanedData::test_validation_quality_threshold PASSED

======================== 19 passed in 4.24s ========================
```

**Coverage**: 67.47% (objetivo: >60% ✅)

---

## 📚 Referencias

### Archivos del Proyecto
- Dataset original: `data/raw/steel_energy_original.csv`
- Dataset sucio: `data/raw/steel_energy_modified.csv`
- Dataset limpio: `data/processed/steel_cleaned.parquet`
- Notebook: `notebooks/exploratory/01_data_cleaning.ipynb`
- Script principal: `src/data/clean_data.py`
- Tests: `tests/test_clean_data.py`

### Documentación Relacionada
- [US-007: DuckDB Loader](us-007.md) - Carga de datos para EDA
- [US-008: EDA Exhaustivo](us-008.md) - Análisis exploratorio
- [US-009: Time Series Analysis](us-009.md) - Análisis temporal

---

## ✅ Estado Final

**US-006: ✅ COMPLETADA**

- Script principal: ✅ Funcional
- Tests unitarios: ✅ 19/19 passing (67.47% coverage)
- Versionado DVC: ✅ Configurado
- Documentación: ✅ Completa
- Calidad: ✅ 100% limpio (34,910 filas, 0 nulos, 0 "NAN")
- Notebook: ✅ Sincronizado con script

**Listo para producción** 🚀

---

**Autor**: Data Engineering Team - Proyecto Atreides
**Fecha de Completación**: 2025-01-17
**Versión**: 2.0 (con correcciones de "NAN" bug)
**Próximo Paso**: US-007 (DuckDB Loader) y US-008 (EDA Exhaustivo)
