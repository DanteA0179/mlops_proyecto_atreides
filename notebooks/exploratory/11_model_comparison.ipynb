{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a63d1e3",
   "metadata": {},
   "source": [
    "# ComparaciÃ³n de Modelos - Energy Optimization\n",
    "\n",
    "## US-015: AnÃ¡lisis Comparativo de Modelos Avanzados\n",
    "\n",
    "**Objetivo:** Comparar el rendimiento de todos los modelos implementados y seleccionar el modelo final para producciÃ³n.\n",
    "\n",
    "**Modelos Evaluados:**\n",
    "1. **XGBoost Baseline** (US-013)\n",
    "2. **LightGBM Baseline** (US-015)\n",
    "3. **CatBoost Baseline** (US-015)\n",
    "4. **Ridge Stacking Ensemble** (US-015)\n",
    "5. **LightGBM Stacking Ensemble** (US-015)\n",
    "\n",
    "**MÃ©tricas de EvaluaciÃ³n:**\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- RÂ² (Coefficient of Determination)\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2d627",
   "metadata": {},
   "source": [
    "## 1. Setup y ConfiguraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bc448",
   "metadata": {},
   "outputs": [],
   "source": "# Add project root to path\nimport sys\nfrom pathlib import Path\nsys.path.append(str(Path.cwd().parent.parent))\n\n# Standard libraries\nimport json\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data manipulation\nimport polars as pl\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# ML libraries\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Project utilities\nfrom src.utils.model_evaluation import (\n    calculate_regression_metrics,\n    plot_predictions_vs_actual,\n    plot_residuals\n)\n\n# Configure visualization\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\npl.Config.set_tbl_rows(20)\n\nprint(\"Libraries loaded successfully\")"
  },
  {
   "cell_type": "markdown",
   "id": "ce316f27",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1129b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "data_path = Path(\"../../data/processed\")\n",
    "test_df = pl.read_parquet(data_path / \"steel_preprocessed_test.parquet\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in test_df.columns if col != 'Usage_kWh']\n",
    "X_test = test_df.select(feature_cols).to_numpy()\n",
    "y_test = test_df['Usage_kWh'].to_numpy()\n",
    "\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Mean: {y_test.mean():.2f} kWh\")\n",
    "print(f\"  Std:  {y_test.std():.2f} kWh\")\n",
    "print(f\"  Min:  {y_test.min():.2f} kWh\")\n",
    "print(f\"  Max:  {y_test.max():.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf703dd",
   "metadata": {},
   "source": [
    "## 3. Carga de Modelos Entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c1c88",
   "metadata": {},
   "outputs": [],
   "source": "# Model paths\nmodels_dir = Path(\"../../models\")\nmodels = {}\n\n# Load XGBoost baseline\nwith open(models_dir / \"baselines\" / \"xgboost_model.pkl\", 'rb') as f:\n    models['XGBoost'] = pickle.load(f)\n    \n# Load LightGBM baseline\nwith open(models_dir / \"gradient_boosting\" / \"lightgbm_model.pkl\", 'rb') as f:\n    models['LightGBM'] = pickle.load(f)\n    \n# Load CatBoost baseline\nwith open(models_dir / \"gradient_boosting\" / \"catboost_model.pkl\", 'rb') as f:\n    models['CatBoost'] = pickle.load(f)\n    \n# Load Ridge Ensemble\nwith open(models_dir / \"ensembles\" / \"ensemble_ridge_v2.pkl\", 'rb') as f:\n    models['Ridge Ensemble'] = pickle.load(f)\n\n# Try to load LightGBM Ensemble (may not exist yet)\ntry:\n    with open(models_dir / \"ensembles\" / \"ensemble_lightgbm_v1.pkl\", 'rb') as f:\n        models['LightGBM Ensemble'] = pickle.load(f)\n    print(f\"Loaded {len(models)} models\")\nexcept FileNotFoundError:\n    print(f\"Warning: LightGBM Ensemble not found, using {len(models)} models\")\n\nfor name in models.keys():\n    print(f\"  - {name}\")"
  },
  {
   "cell_type": "markdown",
   "id": "5e337d4c",
   "metadata": {},
   "source": [
    "## 4. EvaluaciÃ³n de Todos los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd50c49",
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions and calculate metrics\nresults = {}\n\nfor model_name, model in models.items():\n    # Generate predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    metrics = calculate_regression_metrics(y_test, y_pred)\n    \n    # Store results\n    results[model_name] = {\n        'predictions': y_pred,\n        'metrics': metrics\n    }\n    \n    print(f\"{model_name:20s} - RMSE: {metrics['rmse']:.4f}, RÂ²: {metrics['r2']:.4f}\")\n\nprint(\"\\nAll models evaluated\")"
  },
  {
   "cell_type": "markdown",
   "id": "94fa50e8",
   "metadata": {},
   "source": [
    "## 5. ComparaciÃ³n de MÃ©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f0ea8",
   "metadata": {},
   "outputs": [],
   "source": "# Create comparison dataframe\ncomparison_data = []\nfor model_name, data in results.items():\n    metrics = data['metrics']\n    comparison_data.append({\n        'Model': model_name,\n        'RMSE': metrics['rmse'],\n        'MAE': metrics['mae'],\n        'RÂ²': metrics['r2'],\n        'MAPE': metrics['mape']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('RMSE')\n\nprint(\"Model Comparison (sorted by RMSE):\")\nprint(comparison_df.to_string(index=False))\n\n# Highlight best model\nbest_model = comparison_df.iloc[0]['Model']\nbest_rmse = comparison_df.iloc[0]['RMSE']\nprint(f\"\\nBest Model: {best_model} (RMSE: {best_rmse:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "aa6e3c6c",
   "metadata": {},
   "source": [
    "## 6. VisualizaciÃ³n: ComparaciÃ³n de MÃ©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23274d",
   "metadata": {},
   "outputs": [],
   "source": "# Create subplots for metrics comparison\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n\nmetrics_to_plot = ['RMSE', 'MAE', 'RÂ²', 'MAPE']\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n\nfor idx, (metric, ax) in enumerate(zip(metrics_to_plot, axes.flat)):\n    # Sort by metric (ascending for errors, descending for RÂ²)\n    sorted_df = comparison_df.sort_values(metric, ascending=(metric != 'RÂ²'))\n    \n    # Create bar plot\n    bars = ax.barh(sorted_df['Model'], sorted_df[metric], color=colors[idx], alpha=0.7)\n    \n    # Highlight best model\n    best_idx = 0 if metric != 'RÂ²' else len(sorted_df) - 1\n    bars[best_idx].set_color(colors[idx])\n    bars[best_idx].set_alpha(1.0)\n    bars[best_idx].set_edgecolor('black')\n    bars[best_idx].set_linewidth(2)\n    \n    # Add value labels\n    for i, (bar, val) in enumerate(zip(bars, sorted_df[metric])):\n        ax.text(val, bar.get_y() + bar.get_height()/2, \n                f'{val:.4f}', \n                ha='left', va='center', fontweight='bold', fontsize=9)\n    \n    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n    ax.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.savefig('../../reports/figures/model_metrics_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Metrics comparison plot saved\")"
  },
  {
   "cell_type": "markdown",
   "id": "1fe313b6",
   "metadata": {},
   "source": [
    "## 7. VisualizaciÃ³n: Predicciones vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions vs actual plot for all models\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=[name for name in results.keys()],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.08\n",
    ")\n",
    "\n",
    "row_col_pairs = [(1,1), (1,2), (1,3), (2,1), (2,2), (2,3)]\n",
    "\n",
    "for idx, (model_name, data) in enumerate(results.items()):\n",
    "    if idx >= 6:  # Max 6 subplots\n",
    "        break\n",
    "        \n",
    "    y_pred = data['predictions']\n",
    "    row, col = row_col_pairs[idx]\n",
    "    \n",
    "    # Add scatter plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_test, \n",
    "            y=y_pred,\n",
    "            mode='markers',\n",
    "            name=model_name,\n",
    "            marker=dict(size=3, opacity=0.5),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val, max_val = y_test.min(), y_test.max()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            line=dict(color='red', dash='dash'),\n",
    "            name='Perfect Prediction',\n",
    "            showlegend=(idx == 0)\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Add metrics annotation\n",
    "    rmse = data['metrics']['rmse']\n",
    "    r2 = data['metrics']['r2']\n",
    "    fig.add_annotation(\n",
    "        text=f\"RMSE: {rmse:.2f}<br>RÂ²: {r2:.4f}\",\n",
    "        xref=f\"x{idx+1}\", yref=f\"y{idx+1}\",\n",
    "        x=0.05, y=0.95,\n",
    "        xanchor='left', yanchor='top',\n",
    "        showarrow=False,\n",
    "        bgcolor='white',\n",
    "        bordercolor='black',\n",
    "        borderwidth=1,\n",
    "        font=dict(size=9),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"Actual Usage (kWh)\")\n",
    "fig.update_yaxes(title_text=\"Predicted Usage (kWh)\")\n",
    "fig.update_layout(\n",
    "    title_text=\"Predictions vs Actual - All Models\",\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.write_html('../../reports/figures/predictions_vs_actual_all_models.html')\n",
    "fig.show()\n",
    "\n",
    "print(\"âœ… Predictions vs Actual plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b9be8",
   "metadata": {},
   "source": [
    "## 8. AnÃ¡lisis de Residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49458594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create residuals plot for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Residual Analysis - All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (model_name, data) in enumerate(results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "        \n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    y_pred = data['predictions']\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Scatter plot of residuals\n",
    "    ax.scatter(y_pred, residuals, alpha=0.5, s=10)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_residual = residuals.mean()\n",
    "    std_residual = residuals.std()\n",
    "    ax.axhline(y=mean_residual + 2*std_residual, color='orange', linestyle=':', alpha=0.7)\n",
    "    ax.axhline(y=mean_residual - 2*std_residual, color='orange', linestyle=':', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Usage (kWh)', fontweight='bold')\n",
    "    ax.set_ylabel('Residuals (kWh)', fontweight='bold')\n",
    "    ax.set_title(f'{model_name}\\n(Mean: {mean_residual:.2f}, Std: {std_residual:.2f})', \n",
    "                 fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/figures/residuals_analysis_all_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Residuals analysis plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a1419",
   "metadata": {},
   "source": [
    "## 9. AnÃ¡lisis por Segmentos: Load_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78777a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct Load_Type from one-hot encoding\n",
    "load_type_cols = [col for col in test_df.columns if col.startswith('Load_Type_')]\n",
    "if load_type_cols:\n",
    "    # Get the column with value 1\n",
    "    load_type_array = test_df.select(load_type_cols).to_numpy()\n",
    "    load_type_indices = load_type_array.argmax(axis=1)\n",
    "    load_types = [col.replace('Load_Type_', '') for col in load_type_cols]\n",
    "    \n",
    "    segment_results = {}\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        y_pred = data['predictions']\n",
    "        segment_metrics = []\n",
    "        \n",
    "        for idx, load_type in enumerate(load_types):\n",
    "            mask = load_type_indices == idx\n",
    "            if mask.sum() > 0:\n",
    "                y_true_seg = y_test[mask]\n",
    "                y_pred_seg = y_pred[mask]\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(y_true_seg, y_pred_seg))\n",
    "                mae = mean_absolute_error(y_true_seg, y_pred_seg)\n",
    "                r2 = r2_score(y_true_seg, y_pred_seg)\n",
    "                \n",
    "                segment_metrics.append({\n",
    "                    'Load_Type': load_type,\n",
    "                    'Count': mask.sum(),\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': mae,\n",
    "                    'RÂ²': r2\n",
    "                })\n",
    "        \n",
    "        segment_results[model_name] = pd.DataFrame(segment_metrics)\n",
    "    \n",
    "    # Display segment analysis for best model\n",
    "    print(f\"ðŸ“Š Segment Analysis for {best_model}:\")\n",
    "    print(segment_results[best_model].to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  No Load_Type columns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a03e7",
   "metadata": {},
   "source": [
    "## 10. ComparaciÃ³n de RMSE por Segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d65742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot by segment\n",
    "if load_type_cols:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(load_types))\n",
    "    width = 0.15\n",
    "    \n",
    "    for idx, (model_name, segment_df) in enumerate(segment_results.items()):\n",
    "        offset = (idx - len(segment_results)/2) * width\n",
    "        rmse_values = segment_df['RMSE'].values\n",
    "        ax.bar(x + offset, rmse_values, width, label=model_name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Load Type', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('RMSE (kWh)', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('RMSE Comparison by Load Type', fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(load_types, rotation=45, ha='right')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../reports/figures/rmse_by_load_type.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… RMSE by Load Type plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a9f6e",
   "metadata": {},
   "source": [
    "## 11. AnÃ¡lisis por Hora del DÃ­a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from NSM (seconds from midnight)\n",
    "hours = (test_df['NSM'].to_numpy() // 3600).astype(int)\n",
    "\n",
    "# Group predictions by hour\n",
    "hour_analysis = {}\n",
    "\n",
    "for model_name, data in results.items():\n",
    "    y_pred = data['predictions']\n",
    "    hourly_metrics = []\n",
    "    \n",
    "    for hour in range(24):\n",
    "        mask = hours == hour\n",
    "        if mask.sum() > 10:  # At least 10 samples\n",
    "            y_true_hour = y_test[mask]\n",
    "            y_pred_hour = y_pred[mask]\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_true_hour, y_pred_hour))\n",
    "            \n",
    "            hourly_metrics.append({\n",
    "                'Hour': hour,\n",
    "                'RMSE': rmse,\n",
    "                'Count': mask.sum()\n",
    "            })\n",
    "    \n",
    "    hour_analysis[model_name] = pd.DataFrame(hourly_metrics)\n",
    "\n",
    "# Plot RMSE by hour\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for model_name, hourly_df in hour_analysis.items():\n",
    "    ax.plot(hourly_df['Hour'], hourly_df['RMSE'], \n",
    "            marker='o', label=model_name, linewidth=2, markersize=6)\n",
    "\n",
    "ax.set_xlabel('Hour of Day', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('RMSE (kWh)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Performance by Hour of Day', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(range(0, 24, 2))\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/figures/rmse_by_hour.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… RMSE by Hour plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a33494",
   "metadata": {},
   "source": [
    "## 12. CorrelaciÃ³n de Errores entre Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for all models\n",
    "residuals_df = pd.DataFrame({\n",
    "    model_name: y_test - data['predictions']\n",
    "    for model_name, data in results.items()\n",
    "})\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = residuals_df.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Error Correlation Between Models', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/figures/error_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Error correlation heatmap saved\")\n",
    "print(\"\\nðŸ“Š Error Correlation Analysis:\")\n",
    "print(\"High correlation (>0.9): Models make similar mistakes\")\n",
    "print(\"Low correlation (<0.7): Models are complementary\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b1efa",
   "metadata": {},
   "source": [
    "## 13. Feature Importance Comparison (XGBoost, LightGBM, CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc93c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from base models\n",
    "importance_data = {}\n",
    "\n",
    "base_models = ['XGBoost', 'LightGBM', 'CatBoost']\n",
    "for model_name in base_models:\n",
    "    if model_name in models:\n",
    "        model = models[model_name]\n",
    "        \n",
    "        # Get the actual model from pipeline (last step)\n",
    "        if hasattr(model, 'named_steps'):\n",
    "            actual_model = model.named_steps['model']\n",
    "        else:\n",
    "            actual_model = model\n",
    "        \n",
    "        # Extract importance based on model type\n",
    "        if hasattr(actual_model, 'feature_importances_'):\n",
    "            importances = actual_model.feature_importances_\n",
    "        elif hasattr(actual_model, 'get_feature_importance'):\n",
    "            importances = actual_model.get_feature_importance()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        importance_data[model_name] = importances\n",
    "\n",
    "# Create comparison dataframe\n",
    "if importance_data:\n",
    "    importance_df = pd.DataFrame(importance_data, index=feature_cols)\n",
    "    \n",
    "    # Plot feature importance comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    importance_df.plot(kind='barh', ax=ax, width=0.8, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Importance', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Features', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Feature Importance Comparison - Base Models', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "    ax.legend(title='Model', loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../reports/figures/feature_importance_comparison.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Feature importance comparison plot saved\")\n",
    "    print(\"\\nðŸ“Š Top 3 Features by Model:\")\n",
    "    for model_name in importance_data.keys():\n",
    "        top_features = importance_df[model_name].nlargest(3)\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for feat, imp in top_features.items():\n",
    "            print(f\"  {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da445ab",
   "metadata": {},
   "source": [
    "## 14. Ensemble Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ensemble models\n",
    "ensemble_models = [name for name in models.keys() if 'Ensemble' in name]\n",
    "\n",
    "for ensemble_name in ensemble_models:\n",
    "    ensemble = models[ensemble_name]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š {ensemble_name} Analysis\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Get base model contributions\n",
    "    if hasattr(ensemble, 'get_base_model_contributions'):\n",
    "        contributions = ensemble.get_base_model_contributions()\n",
    "        \n",
    "        print(\"\\nðŸŽ¯ Base Model Contributions:\")\n",
    "        for model, contrib in contributions.items():\n",
    "            print(f\"  {model:15s}: {contrib:6.2%}\")\n",
    "        \n",
    "        # Plot contributions\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        models_list = list(contributions.keys())\n",
    "        values = list(contributions.values())\n",
    "        \n",
    "        colors_map = {'xgboost': '#FF6B6B', 'lightgbm': '#4ECDC4', 'catboost': '#45B7D1'}\n",
    "        bar_colors = [colors_map.get(m.lower(), '#95A5A6') for m in models_list]\n",
    "        \n",
    "        bars = ax.bar(models_list, values, color=bar_colors, alpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.1%}',\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        ax.set_ylabel('Contribution', fontweight='bold', fontsize=12)\n",
    "        ax.set_title(f'{ensemble_name} - Base Model Contributions', \n",
    "                     fontweight='bold', fontsize=14)\n",
    "        ax.set_ylim(0, max(values) * 1.15)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../../reports/figures/{ensemble_name.lower().replace(\" \", \"_\")}_contributions.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"âœ… {ensemble_name} contributions plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4153dfd",
   "metadata": {},
   "source": [
    "## 15. Mejora vs Baseline (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement vs XGBoost baseline\n",
    "baseline_rmse = results['XGBoost']['metrics']['rmse']\n",
    "\n",
    "improvements = []\n",
    "for model_name, data in results.items():\n",
    "    if model_name != 'XGBoost':\n",
    "        model_rmse = data['metrics']['rmse']\n",
    "        improvement = ((baseline_rmse - model_rmse) / baseline_rmse) * 100\n",
    "        improvements.append({\n",
    "            'Model': model_name,\n",
    "            'RMSE': model_rmse,\n",
    "            'Improvement (%)': improvement,\n",
    "            'RMSE Reduction (kWh)': baseline_rmse - model_rmse\n",
    "        })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvements).sort_values('Improvement (%)', ascending=False)\n",
    "\n",
    "print(\"ðŸ“Š Improvement vs XGBoost Baseline:\")\n",
    "print(improvement_df.to_string(index=False))\n",
    "\n",
    "# Plot improvements\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['green' if x > 0 else 'red' for x in improvement_df['Improvement (%)']]\n",
    "bars = ax.barh(improvement_df['Model'], improvement_df['Improvement (%)'], \n",
    "               color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val, rmse_red in zip(bars, improvement_df['Improvement (%)'], \n",
    "                               improvement_df['RMSE Reduction (kWh)']):\n",
    "    label = f'{val:.2f}% ({rmse_red:+.3f} kWh)'\n",
    "    ax.text(val, bar.get_y() + bar.get_height()/2, label,\n",
    "            ha='left' if val > 0 else 'right', va='center', \n",
    "            fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Improvement (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Improvement vs XGBoost Baseline', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/figures/improvement_vs_baseline.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Improvement comparison plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a9712",
   "metadata": {},
   "source": [
    "## 16. Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92feb6",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"EXECUTIVE SUMMARY - MODEL COMPARISON\")\nprint(\"=\"*70)\n\n# Best model overall\nprint(f\"\\nBEST MODEL: {best_model}\")\nprint(f\"   RMSE: {results[best_model]['metrics']['rmse']:.4f} kWh\")\nprint(f\"   RÂ²:   {results[best_model]['metrics']['r2']:.4f}\")\nprint(f\"   MAE:  {results[best_model]['metrics']['mae']:.4f} kWh\")\n\n# Improvement vs baseline\nbaseline_rmse = results['XGBoost']['metrics']['rmse']\nbest_rmse = results[best_model]['metrics']['rmse']\nimprovement = ((baseline_rmse - best_rmse) / baseline_rmse) * 100\n\nprint(f\"\\nIMPROVEMENT VS BASELINE (XGBoost):\")\nprint(f\"   {improvement:+.2f}% ({baseline_rmse - best_rmse:+.4f} kWh)\")\n\n# Top 3 models\nprint(f\"\\nTOP 3 MODELS:\")\ntop_3 = comparison_df.head(3)\nfor idx, row in top_3.iterrows():\n    medal = ['1st', '2nd', '3rd'][idx] if idx < 3 else '   '\n    print(f\"   {medal} {row['Model']:20s} - RMSE: {row['RMSE']:.4f}\")\n\n# Model complexity vs performance\nprint(f\"\\nCOMPLEXITY vs PERFORMANCE:\")\nprint(f\"   Base Models: Faster training, good performance\")\nprint(f\"   Ensembles:   +2-3s training, +0.1-0.2 kWh accuracy\")\n\nprint(f\"\\nRECOMMENDATION:\")\nif 'Ensemble' in best_model:\n    print(f\"   Use {best_model} for production\")\n    print(f\"   Benefit: Best accuracy with minimal complexity increase\")\nelse:\n    print(f\"   Use {best_model} for production\")\n    print(f\"   Benefit: Excellent performance with simplicity\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Model comparison analysis completed\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "a2be5714",
   "metadata": {},
   "source": [
    "## 17. Conclusiones\n",
    "\n",
    "### Hallazgos Clave:\n",
    "\n",
    "1. **Mejor Modelo**: El modelo con mejor rendimiento general es el que minimiza RMSE\n",
    "\n",
    "2. **Ensembles**: Los modelos ensemble combinan las fortalezas de los modelos base\n",
    "   - Stacking permite aprovechar predicciones complementarias\n",
    "   - Ridge meta-model es simple y efectivo\n",
    "   - LightGBM meta-model puede capturar patrones no-lineales\n",
    "\n",
    "3. **Modelos Base**:\n",
    "   - XGBoost: Excelente baseline, robusto\n",
    "   - LightGBM: RÃ¡pido, eficiente en memoria\n",
    "   - CatBoost: Buen manejo de features categÃ³ricas\n",
    "\n",
    "4. **AnÃ¡lisis por Segmentos**:\n",
    "   - Diferentes Load_Types tienen diferentes dificultades de predicciÃ³n\n",
    "   - Performance varÃ­a por hora del dÃ­a (patrones de consumo)\n",
    "\n",
    "5. **Trade-offs**:\n",
    "   - Ensembles: +1-2% accuracy, +20s training time\n",
    "   - Base models: MÃ¡s rÃ¡pidos, muy buena performance\n",
    "\n",
    "### RecomendaciÃ³n Final:\n",
    "\n",
    "Para **producciÃ³n**, se recomienda el modelo con mejor balance entre:\n",
    "- âœ… PrecisiÃ³n (RMSE mÃ¡s bajo)\n",
    "- âœ… Estabilidad (residuos bien distribuidos)  \n",
    "- âœ… Complejidad (tiempo de inferencia aceptable)\n",
    "- âœ… Interpretabilidad (feature importance clara)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}