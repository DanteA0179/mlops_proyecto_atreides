{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataframe_inicial(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza una limpieza general inicial en el DataFrame.\n",
    "\n",
    "    Esta función toma un DataFrame, elimina los espacios en blanco\n",
    "    de las columnas de tipo 'object' (texto), convierte la columna 'date'\n",
    "    a formato datetime, la ordena cronológicamente, elimina filas con\n",
    "    fechas nulas y extrae la fecha y la hora en nuevas columnas.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame de entrada que se va a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame procesado y limpio.\n",
    "    \"\"\"\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    for col in df_copia.columns:\n",
    "        if df_copia[col].dtype == 'object':\n",
    "            df_copia[col] = df_copia[col].str.strip()\n",
    "\n",
    "    df_copia['date'] = pd.to_datetime(df_copia['date'], dayfirst=True)\n",
    "    df_copia = df_copia.sort_values(by='date', ascending=True)\n",
    "    df_copia = df_copia.dropna(subset=['date'])\n",
    "\n",
    "    #df_copia['fecha'] = df_copia['date'].dt.date\n",
    "    #df_copia['hora'] = df_copia['date'].dt.time\n",
    "    #df_copia = df_copia.replace(['nan', 'NaN', 'None', 'NULL', 'null'], np.nan)\n",
    "\n",
    "\n",
    "    return df_copia\n",
    "\n",
    "def limpiar_columnas_categoricas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia y estandariza las columnas categóricas del DataFrame.\n",
    "\n",
    "    Esta función corrige valores mal escritos y maneja los datos nulos\n",
    "    en las columnas 'WeekStatus', 'Day_of_week' y 'Load_Type'.\n",
    "    Los valores incorrectos se reemplazan por su versión correcta y los\n",
    "    nulos se rellenan utilizando el método 'forward fill'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con las columnas categóricas a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las columnas categóricas estandarizadas.\n",
    "    \"\"\"\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    # Mapeo de correcciones para cada columna\n",
    "    mapa_correcciones = {\n",
    "        'WeekStatus': {\n",
    "            'wEEKDAY': 'Weekday', 'wEEKEND': 'Weekend', 'NAN': np.nan\n",
    "        },\n",
    "        'Day_of_week': {\n",
    "            'mONDAY': 'Monday', 'tUESDAY': 'Tuesday', 'wEDNESDAY': 'Wednesday',\n",
    "            'tHURSDAY': 'Thursday', 'fRIDAY': 'Friday', 'sATURDAY': 'Saturday',\n",
    "            'sUNDAY': 'Sunday', 'NAN': np.nan\n",
    "        },\n",
    "        'Load_Type': {\n",
    "            'lIGHT_lOAD': 'Light_Load', 'mEDIUM_lOAD': 'Medium_Load',\n",
    "            'mAXIMUM_lOAD': 'Maximum_Load', 'NAN': np.nan\n",
    "        }\n",
    "    }\n",
    "\n",
    "    columnas_categoricas = ['WeekStatus', 'Day_of_week', 'Load_Type']\n",
    "\n",
    "    for col in columnas_categoricas:\n",
    "        df_copia[col] = df_copia[col].replace(mapa_correcciones[col])\n",
    "        df_copia[col] = df_copia[col].astype(str)\n",
    "\n",
    "    return df_copia\n",
    "\n",
    "\n",
    "def limpiar_columnas_numericas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia y estandariza un conjunto predefinido de columnas numéricas.\n",
    "\n",
    "    La función identifica valores no numéricos en columnas que deberían serlo,\n",
    "    los convierte a NaN (Not a Number) y luego utiliza el método 'forward fill'\n",
    "    para imputar los valores faltantes. Finalmente, asegura que todas las\n",
    "    columnas procesadas tengan el tipo de dato 'float'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con columnas numéricas a procesar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las columnas numéricas limpias y\n",
    "                      en formato float.\n",
    "    \"\"\"\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    columnas_numericas = [\n",
    "        'Usage_kWh', 'Lagging_Current_Reactive.Power_kVarh',\n",
    "        'Leading_Current_Reactive_Power_kVarh', 'CO2(tCO2)',\n",
    "        'Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor',\n",
    "        'NSM', 'mixed_type_col'\n",
    "    ]\n",
    "\n",
    "    for col in columnas_numericas:\n",
    "        # Forzar valores no numéricos a NaN\n",
    "        df_copia[col] = pd.to_numeric(df_copia[col], errors='coerce')\n",
    "\n",
    "        # Asegurar el tipo float\n",
    "        df_copia[col] = df_copia[col].astype(float)\n",
    "\n",
    "    return df_copia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "class SteelEnergyEDA:\n",
    "    \"\"\"\n",
    "    Clase para realizar análisis exploratorio de datos (EDA), limpieza, visualización y preprocesamiento\n",
    "    de un dataset de energía y acero.\n",
    "\n",
    "    Esta clase carga datos desde un archivo CSV, realiza limpieza inicial, genera visualizaciones,\n",
    "    aplica transformaciones y guarda los datos procesados.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Ruta del archivo CSV que contiene el dataset.\n",
    "        df (pd.DataFrame): DataFrame cargado desde el archivo.\n",
    "        num_cols (list): Lista de nombres de columnas numéricas.\n",
    "        cat_cols (list): Lista de nombres de columnas categóricas.\n",
    "        scaler (StandardScaler): Objeto de escalado para normalizar datos numéricos.\n",
    "        preprocessor (ColumnTransformer): Transformer usado en preprocesamiento.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.num_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.scaler = None\n",
    "        self.preprocessor = None\n",
    "\n",
    "    # Cargar Datos\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Carga el dataset desde un archivo CSV en el atributo `df`.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Primeras filas del dataset cargado.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        print(f\"Se cargo el dataset correctamente: {self.df.shape}\")\n",
    "\n",
    "        return self.df.head()\n",
    "\n",
    "    # Resumen Básico\n",
    "    def basic_summary(self):\n",
    "        print(\"\\n--- INFO ---\")\n",
    "        self.df.info()\n",
    "        print(\"\\n--- STATS ---\")\n",
    "        display(self.df.describe())\n",
    "\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Genera visualizaciones para el dataset.\n",
    "\n",
    "        Incluye:\n",
    "            - Heatmap de correlación para variables numéricas.\n",
    "            - Histogramas para cada variable numérica.\n",
    "            - Pairplot para correlaciones.\n",
    "            - Gráficos de barras para variables categóricas.\n",
    "        \"\"\"\n",
    "        sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "\n",
    "        # 1. HeatMap Correlación\n",
    "        if len(self.num_cols) > 1:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            corr = self.df[self.num_cols].corr()\n",
    "            sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "            plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "        # 2. Histogramas Variables Numéricas\n",
    "        for col in self.num_cols:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.histplot(self.df[col], kde=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "            plt.title(f\"Distribution of {col}\", fontsize=14)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "        # 3. Pairplot para correlaciones de variables numéricas\n",
    "        if len(self.num_cols) > 1:\n",
    "            sns.pairplot(self.df[self.num_cols], corner=True, diag_kind=\"kde\")\n",
    "            plt.suptitle(\"Pairplot of Numeric Variables\", y=1.02, fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "        # 4. Bar charts para variables categóricas\n",
    "        for col in self.cat_cols:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.countplot(x=col, data=self.df, order=self.df[col].value_counts().index, palette=\"pastel\")\n",
    "            plt.title(f\"Count of {col}\", fontsize=14)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Realiza limpieza del dataset.\n",
    "\n",
    "        Pasos:\n",
    "            - Limpieza general usando funciones externas.\n",
    "            - Reemplazo de valores nulos estándar.\n",
    "            - Identificación de columnas numéricas y categóricas.\n",
    "            - Eliminación de duplicados.\n",
    "            - Imputación de valores faltantes.\n",
    "            - Eliminación de outliers usando método IQR.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Primeras filas del dataset limpio.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- LIMPIEZA ---\")\n",
    "\n",
    "        initial_shape = self.df.shape\n",
    "\n",
    "        ## Limpieza General\n",
    "        self.df = limpiar_dataframe_inicial(self.df)\n",
    "        self.df = limpiar_columnas_categoricas(self.df)\n",
    "        self.df = limpiar_columnas_numericas(self.df)\n",
    "        self.df = self.df.replace(['nan', 'NaN', 'None', 'NULL', 'null'], np.nan)\n",
    "\n",
    "\n",
    "        self.num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        self.cat_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        # Eliminar duplicados\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "        # Valores Faltantes\n",
    "        self.df[self.num_cols] = self.df[self.num_cols].fillna(self.df[self.num_cols].median())\n",
    "        for col in self.cat_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "\n",
    "        # Quitar outliers (IQR)\n",
    "        for col in self.num_cols:\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "            self.df = self.df[(self.df[col] >= lower) & (self.df[col] <= upper)]\n",
    "\n",
    "        print(f\"Resultados de Limpieza de datos: {initial_shape[0]} → {self.df.shape[0]} rows\")\n",
    "        return self.df.head()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Realiza preprocesamiento del dataset.\n",
    "\n",
    "        Incluye:\n",
    "            - Extracción de características temporales.\n",
    "            - Encoding cíclico para variables de fecha.\n",
    "            - Normalización de características numéricas.\n",
    "            - Encoding de variables categóricas ordinales.\n",
    "            - Creación de un DataFrame procesado listo para análisis o modelos.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataset procesado.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- PREPROCESAMIENTO ---\")\n",
    "\n",
    "        print(self.cat_cols)\n",
    "        print(self.num_cols)\n",
    "\n",
    "        # Extraer features de tiempo\n",
    "        self.df['date_hour'] = self.df[\"date\"].dt.hour\n",
    "        self.df['date_day'] = self.df[\"date\"].dt.day\n",
    "        self.df['date_month'] = self.df[\"date\"].dt.month\n",
    "        self.df['date_year'] = self.df[\"date\"].dt.year\n",
    "\n",
    "        # Encoding ciclico\n",
    "        for col, max_val in [('date_hour', 24), ('date_month', 12), ('date_day', 31)]:\n",
    "            self.df[f'{col}_sin'] = np.sin(2 * np.pi * self.df[col] / max_val)\n",
    "            self.df[f'{col}_cos'] = np.cos(2 * np.pi * self.df[col] / max_val)\n",
    "\n",
    "\n",
    "\n",
    "        # Dividir Features (Usage_kWh se normaliza junto con las demás)\n",
    "        numeric_features = [\n",
    "            'Usage_kWh', 'CO2(tCO2)', 'NSM',\n",
    "            'Leading_Current_Reactive_Power_kVarh',\n",
    "            'Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor',\n",
    "            'date_hour_sin', 'date_hour_cos',\n",
    "            'date_day_sin', 'date_day_cos',\n",
    "            'date_month_sin', 'date_month_cos',\n",
    "            'date_year'\n",
    "        ]\n",
    "        numeric_features = [col for col in numeric_features if col in self.df.columns]\n",
    "\n",
    "        weekstatus_feature = ['WeekStatus'] if 'WeekStatus' in self.df.columns else []\n",
    "        ordinal_features = ['Load_Type'] if 'Load_Type' in self.df.columns else []\n",
    "\n",
    "        # Pipelines\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        weekstatus_transformer = Pipeline(steps=[\n",
    "            ('label', OrdinalEncoder())\n",
    "        ])\n",
    "\n",
    "        ordinal_transformer = Pipeline(steps=[\n",
    "            ('ordinal', OrdinalEncoder(categories=[[\"Light_Load\", \"Medium_Load\", \"Maximum_Load\"]]))\n",
    "        ])\n",
    "\n",
    "        # Transformer\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('week', weekstatus_transformer, weekstatus_feature),\n",
    "                ('ord', ordinal_transformer, ordinal_features)\n",
    "            ],\n",
    "            sparse_threshold=0\n",
    "\n",
    "        )\n",
    "\n",
    "        self.df_processed = self.preprocessor.fit_transform(self.df)\n",
    "\n",
    "        processed_cols = numeric_features\n",
    "\n",
    "\n",
    "        if weekstatus_feature:\n",
    "            processed_cols += weekstatus_feature\n",
    "\n",
    "        if ordinal_features:\n",
    "            processed_cols += ordinal_features\n",
    "\n",
    "        print(\"Shape of processed data:\", self.df_processed.shape)\n",
    "        print(\"Number of processed columns:\", len(processed_cols))\n",
    "\n",
    "        self.df_processed = pd.DataFrame(self.df_processed, columns=processed_cols)\n",
    "        self.save_data(self.df_processed)\n",
    "\n",
    "        return self.df_processed\n",
    "\n",
    "\n",
    "    def save_data(self, df, output_path=\"cleaned_steel_energy_v2.csv\"):\n",
    "        \"\"\"\n",
    "        Guarda el DataFrame procesado en un archivo CSV.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset a guardar.\n",
    "            output_path (str): Ruta del archivo de salida (default: \"cleaned_steel_energy_v2.csv\").\n",
    "        \"\"\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved cleaned dataset to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "eda = SteelEnergyEDA(\"../../data/raw/steel_energy_modified.csv\")\n",
    "\n",
    "# 1. Cargar Datos\n",
    "eda.load_data()\n",
    "\n",
    "# 2. Limpiar Datos\n",
    "eda.clean_data()\n",
    "\n",
    "# 3. Resumen\n",
    "eda.basic_summary()\n",
    "\n",
    "# 4. Visualizaciones\n",
    "eda.visualize()\n",
    "\n",
    "# 5. Preprocessamiento\n",
    "output = eda.preprocess_data()\n",
    "\n",
    "output.head()\n",
    "output.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64nr8w7i28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class SteelEnergyModeling:\n",
    "    \"\"\"\n",
    "    Clase para entrenar modelos de machine learning sobre datos procesados.\n",
    "\n",
    "    Attributes:\n",
    "        df_processed (pd.DataFrame): DataFrame con los datos ya procesados por SteelEnergyEDA.\n",
    "        target_col (str): Nombre de la columna objetivo a predecir.\n",
    "        preprocessor: ColumnTransformer usado para normalizar datos.\n",
    "        X_train (pd.DataFrame): Features de entrenamiento.\n",
    "        X_val (pd.DataFrame): Features de validación.\n",
    "        X_test (pd.DataFrame): Features de prueba.\n",
    "        y_train (pd.Series): Target de entrenamiento.\n",
    "        y_val (pd.Series): Target de validación.\n",
    "        y_test (pd.Series): Target de prueba.\n",
    "        models (dict): Diccionario con los modelos entrenados {'nombre_modelo': modelo}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_processed, preprocessor=None, target_col='Usage_kWh'):\n",
    "        \"\"\"\n",
    "        Inicializa el objeto SteelEnergyModeling.\n",
    "\n",
    "        Args:\n",
    "            df_processed (pd.DataFrame): DataFrame procesado que contiene features y target.\n",
    "            preprocessor: ColumnTransformer usado en preprocesamiento (para des-normalizar).\n",
    "            target_col (str): Nombre de la columna objetivo. Default: 'Usage_kWh'.\n",
    "        \"\"\"\n",
    "        self.df_processed = df_processed\n",
    "        self.preprocessor = preprocessor\n",
    "        self.target_col = target_col\n",
    "        self.target_scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        self.models = {}\n",
    "\n",
    "        # Obtener el scaler del target desde el ColumnTransformer\n",
    "        if preprocessor is not None:\n",
    "            # El scaler está en el transformer 'num'\n",
    "            self.target_scaler = preprocessor.named_transformers_['num'].named_steps['scaler']\n",
    "\n",
    "    def _inverse_transform_target(self, y_scaled):\n",
    "        \"\"\"\n",
    "        Des-normaliza los valores del target.\n",
    "\n",
    "        Args:\n",
    "            y_scaled: Valores normalizados del target (array o series).\n",
    "\n",
    "        Returns:\n",
    "            Valores des-normalizados.\n",
    "        \"\"\"\n",
    "        if self.target_scaler is None:\n",
    "            return y_scaled\n",
    "\n",
    "        # Asegurar que es un array 2D\n",
    "        y_scaled_2d = np.array(y_scaled).reshape(-1, 1)\n",
    "\n",
    "        # El StandardScaler normalizó todas las columnas numéricas juntas\n",
    "        # Necesitamos usar los parámetros específicos del target (primera columna = Usage_kWh)\n",
    "        target_mean = self.target_scaler.mean_[0]\n",
    "        target_scale = self.target_scaler.scale_[0]\n",
    "\n",
    "        # Des-normalizar: y = y_scaled * scale + mean\n",
    "        y_original = y_scaled_2d * target_scale + target_mean\n",
    "\n",
    "        return y_original.flatten()\n",
    "\n",
    "    def prepare_data(self, test_size=0.2, val_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Divide los datos en conjuntos de entrenamiento, validación y prueba.\n",
    "\n",
    "        Args:\n",
    "            test_size (float): Proporción del conjunto de prueba (0-1). Default: 0.2.\n",
    "            val_size (float): Proporción del conjunto de validación (0-1). Default: 0.2.\n",
    "            random_state (int): Semilla para reproducibilidad. Default: 42.\n",
    "\n",
    "        Returns:\n",
    "            None. Actualiza los atributos X_train, X_val, X_test, y_train, y_val, y_test del objeto.\n",
    "        \"\"\"\n",
    "        X = self.df_processed.drop(columns=[self.target_col])\n",
    "        y = self.df_processed[self.target_col]\n",
    "\n",
    "        # Primero dividir en train+val y test\n",
    "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Luego dividir train+val en train y val\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n",
    "        )\n",
    "\n",
    "        print(f\"Train: {self.X_train.shape}, Val: {self.X_val.shape}, Test: {self.X_test.shape}\")\n",
    "\n",
    "    def train_linear_regression(self):\n",
    "        \"\"\"\n",
    "        Entrena un modelo de regresión lineal.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None. Guarda el modelo entrenado en self.models['linear_regression']\n",
    "            e imprime el R² en el conjunto de validación.\n",
    "        \"\"\"\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(self.X_train, self.y_train)\n",
    "        self.models['linear_regression'] = lr\n",
    "\n",
    "        y_pred = lr.predict(self.X_val)\n",
    "        print(f\"Linear Regression - Val R²: {r2_score(self.y_val, y_pred):.4f}\")\n",
    "\n",
    "    def train_xgboost(self, **params):\n",
    "        \"\"\"\n",
    "        Entrena un modelo XGBoost.\n",
    "\n",
    "        Args:\n",
    "            **params: Parámetros opcionales para XGBRegressor (ej: n_estimators=100,\n",
    "                     max_depth=6, learning_rate=0.1, random_state=42).\n",
    "\n",
    "        Returns:\n",
    "            None. Guarda el modelo entrenado en self.models['xgboost']\n",
    "            e imprime el R² en el conjunto de validación.\n",
    "        \"\"\"\n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        xgb_model.fit(self.X_train, self.y_train)\n",
    "        self.models['xgboost'] = xgb_model\n",
    "\n",
    "        y_pred = xgb_model.predict(self.X_val)\n",
    "        print(f\"XGBoost - Val R²: {r2_score(self.y_val, y_pred):.4f}\")\n",
    "\n",
    "    def evaluate_models(self, model_names=None):\n",
    "        \"\"\"\n",
    "        Evalúa los modelos entrenados en los conjuntos de validación y prueba.\n",
    "        Des-normaliza las predicciones antes de calcular métricas.\n",
    "\n",
    "        Args:\n",
    "            model_names (list): Lista de nombres de modelos a evaluar.\n",
    "                               Si es None, evalúa todos los modelos. Default: None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame con las métricas (R², RMSE, MAE, CV) de validación y test.\n",
    "                         CV = Coefficient of Variation = RMSE / mean(y_true)\n",
    "        \"\"\"\n",
    "        if model_names is None:\n",
    "            model_names = list(self.models.keys())\n",
    "\n",
    "        results = []\n",
    "        for name in model_names:\n",
    "            if name not in self.models:\n",
    "                print(f\"Modelo '{name}' no encontrado. Saltando...\")\n",
    "                continue\n",
    "\n",
    "            model = self.models[name]\n",
    "\n",
    "            # Predicciones en validación (escaladas)\n",
    "            y_val_pred_scaled = model.predict(self.X_val)\n",
    "\n",
    "            # Des-normalizar\n",
    "            y_val_true = self._inverse_transform_target(self.y_val.values)\n",
    "            y_val_pred = self._inverse_transform_target(y_val_pred_scaled)\n",
    "\n",
    "            # Métricas en validación\n",
    "            val_r2 = r2_score(y_val_true, y_val_pred)\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "            val_mae = mean_absolute_error(y_val_true, y_val_pred)\n",
    "            val_cv = val_rmse / np.abs(y_val_true.mean())\n",
    "\n",
    "            # Predicciones en test (escaladas)\n",
    "            y_test_pred_scaled = model.predict(self.X_test)\n",
    "\n",
    "            # Des-normalizar\n",
    "            y_test_true = self._inverse_transform_target(self.y_test.values)\n",
    "            y_test_pred = self._inverse_transform_target(y_test_pred_scaled)\n",
    "\n",
    "            # Métricas en test\n",
    "            test_r2 = r2_score(y_test_true, y_test_pred)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "            test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "            test_cv = test_rmse / np.abs(y_test_true.mean())\n",
    "\n",
    "            results.append({\n",
    "                'model': name,\n",
    "                'val_r2': val_r2,\n",
    "                'val_rmse': val_rmse,\n",
    "                'val_mae': val_mae,\n",
    "                'val_cv': val_cv,\n",
    "                'test_r2': test_r2,\n",
    "                'test_rmse': test_rmse,\n",
    "                'test_mae': test_mae,\n",
    "                'test_cv': test_cv\n",
    "            })\n",
    "\n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  Val  - R²: {val_r2:.4f}, RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, CV: {val_cv:.4f}\")\n",
    "            print(f\"  Test - R²: {test_r2:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, CV: {test_cv:.4f}\")\n",
    "\n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8osvambituh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear objeto de modelado con los datos procesados y el preprocessor\n",
    "modeling = SteelEnergyModeling(output, preprocessor=eda.preprocessor, target_col='Usage_kWh')\n",
    "\n",
    "# Preparar datos (train/val/test split)\n",
    "modeling.prepare_data(test_size=0.2, val_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar regresión lineal\n",
    "modeling.train_linear_regression()\n",
    "\n",
    "# Entrenar XGBoost\n",
    "modeling.train_xgboost(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Evaluar modelos (des-normaliza automáticamente)\n",
    "results = modeling.evaluate_models()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}