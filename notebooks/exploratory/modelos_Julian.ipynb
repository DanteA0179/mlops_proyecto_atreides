{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad01d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataframe_inicial(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza una limpieza general inicial en el DataFrame.\n",
    "\n",
    "    Esta función toma un DataFrame, elimina los espacios en blanco\n",
    "    de las columnas de tipo 'object' (texto), convierte la columna 'date'\n",
    "    a formato datetime, la ordena cronológicamente, elimina filas con\n",
    "    fechas nulas y extrae la fecha y la hora en nuevas columnas.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame de entrada que se va a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame procesado y limpio.\n",
    "    \"\"\"\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    for col in df_copia.columns:\n",
    "        if df_copia[col].dtype == 'object':\n",
    "            df_copia[col] = df_copia[col].str.strip()\n",
    "\n",
    "    df_copia['date'] = pd.to_datetime(df_copia['date'], dayfirst=True)\n",
    "    df_copia = df_copia.sort_values(by='date', ascending=True)\n",
    "    df_copia = df_copia.dropna(subset=['date'])\n",
    "\n",
    "    #df_copia['fecha'] = df_copia['date'].dt.date\n",
    "    #df_copia['hora'] = df_copia['date'].dt.time\n",
    "    #df_copia = df_copia.replace(['nan', 'NaN', 'None', 'NULL', 'null'], np.nan)\n",
    "\n",
    "\n",
    "    return df_copia\n",
    "\n",
    "def limpiar_columnas_categoricas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia y estandariza las columnas categóricas del DataFrame.\n",
    "\n",
    "    Esta función corrige valores mal escritos y maneja los datos nulos\n",
    "    en las columnas 'WeekStatus', 'Day_of_week' y 'Load_Type'.\n",
    "    Los valores incorrectos se reemplazan por su versión correcta y los\n",
    "    nulos se rellenan utilizando el método 'forward fill'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con las columnas categóricas a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las columnas categóricas estandarizadas.\n",
    "    \"\"\"\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    # Mapeo de correcciones para cada columna\n",
    "    mapa_correcciones = {\n",
    "        'WeekStatus': {\n",
    "            'wEEKDAY': 'Weekday', 'wEEKEND': 'Weekend', 'NAN': np.nan\n",
    "        },\n",
    "        'Day_of_week': {\n",
    "            'mONDAY': 'Monday', 'tUESDAY': 'Tuesday', 'wEDNESDAY': 'Wednesday',\n",
    "            'tHURSDAY': 'Thursday', 'fRIDAY': 'Friday', 'sATURDAY': 'Saturday',\n",
    "            'sUNDAY': 'Sunday', 'NAN': np.nan\n",
    "        },\n",
    "        'Load_Type': {\n",
    "            'lIGHT_lOAD': 'Light_Load', 'mEDIUM_lOAD': 'Medium_Load',\n",
    "            'mAXIMUM_lOAD': 'Maximum_Load', 'NAN': np.nan\n",
    "        }\n",
    "    }\n",
    "\n",
    "    columnas_categoricas = ['WeekStatus', 'Day_of_week', 'Load_Type']\n",
    "\n",
    "    for col in columnas_categoricas:\n",
    "        df_copia[col] = df_copia[col].replace(mapa_correcciones[col])\n",
    "        df_copia[col] = df_copia[col].astype(str)\n",
    "\n",
    "    return df_copia\n",
    "\n",
    "\n",
    "def limpiar_columnas_numericas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia y estandariza un conjunto predefinido de columnas numéricas.\n",
    "\n",
    "    La función identifica valores no numéricos en columnas que deberían serlo,\n",
    "    los convierte a NaN (Not a Number) y luego utiliza el método 'forward fill'\n",
    "    para imputar los valores faltantes. Finalmente, asegura que todas las\n",
    "    columnas procesadas tengan el tipo de dato 'float'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con columnas numéricas a procesar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las columnas numéricas limpias y\n",
    "                      en formato float.\n",
    "    \"\"\"\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    columnas_numericas = [\n",
    "        'Usage_kWh', 'Lagging_Current_Reactive.Power_kVarh',\n",
    "        'Leading_Current_Reactive_Power_kVarh', 'CO2(tCO2)',\n",
    "        'Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor',\n",
    "        'NSM', 'mixed_type_col'\n",
    "    ]\n",
    "\n",
    "    for col in columnas_numericas:\n",
    "        # Forzar valores no numéricos a NaN\n",
    "        df_copia[col] = pd.to_numeric(df_copia[col], errors='coerce')\n",
    "\n",
    "        # Asegurar el tipo float\n",
    "        df_copia[col] = df_copia[col].astype(float)\n",
    "\n",
    "    return df_copia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3b96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class SteelEnergyEDA:\n",
    "    \"\"\"\n",
    "    Clase para realizar análisis exploratorio de datos (EDA), limpieza, visualización y preprocesamiento\n",
    "    de un dataset de energía y acero.\n",
    "\n",
    "    Esta clase carga datos desde un archivo CSV, realiza limpieza inicial, genera visualizaciones,\n",
    "    aplica transformaciones y guarda los datos procesados.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Ruta del archivo CSV que contiene el dataset.\n",
    "        df (pd.DataFrame): DataFrame cargado desde el archivo.\n",
    "        num_cols (list): Lista de nombres de columnas numéricas.\n",
    "        cat_cols (list): Lista de nombres de columnas categóricas.\n",
    "        scaler (StandardScaler): Objeto de escalado para normalizar datos numéricos.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.num_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.scaler = None\n",
    "\n",
    "    # Cargar Datos\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Carga el dataset desde un archivo CSV en el atributo `df`.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Primeras filas del dataset cargado.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        print(f\"Se cargo el dataset correctamente: {self.df.shape}\")\n",
    "        \n",
    "        return self.df.head()\n",
    "\n",
    "    # Resumen Básico\n",
    "    def basic_summary(self):\n",
    "        print(\"\\n--- INFO ---\")\n",
    "        self.df.info()\n",
    "        print(\"\\n--- STATS ---\")\n",
    "        display(self.df.describe())\n",
    "\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Genera visualizaciones para el dataset.\n",
    "\n",
    "        Incluye:\n",
    "            - Heatmap de correlación para variables numéricas.\n",
    "            - Histogramas para cada variable numérica.\n",
    "            - Pairplot para correlaciones.\n",
    "            - Gráficos de barras para variables categóricas.\n",
    "        \"\"\"\n",
    "        sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "\n",
    "        # 1. HeatMap Correlación\n",
    "        if len(self.num_cols) > 1:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            corr = self.df[self.num_cols].corr()\n",
    "            sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "            plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "        # 2. Histogramas Variables Numéricas\n",
    "        for col in self.num_cols:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.histplot(self.df[col], kde=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "            plt.title(f\"Distribution of {col}\", fontsize=14)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "        # 3. Pairplot para correlaciones de variables numéricas\n",
    "        if len(self.num_cols) > 1:\n",
    "            sns.pairplot(self.df[self.num_cols], corner=True, diag_kind=\"kde\")\n",
    "            plt.suptitle(\"Pairplot of Numeric Variables\", y=1.02, fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "        # 4. Bar charts para variables categóricas\n",
    "        for col in self.cat_cols:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.countplot(x=col, data=self.df, order=self.df[col].value_counts().index, palette=\"pastel\")\n",
    "            plt.title(f\"Count of {col}\", fontsize=14)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Realiza limpieza del dataset.\n",
    "\n",
    "        Pasos:\n",
    "            - Limpieza general usando funciones externas.\n",
    "            - Reemplazo de valores nulos estándar.\n",
    "            - Identificación de columnas numéricas y categóricas.\n",
    "            - Eliminación de duplicados.\n",
    "            - Imputación de valores faltantes.\n",
    "            - Eliminación de outliers usando método IQR.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Primeras filas del dataset limpio.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- LIMPIEZA ---\")\n",
    "\n",
    "        initial_shape = self.df.shape\n",
    "\n",
    "        ## Limpieza General\n",
    "        self.df = limpiar_dataframe_inicial(self.df)\n",
    "        self.df = limpiar_columnas_categoricas(self.df)\n",
    "        self.df = limpiar_columnas_numericas(self.df)\n",
    "        self.df = self.df.replace(['nan', 'NaN', 'None', 'NULL', 'null'], np.nan)\n",
    "\n",
    "\n",
    "        self.num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        self.cat_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        # Eliminar duplicados\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "        # Valores Faltantes\n",
    "        self.df[self.num_cols] = self.df[self.num_cols].fillna(self.df[self.num_cols].median())\n",
    "        for col in self.cat_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "\n",
    "        # Quitar outliers (IQR)\n",
    "        for col in self.num_cols:\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "            self.df = self.df[(self.df[col] >= lower) & (self.df[col] <= upper)]\n",
    "\n",
    "        print(f\"Resultados de Limpieza de datos: {initial_shape[0]} → {self.df.shape[0]} rows\")\n",
    "        return self.df.head()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Realiza preprocesamiento del dataset.\n",
    "\n",
    "        Incluye:\n",
    "            - Extracción de características temporales.\n",
    "            - Encoding cíclico para variables de fecha.\n",
    "            - Normalización de características numéricas.\n",
    "            - Encoding de variables categóricas ordinales.\n",
    "            - Creación de un DataFrame procesado listo para análisis o modelos.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataset procesado.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- PREPROCESAMIENTO ---\")\n",
    "\n",
    "        print(self.cat_cols)\n",
    "        print(self.num_cols)\n",
    "\n",
    "        # Extraer features de tiempo\n",
    "        self.df[f'date_hour'] = self.df[\"date\"].dt.hour\n",
    "        self.df[f'date_day'] = self.df[\"date\"].dt.day\n",
    "        self.df[f'date_month'] = self.df[\"date\"].dt.month\n",
    "        self.df[f'date_year'] = self.df[\"date\"].dt.year\n",
    "\n",
    "        # Encoding ciclico\n",
    "        for col, max_val in [('date_hour', 24), ('date_month', 12), ('date_day', 31)]:\n",
    "            self.df[f'{col}_sin'] = np.sin(2 * np.pi * self.df[col] / max_val)\n",
    "            self.df[f'{col}_cos'] = np.cos(2 * np.pi * self.df[col] / max_val)\n",
    "\n",
    "        \n",
    "\n",
    "        # Dividir Features\n",
    "        numeric_features = [\n",
    "            'CO2(tCO2)', 'NSM',\n",
    "            'Leading_Current_Reactive_Power_kVarh',\n",
    "            'Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor',                        \n",
    "            'date_hour_sin', 'date_hour_cos',\n",
    "            'date_day_sin', 'date_day_cos',\n",
    "            'date_month_sin', 'date_month_cos',\n",
    "            'date_year'\n",
    "        ]\n",
    "        numeric_features = [col for col in numeric_features if col in self.df.columns]\n",
    "\n",
    "        weekstatus_feature = ['WeekStatus'] if 'WeekStatus' in self.df.columns else []\n",
    "        ordinal_features = ['Load_Type'] if 'Load_Type' in self.df.columns else []\n",
    "\n",
    "        # Pipelines\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        weekstatus_transformer = Pipeline(steps=[\n",
    "            ('label', OrdinalEncoder())\n",
    "        ])\n",
    "\n",
    "        ordinal_transformer = Pipeline(steps=[\n",
    "            ('ordinal', OrdinalEncoder(categories=[[\"Light_Load\", \"Medium_Load\", \"Maximum_Load\"]]))\n",
    "        ])\n",
    "\n",
    "        # Transformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('week', weekstatus_transformer, weekstatus_feature),\n",
    "                ('ord', ordinal_transformer, ordinal_features)\n",
    "            ],\n",
    "            sparse_threshold=0\n",
    "\n",
    "        )\n",
    "\n",
    "        self.df_processed = preprocessor.fit_transform(self.df)\n",
    "\n",
    "        processed_cols = numeric_features\n",
    "\n",
    "\n",
    "        if weekstatus_feature:\n",
    "            processed_cols += weekstatus_feature\n",
    "\n",
    "        if ordinal_features:\n",
    "            processed_cols += ordinal_features\n",
    "\n",
    "        print(\"Shape of processed data:\", self.df_processed.shape)\n",
    "        print(\"Number of processed columns:\", len(processed_cols))\n",
    "\n",
    "        self.df_processed = pd.DataFrame(self.df_processed, columns=processed_cols)\n",
    "        self.save_data(self.df_processed)\n",
    "\n",
    "        return self.df_processed\n",
    "\n",
    "\n",
    "    def save_data(self, df, output_path=\"cleaned_steel_energy_v2.csv\"):\n",
    "        \"\"\"\n",
    "        Guarda el DataFrame procesado en un archivo CSV.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset a guardar.\n",
    "            output_path (str): Ruta del archivo de salida (default: \"cleaned_steel_energy_v2.csv\").\n",
    "        \"\"\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved cleaned dataset to '{output_path}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0189e3de",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/raw/steel_energy_modified.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m eda \u001b[38;5;241m=\u001b[39m SteelEnergyEDA(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/raw/steel_energy_modified.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Cargar Datos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43meda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2. Limpiar Datos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m eda\u001b[38;5;241m.\u001b[39mclean_data()\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mSteelEnergyEDA.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Carga el dataset desde un archivo CSV en el atributo `df`.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m        pd.DataFrame: Primeras filas del dataset cargado.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSe cargo el dataset correctamente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/raw/steel_energy_modified.csv'"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "eda = SteelEnergyEDA(\"../../data/raw/steel_energy_modified.csv\")\n",
    "\n",
    "# 1. Cargar Datos\n",
    "eda.load_data()\n",
    "\n",
    "# 2. Limpiar Datos\n",
    "eda.clean_data()\n",
    "\n",
    "# 3. Resumen \n",
    "eda.basic_summary()\n",
    "\n",
    "# 4. Visualizaciones\n",
    "eda.visualize()\n",
    "\n",
    "# 5. Preprocessamiento\n",
    "output = eda.preprocess_data()\n",
    "\n",
    "output.head()\n",
    "output.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64nr8w7i28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "class SteelEnergyModeling:\n",
    "    \"\"\"\n",
    "    Clase para entrenar modelos de machine learning sobre datos procesados.\n",
    "    \n",
    "    Attributes:\n",
    "        df_processed (pd.DataFrame): DataFrame con los datos ya procesados por SteelEnergyEDA.\n",
    "        target_col (str): Nombre de la columna objetivo a predecir.\n",
    "        X_train (pd.DataFrame): Features de entrenamiento.\n",
    "        X_test (pd.DataFrame): Features de prueba.\n",
    "        y_train (pd.Series): Target de entrenamiento.\n",
    "        y_test (pd.Series): Target de prueba.\n",
    "        models (dict): Diccionario con los modelos entrenados {'nombre_modelo': modelo}.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df_processed, target_col='Usage_kWh'):\n",
    "        \"\"\"\n",
    "        Inicializa el objeto SteelEnergyModeling.\n",
    "        \n",
    "        Args:\n",
    "            df_processed (pd.DataFrame): DataFrame procesado que contiene features y target.\n",
    "            target_col (str): Nombre de la columna objetivo. Default: 'Usage_kWh'.\n",
    "        \"\"\"\n",
    "        self.df_processed = df_processed\n",
    "        self.target_col = target_col\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.models = {}\n",
    "        \n",
    "    def prepare_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Divide los datos en conjuntos de entrenamiento y prueba.\n",
    "        \n",
    "        Args:\n",
    "            test_size (float): Proporción del conjunto de prueba (0-1). Default: 0.2.\n",
    "            random_state (int): Semilla para reproducibilidad. Default: 42.\n",
    "            \n",
    "        Returns:\n",
    "            None. Actualiza los atributos X_train, X_test, y_train, y_test del objeto.\n",
    "        \"\"\"\n",
    "        X = self.df_processed.drop(columns=[self.target_col])\n",
    "        y = self.df_processed[self.target_col]\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"Train: {self.X_train.shape}, Test: {self.X_test.shape}\")\n",
    "        \n",
    "    def train_linear_regression(self):\n",
    "        \"\"\"\n",
    "        Entrena un modelo de regresión lineal.\n",
    "        \n",
    "        Args:\n",
    "            None.\n",
    "            \n",
    "        Returns:\n",
    "            None. Guarda el modelo entrenado en self.models['linear_regression'] \n",
    "            e imprime el R² en el conjunto de prueba.\n",
    "        \"\"\"\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(self.X_train, self.y_train)\n",
    "        self.models['linear_regression'] = lr\n",
    "        \n",
    "        y_pred = lr.predict(self.X_test)\n",
    "        print(f\"Linear Regression - R²: {r2_score(self.y_test, y_pred):.4f}\")\n",
    "        \n",
    "    def train_xgboost(self, **params):\n",
    "        \"\"\"\n",
    "        Entrena un modelo XGBoost.\n",
    "        \n",
    "        Args:\n",
    "            **params: Parámetros opcionales para XGBRegressor (ej: n_estimators=100, \n",
    "                     max_depth=6, learning_rate=0.1, random_state=42).\n",
    "            \n",
    "        Returns:\n",
    "            None. Guarda el modelo entrenado en self.models['xgboost'] \n",
    "            e imprime el R² en el conjunto de prueba.\n",
    "        \"\"\"\n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        xgb_model.fit(self.X_train, self.y_train)\n",
    "        self.models['xgboost'] = xgb_model\n",
    "        \n",
    "        y_pred = xgb_model.predict(self.X_test)\n",
    "        print(f\"XGBoost - R²: {r2_score(self.y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8osvambituh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear objeto de modelado con los datos procesados\n",
    "modeling = SteelEnergyModeling(output, target_col='Usage_kWh')\n",
    "\n",
    "# Preparar datos (train/test split)\n",
    "modeling.prepare_data(test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar regresión lineal\n",
    "modeling.train_linear_regression()\n",
    "\n",
    "# Entrenar XGBoost\n",
    "modeling.train_xgboost(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
